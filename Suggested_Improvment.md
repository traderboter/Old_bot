# Ù…Ø´Ú©Ù„Ø§Øª Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡ Ùˆ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¨Ù‡Ø¨ÙˆØ¯ Ø³ÛŒØ³ØªÙ… Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ

Ø§ÛŒÙ† ÙØ§ÛŒÙ„ Ø´Ø§Ù…Ù„ Ù…Ø´Ú©Ù„Ø§ØªØŒ Ù†ÙˆØ§Ù‚Øµ Ùˆ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¨Ù‡Ø¨ÙˆØ¯ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¨Ø®Ø´ Ø§Ø² Ø³ÛŒØ³ØªÙ… Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ø§Ø³Øª.

> **ØªÙˆØ¬Ù‡:** ØªÙˆØ¶ÛŒØ­Ø§Øª Ù†Ø­ÙˆÙ‡ Ú©Ø§Ø± ÙØ¹Ù„ÛŒ Ø³ÛŒØ³ØªÙ… Ø¯Ø± ÙØ§ÛŒÙ„ `signal.md` Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯.

---

## ÙÙ‡Ø±Ø³Øª Ù…Ø·Ø§Ù„Ø¨
- [Ø¨Ø®Ø´ Û²: ØªØ­Ù„ÛŒÙ„ ÛŒÚ© ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…](#Ø¨Ø®Ø´-Û²-ØªØ­Ù„ÛŒÙ„-ÛŒÚ©-ØªØ§ÛŒÙ…ÙØ±ÛŒÙ…)
  - [Ù…Ø±Ø­Ù„Ù‡ 1: ØªØ´Ø®ÛŒØµ Ø±ÙˆÙ†Ø¯ (Trend Detection)](#Ù…Ø±Ø­Ù„Ù‡-1-ØªØ´Ø®ÛŒØµ-Ø±ÙˆÙ†Ø¯-trend-detection)

---

# Ø¨Ø®Ø´ Û²: ØªØ­Ù„ÛŒÙ„ ÛŒÚ© ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…

## Ù…Ø±Ø­Ù„Ù‡ 1: ØªØ´Ø®ÛŒØµ Ø±ÙˆÙ†Ø¯ (Trend Detection)

### âœ… Ù†Ú©Ø§Øª Ù…Ø«Ø¨Øª Ùˆ Ù…Ù†Ø·Ù‚ÛŒ

#### 1. Ø¹Ø¯Ù… Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ… - Ø±ÙˆÛŒÚ©Ø±Ø¯ Ø¯Ø±Ø³Øª Ø§Ø³Øª
- EMA ÛŒÚ© Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± ØªØ£Ø®ÛŒØ±ÛŒ (lagging) Ø§Ø³Øª
- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¢Ù† Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÙÛŒÙ„ØªØ± Ø¬Ù‡Øª Ù…Ù†Ø·Ù‚ÛŒâ€ŒØªØ± Ø§Ø² Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ… Ø§Ø³Øª
- Ø§ÛŒÙ† Ø±ÙˆÛŒÚ©Ø±Ø¯ Ø§Ø² "double-counting" Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯

#### 2. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Context
- Trend ÛŒÚ© context Ú©Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ø³Ø§ÛŒØ± Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ ÙØ±Ø§Ù‡Ù… Ù…ÛŒâ€ŒÚ©Ù†Ø¯
- Ø§ÛŒÙ† Ø¨Ø§ Ø§ØµÙ„ "Trade with the trend" Ù‡Ù…Ø®ÙˆØ§Ù†ÛŒ Ø¯Ø§Ø±Ø¯
- Ø¨Ù‡â€ŒÙˆÛŒÚ˜Ù‡ Ø¯Ø± ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ù„Ø§ØªØ± (4h, 1h) Ø¨Ø³ÛŒØ§Ø± Ø­ÛŒØ§ØªÛŒ Ø§Ø³Øª

#### 3. Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø­Ø§Ù„Øªâ€ŒÙ‡Ø§ÛŒ Pullback
- Ú©Ø¯ Ø­Ø§Ù„Øªâ€ŒÙ‡Ø§ÛŒ `bullish_pullback` Ùˆ `bearish_pullback` Ø±Ø§ ØªØ´Ø®ÛŒØµ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯
- Ø§ÛŒÙ† Ø­Ø§Ù„Øªâ€ŒÙ‡Ø§ ÙØ±ØµØªâ€ŒÙ‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ Ø®ÙˆØ¨ÛŒ Ø¯Ø± Ø·ÙˆÙ„ Ø±ÙˆÙ†Ø¯ Ù‡Ø³ØªÙ†Ø¯
- Ù…Ù†Ø·Ù‚ Ù¾Ø´Øª Ø¢Ù†: "Ø®Ø±ÛŒØ¯ Ø¯Ø± Ø§ØµÙ„Ø§Ø­ ÛŒÚ© Ø±ÙˆÙ†Ø¯ ØµØ¹ÙˆØ¯ÛŒ"

---

### âš ï¸ Ù…Ø´Ú©Ù„Ø§Øª Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡

#### Ù…Ø´Ú©Ù„ 1: Ø¶Ø±Ø§ÛŒØ¨ Trend Bonus/Penalty Ø¨Ø³ÛŒØ§Ø± Ø¨Ø²Ø±Ú¯ Ù‡Ø³ØªÙ†Ø¯ ğŸš¨

**ØªÙˆØ¶ÛŒØ­ Ù…Ø´Ú©Ù„:**

**âš ï¸ Ù†Ú©ØªÙ‡ Ù…Ù‡Ù…:** Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ `structure_score` Ø´Ø§Ù…Ù„ **4 Ù…Ø±Ø­Ù„Ù‡** Ø§Ø³Øª:

```python
# Ø¯Ø± Ú©Ø¯ ÙØ¹Ù„ÛŒ (signal_generator.py:4395-4429):

# Ù…Ø±Ø­Ù„Ù‡ 1: Ø§Ù…ØªÛŒØ§Ø² Ù¾Ø§ÛŒÙ‡
structure_score = 1.0  # base_score

# Ù…Ø±Ø­Ù„Ù‡ 2: Bonus/Penalty Ø«Ø§Ø¨Øª
if trends_aligned:
    structure_score += 0.2  # confirm_bonus
else:
    structure_score -= 0.3  # contradict_penalty

# Ù…Ø±Ø­Ù„Ù‡ 3: Multiplier Ù…ØªØºÛŒØ± (Ù…Ø´Ú©Ù„ Ø§ÛŒÙ†Ø¬Ø§Ø³Øª! ğŸš¨)
if trends_aligned:
    structure_score *= (1 + 1.5 * (min_strength / 3))
else:
    structure_score *= (1 - 1.5 * (min_strength / 3))

# Ù…Ø±Ø­Ù„Ù‡ 4: Ù…Ø­Ø¯ÙˆØ¯ÛŒØª min/max (Ø§Ø² Ù…Ù†ÙÛŒ Ø´Ø¯Ù† Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯)
structure_score = max(min(structure_score, 1.5), 0.5)
```

**Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø§ strength=3:**

| Scenario | Base | +Bonus/-Penalty | Ã—Multiplier | Ù…Ø­Ø¯ÙˆØ¯ÛŒØª | Ù†ØªÛŒØ¬Ù‡ |
|----------|------|----------------|------------|---------|-------|
| Aligned | 1.0 | 1.2 | 1.2 Ã— 2.5 = 3.0 | min(3.0, 1.5) | **1.5** |
| Conflicting | 1.0 | 0.7 | 0.7 Ã— (-0.5) = -0.35 | max(-0.35, 0.5) | **0.5** |

**Ù†ØªÛŒØ¬Ù‡:** ØªÙØ§ÙˆØª ÙˆØ§Ù‚Ø¹ÛŒ **3x** Ø§Ø³Øª (1.5 / 0.5)ØŒ Ù†Ù‡ multiplier Ø®ÙˆØ¯ (Ú©Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ ØªØ§ 3.0 ÛŒØ§ -0.35 Ø¨Ø±Ø³Ø¯)

**Ú†Ø±Ø§ Ø§ÛŒÙ† Ù…Ø´Ú©Ù„ Ø§Ø³Øª:**

1. **ØªÙØ§ÙˆØª 3x Ø¨Ø³ÛŒØ§Ø± Ø²ÛŒØ§Ø¯ Ø§Ø³Øª:**
   - Ø³ÛŒÚ¯Ù†Ø§Ù„ aligned Ù‡Ù…ÛŒØ´Ù‡ 1.5 Ù…ÛŒâ€ŒØ´ÙˆØ¯ (Ø­Ø¯Ø§Ú©Ø«Ø±)
   - Ø³ÛŒÚ¯Ù†Ø§Ù„ conflicting Ù‡Ù…ÛŒØ´Ù‡ 0.5 Ù…ÛŒâ€ŒØ´ÙˆØ¯ (Ø­Ø¯Ø§Ù‚Ù„)
   - ØªÙ…Ø§ÛŒØ² Ø¨ÛŒÙ† strength=1ØŒ 2ØŒ 3 Ø§Ø² Ø¨ÛŒÙ† Ù…ÛŒâ€ŒØ±ÙˆØ¯! (Ù‡Ù…Ù‡ Ø¨Ù‡ min/max Ù…ÛŒâ€ŒØ±Ø³Ù†Ø¯)
   - Ø§ÛŒÙ† Ø¨Ø±Ø®Ù„Ø§Ù Ø§ØµÙ„ "Trend ÙÛŒÙ„ØªØ± Ø§Ø³ØªØŒ Ù†Ù‡ Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø§ØµÙ„ÛŒ" Ø§Ø³Øª

2. **Ø§Ø² Ø¯Ø³Øª Ø±ÙØªÙ† Ø§Ø·Ù„Ø§Ø¹Ø§Øª:**
   - multiplier Ø®ÛŒÙ„ÛŒ Ø¨Ø²Ø±Ú¯ Ø§Ø³Øª (2.5 Ø¨Ø±Ø§ÛŒ alignedØŒ -0.5 Ø¨Ø±Ø§ÛŒ conflicting)
   - Ù…Ø­Ø¯ÙˆØ¯ÛŒØª min/max Ù…Ø¬Ø¨ÙˆØ± Ø§Ø³Øª Ù‡Ù…Ù‡ Ø±Ø§ ÛŒÚ©Ø³Ø§Ù† Ú©Ù†Ø¯
   - **ØªÙ…Ø§ÛŒØ² Ø¨ÛŒÙ† strengthâ€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø§Ø² Ø¨ÛŒÙ† Ù…ÛŒâ€ŒØ±ÙˆØ¯**

3. **Ø§Ø² Ø¯Ø³Øª Ø¯Ø§Ø¯Ù† ÙØ±ØµØªâ€ŒÙ‡Ø§ÛŒ reversal:**
   - ØªÙ…Ø§Ù… Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ conflicting Ø¨Ù‡ 0.5 Ú©Ø§Ù‡Ø´ Ù…ÛŒâ€ŒÛŒØ§Ø¨Ù†Ø¯
   - Ø­ØªÛŒ Ø§Ú¯Ø± Ø§Ù„Ú¯ÙˆÛŒ reversal Ù‚ÙˆÛŒ (Ù…Ø«Ù„ H&S) Ø¯Ø± Ø³Ø·Ø­ S/R Ø¨Ø§Ø´Ø¯
   - Ø¨Ø±Ø®ÛŒ Ù…Ø¹Ø§Ù…Ù„Ù‡â€ŒÚ¯Ø±Ø§Ù† Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø¯Ø± Ù†Ù‚Ø§Ø· Ø¨Ø§Ø²Ú¯Ø´Øª (reversal) Ù…Ø¹Ø§Ù…Ù„Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯

**ØªØ£Ø«ÛŒØ± Ø¨Ø± Ø³ÛŒØ³ØªÙ…:**

- **Over-fitting Ø¨Ù‡ Ø±ÙˆÙ†Ø¯:** ØªÙØ§ÙˆØª 3x Ø®ÛŒÙ„ÛŒ Ø²ÛŒØ§Ø¯ Ø§Ø³Øª
- **ÙÙ‚Ø¯Ø§Ù† granularity:** ØªÙØ§ÙˆØª Ø¨ÛŒÙ† strength=1ØŒ 2ØŒ 3 Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
- **Ø§Ø² Ø¯Ø³Øª Ø¯Ø§Ø¯Ù† ÙØ±ØµØªâ€ŒÙ‡Ø§ÛŒ reversal:** Ù‡Ù…Ù‡ Ø¨Ù‡ 0.5 Ú©Ø§Ù‡Ø´ Ù…ÛŒâ€ŒÛŒØ§Ø¨Ù†Ø¯
- **Ø¹Ø¯Ù… ØªØ¹Ø§Ø¯Ù„:** ØªØ£Ø«ÛŒØ± trend Ø¨ÛŒØ´ Ø§Ø² Ø­Ø¯ Ø±ÙˆÛŒ Ø§Ù…ØªÛŒØ§Ø² Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø³Øª

---

#### Ù…Ø´Ú©Ù„ 2: ÙÙ„Ø³ÙÙ‡ Ù…Ø¨Ù‡Ù… Trend Phase Multiplier - Ù†Ù‡ Bug Ø¨Ù„Ú©Ù‡ ØªÙØ§ÙˆØª Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ âš ï¸

**âš ï¸ ØªÙˆØ¬Ù‡:** Ø§ÛŒÙ† ÛŒÚ© **bug** Ù†ÛŒØ³Øª - Ø§ÛŒÙ† ÛŒÚ© **ØªÙØ§ÙˆØª ÙÙ„Ø³ÙÛŒ** Ø¯Ø± Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒ Ø§Ø³Øª!

**Ú©Ø¯ ÙØ¹Ù„ÛŒ (signal_generator.py:4796-4804):**

```python
phase_multipliers = {
    'early': 1.2,       # Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† multiplier - ÙÙ„Ø³ÙÙ‡: catch trends early
    'developing': 1.1,
    'mature': 0.9,      # Ú©Ù…ØªØ±ÛŒÙ† multiplier - ÙÙ„Ø³ÙÙ‡: Ø§Ø­ØªÛŒØ§Ø· Ø§Ø² exhaustion
    'late': 0.7,
    'pullback': 1.1,
    'transition': 0.8,
    'undefined': 1.0
}
```

**Ø¯Ùˆ ÙÙ„Ø³ÙÙ‡ Ù…ØªÙØ§ÙˆØª ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯:**

| ÙÙ„Ø³ÙÙ‡ | early | mature | Ø§Ø³ØªØ¯Ù„Ø§Ù„ |
|-------|-------|--------|---------|
| **Early Trend Catching** (Ú©Ø¯ ÙØ¹Ù„ÛŒ) | 1.2 â¬†ï¸ | 0.9 â¬‡ï¸ | ÙˆØ±ÙˆØ¯ Ø²ÙˆØ¯Ù‡Ù†Ú¯Ø§Ù… â†’ RR Ø¨Ù‡ØªØ±ØŒ ÙØ¶Ø§ÛŒ Ø³ÙˆØ¯ Ø¨ÛŒØ´ØªØ± |
| **Trend Following** (Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯) | 1.03 | 1.15 â¬†ï¸ | Ø±ÙˆÙ†Ø¯ Ù‚ÙˆÛŒ Ù…Ø³ØªÙ‚Ø± â†’ Ø§Ø­ØªÙ…Ø§Ù„ Ø§Ø¯Ø§Ù…Ù‡ Ø¨ÛŒØ´ØªØ± |

**Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ú©Ø¯ ÙØ¹Ù„ÛŒ (Early Catching):**
- `early` (strength=1): Ù‚ÛŒÙ…Øª ØªØ§Ø²Ù‡ Ø§Ø² EMA20 Ø¹Ø¨ÙˆØ± Ú©Ø±Ø¯Ù‡ â†’ **ÙØ±ØµØª ÙˆØ±ÙˆØ¯ Ø²ÙˆØ¯Ù‡Ù†Ú¯Ø§Ù…** â†’ RR Ø¹Ø§Ù„ÛŒ
- `mature` (strength=3): Ø±ÙˆÙ†Ø¯ Ú©Ø§Ù…Ù„ Ù…Ø³ØªÙ‚Ø± Ø´Ø¯Ù‡ â†’ **Ø®Ø·Ø± Ù†Ø²Ø¯ÛŒÚ© exhaustion** â†’ Ø§Ø­ØªÛŒØ§Ø·

**Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ù…Ø®Ø§Ù„Ù (Trend Following):**
- `early` (strength=1): ÙÙ‚Ø· Ù‚ÛŒÙ…Øª > EMA20 â†’ **Ù‡Ù†ÙˆØ² Ø¶Ø¹ÛŒÙ** â†’ Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªØ£ÛŒÛŒØ¯
- `mature` (strength=3): Ù‡Ù…Ù‡ EMAs aligned â†’ **Ù‚ÙˆÛŒâ€ŒØªØ±ÛŒÙ† Ø­Ø§Ù„Øª** â†’ Ø§Ø­ØªÙ…Ø§Ù„ Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ø§Ù„Ø§

**âš ï¸ Ù‡ÛŒÚ†â€ŒÚ©Ø¯Ø§Ù… "Ø¯Ø±Ø³Øª" ÛŒØ§ "ØºÙ„Ø·" Ù†ÛŒØ³ØªÙ†Ø¯ - Ø¨Ø³ØªÚ¯ÛŒ Ø¨Ù‡ Ù‡Ø¯Ù Ø¯Ø§Ø±Ø¯:**

| Ø´Ø±Ø§ÛŒØ· Ø¨Ø§Ø²Ø§Ø± | Ú©Ø¯ ÙØ¹Ù„ÛŒ (Early) | Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ (Following) |
|-------------|-----------------|---------------------|
| **Trending Markets** | âš ï¸ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¯ÛŒØ± ÙˆØ§Ø±Ø¯ Ø´ÙˆØ¯ | âœ… Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ù‡ØªØ± |
| **Range-bound Markets** | âœ… catch breakouts Ø²ÙˆØ¯ØªØ± | âš ï¸ false breakouts Ø¨ÛŒØ´ØªØ± |
| **High Volatility** | âš ï¸ whipsaws Ø¨ÛŒØ´ØªØ± | âœ… ØªØ£ÛŒÛŒØ¯ Ø¨ÛŒØ´ØªØ± |

**ØªÙˆØµÛŒÙ‡:**
- Ø§ÛŒÙ† ØªØµÙ…ÛŒÙ… Ø¨Ø§ÛŒØ¯ Ø¨Ø± Ø§Ø³Ø§Ø³ **backtesting** Ú¯Ø±ÙØªÙ‡ Ø´ÙˆØ¯
- Ù†Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ ÙØ±Ø¶ÛŒØ§Øª ØªØ¦ÙˆØ±ÛŒÚ©
- Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø²Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ùˆ timeframeÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ù…ØªÙØ§ÙˆØª Ø¨Ø§Ø´Ø¯

---

#### Ù…Ø´Ú©Ù„ 3: Ø¹Ø¯Ù… ØªÙ…Ø§ÛŒØ² Ø¨ÛŒÙ† Reversal Ùˆ Counter-Trend Trading ğŸ¤”

**ØªÙˆØ¶ÛŒØ­ Ù…Ø´Ú©Ù„:**

Ø¯Ø± Ú©Ø¯ (signal_generator.py:5074):

```python
score.trend_alignment = max(0.5, 1.0 - (reversal_strength * 0.5))
```

Ø§ÛŒÙ† ÙØ±Ù…ÙˆÙ„ Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø®Ù„Ø§Ù Ø±ÙˆÙ†Ø¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ø§Ù…Ø§:

1. **Reversal (Ø¨Ø§Ø²Ú¯Ø´Øª Ø±ÙˆÙ†Ø¯):** Ø³ÛŒÚ¯Ù†Ø§Ù„ÛŒ Ú©Ù‡ Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡ ØªØºÛŒÛŒØ± Ø¬Ù‡Øª Ø±ÙˆÙ†Ø¯ Ø§Ø³Øª
   - Ù…Ø«Ø§Ù„: Ø§Ù„Ú¯ÙˆÛŒ Head & Shoulders Ø¯Ø± Ø§Ù†ØªÙ‡Ø§ÛŒ Ø±ÙˆÙ†Ø¯ ØµØ¹ÙˆØ¯ÛŒ
   - Ø§ÛŒÙ† Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ Ø§Ø±Ø²Ø´Ù…Ù†Ø¯ Ù‡Ø³ØªÙ†Ø¯!

2. **Counter-Trend Trading:** Ù…Ø¹Ø§Ù…Ù„Ù‡ Ø¯Ø± Ø®Ù„Ø§Ù Ø¬Ù‡Øª Ø±ÙˆÙ†Ø¯ Ø¨Ø¯ÙˆÙ† Ø¯Ù„ÛŒÙ„ Ù‚ÙˆÛŒ
   - Ù…Ø«Ø§Ù„: Ø®Ø±ÛŒØ¯ Ø¯Ø± ÛŒÚ© Ù†Ø²ÙˆÙ„ Ù‚ÙˆÛŒ Ø¨Ø¯ÙˆÙ† Ø§Ù„Ú¯ÙˆÛŒ Ø¨Ø§Ø²Ú¯Ø´Øª
   - Ø§ÛŒÙ† Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ Ø®Ø·Ø±Ù†Ø§Ú© Ù‡Ø³ØªÙ†Ø¯!

**Ú†Ø±Ø§ Ø§ÛŒÙ† Ù…Ø´Ú©Ù„ Ø§Ø³Øª:**

- Ú©Ø¯ ÙØ¹Ù„ÛŒ Ù‡Ø± Ø¯Ùˆ Ø±Ø§ ÛŒÚ©Ø³Ø§Ù† Ø¬Ø±ÛŒÙ…Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
- Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ reversal Ù…Ø¹ØªØ¨Ø± (Ø¨Ø§ Ø§Ù„Ú¯ÙˆÛŒ Ù‚ÙˆÛŒ) Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
- Ø¹Ø¯Ù… Ù‡ÙˆØ´Ù…Ù†Ø¯ÛŒ Ø¯Ø± ØªØ´Ø®ÛŒØµ context Ù…Ø¹Ø§Ù…Ù„Ù‡

---

#### Ù…Ø´Ú©Ù„ 4: Ø¹Ø¯Ù… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² EMA50 Slope Ø¯Ø± ØªØ´Ø®ÛŒØµ Ø±ÙˆÙ†Ø¯ Ù‚ÙˆÛŒ

**ØªÙˆØ¶ÛŒØ­ Ù…Ø´Ú©Ù„:**

Ø¯Ø± Ú©Ø¯ (signal_generator.py:1767-1768):

```python
ema20_slope = ema20[last_valid_idx] - ema20[last_valid_idx - 5]
ema50_slope = ema50[last_valid_idx] - ema50[last_valid_idx - 5]
```

`ema50_slope` Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ø§Ù…Ø§ Ø¯Ø± Ø¨ÛŒØ´ØªØ± Ø´Ø±Ø§ÛŒØ· Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯:

```python
# Ø®Ø· 1784: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
if ... and ema20_slope > 0 and ema50_slope > 0:
    strength = 3

# Ø®Ø· 1788: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯
elif ... and ema20_slope > 0:  # ema50_slope Ú†Ú© Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯
    strength = 2
```

**Ú†Ø±Ø§ Ø§ÛŒÙ† Ù…Ø´Ú©Ù„ Ø§Ø³Øª:**

- **Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ù‡Ù… Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯:**
  - `ema50_slope` Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡ Ø±ÙˆÙ†Ø¯ Ù…ÛŒØ§Ù†â€ŒÙ…Ø¯Øª Ø§Ø³Øª
  - Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ú©Ù…Ú© Ú©Ù†Ø¯ ØªØ´Ø®ÛŒØµ Ø¯Ù‡ÛŒÙ… Ø±ÙˆÙ†Ø¯ ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø³Øª ÛŒØ§ Ù†ÙˆØ³Ø§Ù† Ú©ÙˆØªØ§Ù‡â€ŒÙ…Ø¯Øª

- **Ø±ÙˆÙ†Ø¯ Strength=2 Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªÙ…Ø§Ø¯ Ù†ÛŒØ³Øª:**
  - ÙÙ‚Ø· `ema20_slope > 0` Ú†Ú© Ù…ÛŒâ€ŒØ´ÙˆØ¯
  - Ù…Ù…Ú©Ù† Ø§Ø³Øª EMA50 Ø¯Ø± Ø­Ø§Ù„ Ø³Ù‚ÙˆØ· Ø¨Ø§Ø´Ø¯ (Ø±ÙˆÙ†Ø¯ Ø¶Ø¹ÛŒÙ)

---

#### Ù…Ø´Ú©Ù„ 5: Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ ÙÛŒÙ„Ø¯ Confidence Ø¯Ø± Ø®Ø±ÙˆØ¬ÛŒ

**ØªÙˆØ¶ÛŒØ­ Ù…Ø´Ú©Ù„:**

Ø¯Ø± Ú©Ø¯ ÙØ¹Ù„ÛŒØŒ Ø®Ø±ÙˆØ¬ÛŒ `detect_trend()` Ø´Ø§Ù…Ù„ ÙÛŒÙ„Ø¯ `confidence` Ù†ÛŒØ³Øª:

```python
results.update({
    'trend': trend,
    'strength': strength,
    'method': 'moving_averages',
    'phase': trend_phase,
    'details': {...}
})
# confidence ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯!
```

**Ú†Ø±Ø§ Ø§ÛŒÙ† Ù…Ø´Ú©Ù„ Ø§Ø³Øª:**

1. **Ø¹Ø¯Ù… Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ØªØ´Ø®ÛŒØµ:**
   - Ù‡Ù…Ù‡ ØªØ´Ø®ÛŒØµâ€ŒÙ‡Ø§ ÛŒÚ© Ø§Ø¹ØªÙ…Ø§Ø¯ ÛŒÚ©Ø³Ø§Ù† Ù†Ø¯Ø§Ø±Ù†Ø¯
   - Ù…Ø«Ù„Ø§Ù‹: strength=3 Ø¨Ø§ EMA ØªÙ…ÛŒØ² â†’ confidence Ø¨Ø§Ù„Ø§
   - strength=3 Ø¨Ø§ EMA Ù†Ø²Ø¯ÛŒÚ© Ø¨Ù‡ Ù‡Ù… â†’ confidence Ù¾Ø§ÛŒÛŒÙ†

2. **Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù† ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ø¨Ù‡ØªØ± Ú©Ø±Ø¯:**
   - Ø¯Ø± Ø¨Ø±Ø®ÛŒ Ù…ÙˆØ§Ø±Ø¯ Ø¨Ù‡ØªØ± Ø§Ø³Øª Ø¨Ù‡ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ confidence Ø¨Ø§Ù„Ø§ Ø¨ÛŒØ´ØªØ± Ø§Ø¹ØªÙ…Ø§Ø¯ Ú©Ù†ÛŒÙ…
   - Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø§Ø² confidence Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†Ø¯

**Ù…Ø«Ø§Ù„ Ú©Ø§Ø±Ø¨Ø±Ø¯:**

```python
# Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯
if confidence > 0.8:
    trend_multiplier *= 1.1  # Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¨Ø§Ù„Ø§ â†’ Ù¾Ø§Ø¯Ø§Ø´
elif confidence < 0.5:
    trend_multiplier *= 0.9  # Ø§Ø¹ØªÙ…Ø§Ø¯ Ù¾Ø§ÛŒÛŒÙ† â†’ Ø§Ø­ØªÛŒØ§Ø·
```

---

### ğŸ’¡ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¨Ù‡Ø¨ÙˆØ¯

#### Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ 1: Ø§ØµÙ„Ø§Ø­ Ø¶Ø±Ø§ÛŒØ¨ Trend Bonus/Penalty âœ¨

**Ù‡Ø¯Ù:** ØªØ¹Ø§Ø¯Ù„ Ø¨Ù‡ØªØ± Ø¨ÛŒÙ† ØªØ£Ø«ÛŒØ± trend Ùˆ Ø³Ø§ÛŒØ± ÙØ§Ú©ØªÙˆØ±Ù‡Ø§

**Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ú©Ø¯ Ø¬Ø¯ÛŒØ¯:**

```python
# signal_generator.py:4402-4407
TREND_BONUS_BASE = 0.4  # Ø¨Ù‡ Ø¬Ø§ÛŒ 1.5
TREND_PENALTY_BASE = 0.3  # Ø¨Ù‡ Ø¬Ø§ÛŒ 1.5

if trends_aligned:
    # Ø­Ø¯Ø§Ú©Ø«Ø± Ø§ÙØ²Ø§ÛŒØ´ 40% (Ø¨Ù‡ Ø¬Ø§ÛŒ 150%)
    structure_score *= (1 + TREND_BONUS_BASE * (min_strength / 3))
else:
    # Ø­Ø¯Ø§Ú©Ø«Ø± Ú©Ø§Ù‡Ø´ 30% (Ø¨Ù‡ Ø¬Ø§ÛŒ 150%)
    structure_score *= (1 - TREND_PENALTY_BASE * (min_strength / 3))
```

**Ù†ØªØ§ÛŒØ¬ Ø¬Ø¯ÛŒØ¯:**

| Scenario | Old Multiplier | New Multiplier | ØªÙØ§ÙˆØª |
|----------|---------------|---------------|-------|
| Aligned, strength=3 | 2.5 (+150%) | 1.4 (+40%) | âœ… Ù…Ø¹Ù‚ÙˆÙ„â€ŒØªØ± |
| Aligned, strength=2 | 2.0 (+100%) | 1.27 (+27%) | âœ… Ù…ØªØ¹Ø§Ø¯Ù„ |
| Aligned, strength=1 | 1.5 (+50%) | 1.13 (+13%) | âœ… Ù…Ø­Ø§ÙØ¸Ù‡â€ŒÚ©Ø§Ø±Ø§Ù†Ù‡ |
| Conflicting, strength=3 | -0.5 (Ù…Ù†ÙÛŒ!) | 0.7 (-30%) | âœ… Ù…Ù†Ø·Ù‚ÛŒ |
| Conflicting, strength=2 | 0.0 (ØµÙØ±!) | 0.8 (-20%) | âœ… Ø¹Ø§Ø¯Ù„Ø§Ù†Ù‡ |
| Conflicting, strength=1 | 0.5 (-50%) | 0.9 (-10%) | âœ… Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ |

**Ù…Ø²Ø§ÛŒØ§:**

- âœ… Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² over-fitting Ø¨Ù‡ Ø±ÙˆÙ†Ø¯
- âœ… Ø­ÙØ¸ ÙØ±ØµØªâ€ŒÙ‡Ø§ÛŒ reversal
- âœ… ØªØ¹Ø§Ø¯Ù„ Ø¨Ù‡ØªØ± Ø¯Ø± Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ
- âœ… Ú©Ø§Ù‡Ø´ false negatives

---

#### Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ 2: Ø§ØµÙ„Ø§Ø­ ÙÙ„Ø³ÙÙ‡ Trend Phase Multiplier ğŸ”„

**Ù‡Ø¯Ù:** Ù‡Ù…Ø§Ù‡Ù†Ú¯ÛŒ Ø¨Ø§ Ø§ØµÙˆÙ„ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„

**Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ú©Ø¯ Ø¬Ø¯ÛŒØ¯:**

```python
# signal_generator.py:4796-4804
def _get_trend_phase_multiplier(self, phase: str, direction: str) -> float:
    """
    ÙÙ„Ø³ÙÙ‡ Ø¬Ø¯ÛŒØ¯:
    - mature: Ù‚ÙˆÛŒâ€ŒØªØ±ÛŒÙ† Ø±ÙˆÙ†Ø¯ (Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† multiplier)
    - developing: Ø±ÙˆÙ†Ø¯ Ø¯Ø± Ø­Ø§Ù„ ØªÙ‚ÙˆÛŒØª
    - early: Ø±ÙˆÙ†Ø¯ ØªØ§Ø²Ù‡ (Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªØ£ÛŒÛŒØ¯ Ø¨ÛŒØ´ØªØ±)
    - pullback: ÙØ±ØµØª ÙˆØ±ÙˆØ¯ Ø¯Ø± Ø§ØµÙ„Ø§Ø­
    - late: Ø®Ø·Ø± Ø¨Ø§Ø²Ú¯Ø´Øª Ø±ÙˆÙ†Ø¯
    """
    phase_multipliers = {
        'mature': 1.15,      # Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† - Ø±ÙˆÙ†Ø¯ Ù‚ÙˆÛŒ Ùˆ Ù…Ø³ØªÙ‚Ø±
        'pullback': 1.12,    # ÙØ±ØµØª Ø®ÙˆØ¨ - Ø®Ø±ÛŒØ¯ Ø¯Ø± Ø§ØµÙ„Ø§Ø­
        'developing': 1.08,  # Ø®ÙˆØ¨ - Ø±ÙˆÙ†Ø¯ Ø¯Ø± Ø­Ø§Ù„ ØªÙ‚ÙˆÛŒØª
        'early': 1.03,       # Ù…Ø­ØªØ§Ø·Ø§Ù†Ù‡ - Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªØ£ÛŒÛŒØ¯
        'transition': 0.95,  # Ø§Ø­ØªÛŒØ§Ø· - ØªØºÛŒÛŒØ± Ø±ÙˆÙ†Ø¯
        'late': 0.85,        # Ø®Ø·Ø±Ù†Ø§Ú© - Ø§Ø­ØªÙ…Ø§Ù„ Ø¨Ø§Ø²Ú¯Ø´Øª
        'undefined': 1.0     # Ø¨Ø¯ÙˆÙ† ØªØ£Ø«ÛŒØ±
    }
    return phase_multipliers.get(phase, 1.0)
```

**Ø§Ø³ØªØ¯Ù„Ø§Ù„:**

| Phase | Old | New | Ú†Ø±Ø§ ØªØºÛŒÛŒØ± Ú©Ø±Ø¯ØŸ |
|-------|-----|-----|----------------|
| mature | 0.9 | 1.15 | Ø±ÙˆÙ†Ø¯ Ù‚ÙˆÛŒ Ø¨Ø§ÛŒØ¯ Ù¾Ø§Ø¯Ø§Ø´ Ø¨Ú¯ÛŒØ±Ø¯ØŒ Ù†Ù‡ Ø¬Ø±ÛŒÙ…Ù‡ |
| developing | 1.1 | 1.08 | Ø®ÙˆØ¨ Ø§Ø³Øª Ø§Ù…Ø§ Ù‡Ù†ÙˆØ² Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ mature Ù‚ÙˆÛŒ Ù†ÛŒØ³Øª |
| early | 1.2 | 1.03 | Ø±ÙˆÙ†Ø¯ ØªØ§Ø²Ù‡ Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªØ£ÛŒÛŒØ¯ Ø¨ÛŒØ´ØªØ± Ø¯Ø§Ø±Ø¯ |
| pullback | 1.1 | 1.12 | ÙØ±ØµØª Ø¹Ø§Ù„ÛŒ - Ø®Ø±ÛŒØ¯ Ø¯Ø± Ø§ØµÙ„Ø§Ø­ Ø±ÙˆÙ†Ø¯ Ù‚ÙˆÛŒ |

**Ù…Ø²Ø§ÛŒØ§:**

- âœ… Ù‡Ù…Ø§Ù‡Ù†Ú¯ Ø¨Ø§ Ø§ØµÙˆÙ„ Trend Following
- âœ… Ú©Ø§Ù‡Ø´ Ø±ÛŒØ³Ú© ÙˆØ±ÙˆØ¯ Ø²ÙˆØ¯Ù‡Ù†Ú¯Ø§Ù…
- âœ… Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‚Øª Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§
- âœ… ÙÙ„Ø³ÙÙ‡ ÙˆØ§Ø¶Ø­ Ùˆ Ù‚Ø§Ø¨Ù„ ÙÙ‡Ù…

---

#### Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ 3: Ø§ÙØ²ÙˆØ¯Ù† ØªØ´Ø®ÛŒØµ Ù‡ÙˆØ´Ù…Ù†Ø¯ Reversal ğŸ¯

**Ù‡Ø¯Ù:** ØªÙ…Ø§ÛŒØ² Ø¨ÛŒÙ† reversal Ù…Ø¹ØªØ¨Ø± Ùˆ counter-trend Ø®Ø·Ø±Ù†Ø§Ú©

**Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ú©Ø¯ Ø¬Ø¯ÛŒØ¯:**

```python
def _calculate_reversal_quality(self, signal_data, trend_data, pattern_data) -> float:
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©ÛŒÙÛŒØª Ø³ÛŒÚ¯Ù†Ø§Ù„ reversal

    Returns:
        0.0-1.0: Ú©ÛŒÙÛŒØª reversal
        - 1.0: reversal Ø¨Ø³ÛŒØ§Ø± Ù…Ø¹ØªØ¨Ø±
        - 0.0: counter-trend Ø®Ø·Ø±Ù†Ø§Ú©
    """
    quality = 0.0

    # 1. Ø¢ÛŒØ§ Ø§Ù„Ú¯ÙˆÛŒ Ø¨Ø§Ø²Ú¯Ø´Øª Ù‚ÙˆÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŸ
    reversal_patterns = [
        'head_and_shoulders', 'inverse_head_and_shoulders',
        'double_top', 'double_bottom',
        'triple_top', 'triple_bottom',
        'falling_wedge', 'rising_wedge'
    ]

    pattern_names = pattern_data.get('pattern_names', [])
    has_reversal_pattern = any(p in pattern_names for p in reversal_patterns)

    if has_reversal_pattern:
        quality += 0.4  # Ø§Ù„Ú¯ÙˆÛŒ Ø¨Ø§Ø²Ú¯Ø´Øª = +40%

    # 2. Ø¢ÛŒØ§ Ø¯Ø± Ø³Ø·Ø­ Ù…Ù‚Ø§ÙˆÙ…Øª/Ø­Ù…Ø§ÛŒØª Ù‚ÙˆÛŒ Ù‡Ø³ØªÛŒÙ…ØŸ
    at_major_sr = signal_data.get('at_major_support_resistance', False)
    if at_major_sr:
        quality += 0.3  # Ø³Ø·Ø­ Ù‚ÙˆÛŒ = +30%

    # 3. Ø¢ÛŒØ§ divergence ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŸ
    has_divergence = signal_data.get('has_macd_divergence', False) or \
                     signal_data.get('has_rsi_divergence', False)
    if has_divergence:
        quality += 0.2  # divergence = +20%

    # 4. Ø¢ÛŒØ§ Ø­Ø¬Ù… Ù…Ø¹Ø§Ù…Ù„Ù‡ ØªØ£ÛŒÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŸ
    volume_confirmation = signal_data.get('volume_confirmation', 0)
    if volume_confirmation > 0.7:
        quality += 0.1  # Ø­Ø¬Ù… Ø¨Ø§Ù„Ø§ = +10%

    return min(1.0, quality)

def _apply_trend_alignment_multiplier(self, score, signal_direction, trend_data, reversal_quality):
    """
    Ø§Ø¹Ù…Ø§Ù„ multiplier Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ reversal quality
    """
    trend = trend_data['trend']
    strength = trend_data['strength']

    is_against_trend = (signal_direction == 'long' and trend == 'bearish') or \
                      (signal_direction == 'short' and trend == 'bullish')

    if is_against_trend:
        if reversal_quality > 0.7:
            # reversal Ù…Ø¹ØªØ¨Ø± - penalty Ú©Ù…
            penalty = 0.1 * abs(strength) / 3
            multiplier = 1.0 - penalty  # Ø­Ø¯Ø§Ú©Ø«Ø± -10%
        elif reversal_quality > 0.4:
            # reversal Ù…ØªÙˆØ³Ø· - penalty Ù…ØªÙˆØ³Ø·
            penalty = 0.2 * abs(strength) / 3
            multiplier = 1.0 - penalty  # Ø­Ø¯Ø§Ú©Ø«Ø± -20%
        else:
            # counter-trend Ø®Ø·Ø±Ù†Ø§Ú© - penalty Ø²ÛŒØ§Ø¯
            penalty = 0.4 * abs(strength) / 3
            multiplier = 1.0 - penalty  # Ø­Ø¯Ø§Ú©Ø«Ø± -40%
    else:
        # Ù‡Ù…Ø±Ø§Ø³ØªØ§ Ø¨Ø§ Ø±ÙˆÙ†Ø¯
        bonus = 0.3 * abs(strength) / 3
        multiplier = 1.0 + bonus  # Ø­Ø¯Ø§Ú©Ø«Ø± +30%

    return score * multiplier
```

**Ù…Ø«Ø§Ù„ Ú©Ø§Ø±Ø¨Ø±Ø¯:**

```python
# Ø³Ù†Ø§Ø±ÛŒÙˆ 1: Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø®Ø±ÛŒØ¯ Ø¯Ø± Ø±ÙˆÙ†Ø¯ Ù†Ø²ÙˆÙ„ÛŒ Ø¨Ø§ Head & Shoulders
reversal_quality = 0.9  # Ø§Ù„Ú¯ÙˆÛŒ Ù‚ÙˆÛŒ + Ø³Ø·Ø­ Ø­Ù…Ø§ÛŒØª + divergence
penalty = 0.1 * 3 / 3 = 0.1
multiplier = 0.9  # ÙÙ‚Ø· -10% Ú©Ø§Ù‡Ø´ (Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„)

# Ø³Ù†Ø§Ø±ÛŒÙˆ 2: Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø®Ø±ÛŒØ¯ Ø¯Ø± Ø±ÙˆÙ†Ø¯ Ù†Ø²ÙˆÙ„ÛŒ Ø¨Ø¯ÙˆÙ† Ø¯Ù„ÛŒÙ„
reversal_quality = 0.1  # Ø¨Ø¯ÙˆÙ† Ø§Ù„Ú¯ÙˆØŒ Ø¨Ø¯ÙˆÙ† Ø³Ø·Ø­ Ù‚ÙˆÛŒ
penalty = 0.4 * 3 / 3 = 0.4
multiplier = 0.6  # -40% Ú©Ø§Ù‡Ø´ (Ø¬Ø±ÛŒÙ…Ù‡ Ø³Ù†Ú¯ÛŒÙ†)
```

**Ù…Ø²Ø§ÛŒØ§:**

- âœ… Ø­ÙØ¸ ÙØ±ØµØªâ€ŒÙ‡Ø§ÛŒ reversal Ù…Ø¹ØªØ¨Ø±
- âœ… Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² counter-trend Ø®Ø·Ø±Ù†Ø§Ú©
- âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨ÛŒØ´ØªØ± (Ø§Ù„Ú¯ÙˆØŒ S/RØŒ divergence)
- âœ… Ù‡ÙˆØ´Ù…Ù†Ø¯ÛŒ Ø¨Ø§Ù„Ø§ØªØ± Ø¯Ø± ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ

**âš ï¸ Ù†Ú©Ø§Øª Ù…Ù‡Ù… Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ:**

1. **ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ú©Ù‡ Ø¯Ø± Ø­Ø§Ù„ Ø­Ø§Ø¶Ø± ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ù†Ø¯:**
   ```python
   # Ø§ÛŒÙ† ÙÛŒÙ„Ø¯Ù‡Ø§ Ø¨Ø§ÛŒØ¯ Ø§Ø¶Ø§ÙÙ‡ Ø´ÙˆÙ†Ø¯:
   signal_data = {
       'at_major_support_resistance': False,  # âŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ - Ø¨Ø§ÛŒØ¯ Ø§Ø¶Ø§ÙÙ‡ Ø´ÙˆØ¯
       'has_macd_divergence': False,          # âŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ - Ø¨Ø§ÛŒØ¯ Ø§Ø² momentum Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´ÙˆØ¯
       'has_rsi_divergence': True,            # âœ… Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± momentum signals
       'volume_confirmation': 0.8             # âœ… Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± volume analysis
   }
   ```

2. **Integration Ø¨Ø§ Ú©Ø¯ Ù…ÙˆØ¬ÙˆØ¯:**
   - ØªØ§Ø¨Ø¹ `_calculate_reversal_quality` Ø¨Ø§ÛŒØ¯ **Ù‚Ø¨Ù„ Ø§Ø²** Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ù†Ù‡Ø§ÛŒÛŒ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ø´ÙˆØ¯
   - Ø¨Ø§ÛŒØ¯ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† ÛŒØ§ ØªØ±Ú©ÛŒØ¨ Ø´ÙˆØ¯ Ø¨Ø§ `is_reversal` ÙØ¹Ù„ÛŒ (Ø®Ø· 3709-3730)
   - Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ `_apply_trend_alignment_multiplier` Ø¯Ø± scoring Ù…Ù†ØªÙ‚Ù„ Ø´ÙˆØ¯

3. **Ù…Ø­Ù„ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**
   ```python
   # Ø¯Ø± signal_generator.py Ø­Ø¯ÙˆØ¯ Ø®Ø· 5070:

   # Ù…Ø­Ø§Ø³Ø¨Ù‡ reversal quality
   reversal_quality = self._calculate_reversal_quality(
       signal_data=score_result,
       trend_data=primary_trend,
       pattern_data={'pattern_names': pattern_names}
   )

   # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ
   if is_reversal or reversal_quality > 0.4:
       score.trend_alignment = self._apply_trend_alignment_multiplier(
           score.trend_alignment,
           signal.direction,
           primary_trend,
           reversal_quality
       )
   ```

4. **ØªØºÛŒÛŒØ±Ø§Øª Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø¯Ø± Support/Resistance:**
   - Ø¨Ø§ÛŒØ¯ ØªØ§Ø¨Ø¹ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ø´ÙˆØ¯ Ú©Ù‡ ØªØ´Ø®ÛŒØµ Ø¯Ù‡Ø¯ Ø¢ÛŒØ§ Ù‚ÛŒÙ…Øª Ø¯Ø± Ø³Ø·Ø­ "major" S/R Ø§Ø³Øª ÛŒØ§ Ø®ÛŒØ±
   - Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯: `at_major_sr = abs(current_price - sr_level) < atr * 0.5 and sr_strength > 0.7`

---

#### Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ 4: Ø¨Ù‡Ø¨ÙˆØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² EMA50 Slope ğŸ“ˆ

**Ù‡Ø¯Ù:** Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø§Ù…Ù„ Ø§Ø² Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙˆØ¬ÙˆØ¯

**Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ú©Ø¯ Ø¬Ø¯ÛŒØ¯:**

```python
# signal_generator.py:1784-1816
# Ø§ØµÙ„Ø§Ø­ ØªØ´Ø®ÛŒØµ Ø±ÙˆÙ†Ø¯

# Strength = 3: Ù‡Ù…Ù‡ Ø´Ø±Ø§ÛŒØ· Ø¨Ø§ÛŒØ¯ OK Ø¨Ø§Ø´Ø¯
if current_close > current_ema20 > current_ema50 > current_ema100 and \
   ema20_slope > 0 and ema50_slope > 0:
    trend = 'bullish'
    strength = 3
    trend_phase = 'mature'

# Strength = 2: Ø¨Ø§ÛŒØ¯ EMA50 slope Ù‡Ù… Ù…Ø«Ø¨Øª Ø¨Ø§Ø´Ø¯ (ØªØºÛŒÛŒØ± Ù…Ù‡Ù…!)
elif current_close > current_ema20 > current_ema50 and \
     ema20_slope > 0 and ema50_slope > 0:  # âœ… Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
    trend = 'bullish'
    strength = 2
    trend_phase = 'developing'

# Strength = 1.5: EMA50 slope Ù†Ø²Ø¯ÛŒÚ© ØµÙØ± (Developing Ø§Ù…Ø§ Ø¶Ø¹ÛŒÙâ€ŒØªØ±)
elif current_close > current_ema20 > current_ema50 and \
     ema20_slope > 0 and abs(ema50_slope) < threshold:  # âœ… Ø¬Ø¯ÛŒØ¯
    trend = 'bullish'
    strength = 1.5  # âœ… Ø³Ø·Ø­ Ø¬Ø¯ÛŒØ¯
    trend_phase = 'early_developing'

# Strength = 1: ÙÙ‚Ø· Ù‚ÛŒÙ…Øª Ø¨Ø§Ù„Ø§ÛŒ EMA20
elif current_close > current_ema20 and ema20_slope > 0:
    # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø¶Ø§ÙÛŒ: Ø¢ÛŒØ§ EMA50 Ø¯Ø± Ø®Ù„Ø§Ù Ø¬Ù‡Øª Ø§Ø³ØªØŸ
    if ema50_slope < -threshold:  # âœ… Ø¬Ø¯ÛŒØ¯
        strength = 0.5  # Ø±ÙˆÙ†Ø¯ Ø¨Ø³ÛŒØ§Ø± Ø¶Ø¹ÛŒÙ
        trend_phase = 'very_early'
    else:
        strength = 1
        trend_phase = 'early'
```

**ØªØ¹Ø±ÛŒÙ threshold:**

```python
# threshold Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¯Ø±ØµØ¯ÛŒ Ø§Ø² ATR Ø¨Ø§Ø´Ø¯
threshold = self._calculate_ema_slope_threshold(df)

def _calculate_ema_slope_threshold(self, df):
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ø³ØªØ§Ù†Ù‡ Ù…Ø¹Ù†ÛŒâ€ŒØ¯Ø§Ø± Ø¨Ø±Ø§ÛŒ EMA slope
    """
    atr = talib.ATR(df['high'].values, df['low'].values,
                    df['close'].values, timeperiod=14)
    # âš ï¸ Ø¹Ø¯Ø¯ 5% arbitrary Ø§Ø³Øª - Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙ†Ø¸ÛŒÙ… Ø¯Ø§Ø±Ø¯
    # slope Ú©Ù…ØªØ± Ø§Ø² 5% ATR Ø±Ø§ "Ù†Ø²Ø¯ÛŒÚ© ØµÙØ±" Ø¯Ø± Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ±
    return atr[-1] * 0.05
```

**Ù…Ø²Ø§ÛŒØ§:**

- âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø§Ù…Ù„ Ø§Ø² `ema50_slope`
- âœ… ØªØ´Ø®ÛŒØµ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ù‚Ø¯Ø±Øª Ø±ÙˆÙ†Ø¯
- âœ… Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² false signals Ø¯Ø± Ù†ÙˆØ³Ø§Ù†Ø§Øª Ú©ÙˆØªØ§Ù‡â€ŒÙ…Ø¯Øª
- âœ… Ø³Ø·Ø­â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ù‡ØªØ± strength (1.5, 0.5)

**âš ï¸ Ù†Ú©Ø§Øª Ù…Ù‡Ù…:**

1. **Threshold (5% ATR) arbitrary Ø§Ø³Øª:**
   - Ø§ÛŒÙ† Ø¹Ø¯Ø¯ Ø¨Ø¯ÙˆÙ† ØªØ³Øª Ùˆ backtesting Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡
   - Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¨Ø±Ø§ÛŒ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¨Ø§ÛŒØ¯ Ù…ØªÙØ§ÙˆØª Ø¨Ø§Ø´Ø¯
   - Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯: Ø§Ø² backtesting Ø¨Ø±Ø§ÛŒ ØªØ¹ÛŒÛŒÙ† Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ù‚Ø¯Ø§Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´ÙˆØ¯

2. **Strength Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ (1.5ØŒ 0.5) Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªØ³Øª Ø¯Ø§Ø±Ù†Ø¯:**
   - Ø¨Ø§ÛŒØ¯ Ù…Ø·Ù…Ø¦Ù† Ø´ÙˆÛŒØ¯ Ú©Ù‡ Ø§ÛŒÙ† Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¯Ø± Ú©Ø¯ Ø¯ÛŒÚ¯Ø± Ù…Ø´Ú©Ù„ Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯
   - Ø¨Ø±Ø®ÛŒ Ù‚Ø³Ù…Øªâ€ŒÙ‡Ø§ Ù…Ù…Ú©Ù† Ø§Ø³Øª ÙØ±Ø¶ Ú©Ù†Ù†Ø¯ strength ÙÙ‚Ø· Ø§Ø¹Ø¯Ø§Ø¯ ØµØ­ÛŒØ­ Ø§Ø³Øª

3. **Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯:**
   - Ú©Ø¯ Ù¾ÛŒÚ†ÛŒØ¯Ù‡â€ŒØªØ± Ù…ÛŒâ€ŒØ´ÙˆØ¯
   - Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªØ³Øª Ùˆ debugging Ø¨ÛŒØ´ØªØ±

---

#### Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ 5: Ø§ÙØ²ÙˆØ¯Ù† ÙÛŒÙ„Ø¯ Confidence Ø¨Ù‡ Ø®Ø±ÙˆØ¬ÛŒ ğŸ¯

**Ù‡Ø¯Ù:** Ø§Ø±Ø§Ø¦Ù‡ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ØªØ´Ø®ÛŒØµ Ø±ÙˆÙ†Ø¯

**Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ú©Ø¯ Ø¬Ø¯ÛŒØ¯:**

```python
# signal_generator.py: Ø§Ù†ØªÙ‡Ø§ÛŒ ØªØ§Ø¨Ø¹ detect_trend

def _calculate_trend_confidence(self, trend, strength, details):
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ØªØ´Ø®ÛŒØµ Ø±ÙˆÙ†Ø¯

    ÙØ§Ú©ØªÙˆØ±Ù‡Ø§ÛŒ ØªØ£Ø«ÛŒØ±Ú¯Ø°Ø§Ø±:
    1. ÙØ§ØµÙ„Ù‡ Ø¨ÛŒÙ† EMAs (Ù‡Ø±Ú†Ù‡ Ø¨ÛŒØ´ØªØ± â†’ confidence Ø¨ÛŒØ´ØªØ±)
    2. ÛŒÚ©Ù†ÙˆØ§Ø®ØªÛŒ slope (Ù‡Ø±Ú†Ù‡ ÛŒÚ©Ù†ÙˆØ§Ø®Øªâ€ŒØªØ± â†’ confidence Ø¨ÛŒØ´ØªØ±)
    3. ÙØ§ØµÙ„Ù‡ Ù‚ÛŒÙ…Øª ØªØ§ EMA Ù†Ø²Ø¯ÛŒÚ© (Ù‡Ø±Ú†Ù‡ Ø¨ÛŒØ´ØªØ± â†’ confidence Ø¨ÛŒØ´ØªØ±)
    """
    if strength == 0:
        return 0.5  # neutral = Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ù…ØªÙˆØ³Ø·

    confidence = 0.0

    # 1. ÙØ§ØµÙ„Ù‡ Ø¨ÛŒÙ† EMAs (40% ÙˆØ²Ù†)
    ema20 = details['ema20']
    ema50 = details['ema50']
    ema100 = details['ema100']

    ema_separation = abs(ema20 - ema50) / ema50
    if abs(strength) >= 2:
        ema_separation += abs(ema50 - ema100) / ema100
        ema_separation /= 2

    # Normalize Ø¨Ù‡ 0-0.4
    confidence += min(0.4, ema_separation * 100)

    # 2. Ù‚Ø¯Ø±Øª slope (30% ÙˆØ²Ù†)
    ema20_slope = details.get('ema20_slope', 0)
    price = details['close']
    slope_strength = abs(ema20_slope) / price

    # Normalize Ø¨Ù‡ 0-0.3
    confidence += min(0.3, slope_strength * 100)

    # 3. ÙØ§ØµÙ„Ù‡ Ù‚ÛŒÙ…Øª ØªØ§ EMA20 (30% ÙˆØ²Ù†)
    price_distance = abs(price - ema20) / ema20

    # Normalize Ø¨Ù‡ 0-0.3
    confidence += min(0.3, price_distance * 100)

    return min(1.0, confidence)

# Ø¯Ø± Ø§Ù†ØªÙ‡Ø§ÛŒ detect_trend:
confidence = self._calculate_trend_confidence(trend, strength, results['details'])
results['confidence'] = round(confidence, 3)
```

**Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² confidence Ø¯Ø± Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ:**

```python
# Ø¯Ø± ØªØ§Ø¨Ø¹ Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ
trend_confidence = trend_data.get('confidence', 0.7)

if trend_confidence > 0.8:
    # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø¨Ø§Ù„Ø§ - Ø§ÙØ²Ø§ÛŒØ´ ØªØ£Ø«ÛŒØ± trend
    trend_multiplier *= 1.1
elif trend_confidence < 0.5:
    # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ù¾Ø§ÛŒÛŒÙ† - Ú©Ø§Ù‡Ø´ ØªØ£Ø«ÛŒØ± trend
    trend_multiplier *= 0.9
```

**Ù…Ø²Ø§ÛŒØ§:**

- âœ… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨ÛŒØ´ØªØ± Ø¨Ø±Ø§ÛŒ ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ
- âœ… Ø§Ù…Ú©Ø§Ù† ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø±ÙˆÙ†Ø¯Ù‡Ø§ÛŒ Ø¶Ø¹ÛŒÙ
- âœ… Ø¨Ù‡Ø¨ÙˆØ¯ Ú©ÛŒÙÛŒØª Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§
- âœ… Ø´ÙØ§ÙÛŒØª Ø¨ÛŒØ´ØªØ± Ø¯Ø± ØªØ­Ù„ÛŒÙ„

**âš ï¸ Ù†Ú©Ø§Øª Ù…Ù‡Ù…:**

1. **ÙˆØ²Ù†â€ŒÙ‡Ø§ arbitrary Ù‡Ø³ØªÙ†Ø¯ (40%ØŒ 30%ØŒ 30%):**
   - Ø§ÛŒÙ† ÙˆØ²Ù†â€ŒÙ‡Ø§ Ø¨Ø¯ÙˆÙ† ØªØ³Øª Ùˆ backtesting Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯
   - Ú†Ø±Ø§ ÙØ§ØµÙ„Ù‡ EMA 40% Ø§Ø³Øª Ùˆ slope ÙÙ‚Ø· 30%ØŸ
   - Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯: Ø§Ø² backtesting ÛŒØ§ ØªØ­Ù„ÛŒÙ„ Ø­Ø³Ø§Ø³ÛŒØª Ø¨Ø±Ø§ÛŒ ØªØ¹ÛŒÛŒÙ† Ø¨Ù‡ØªØ±ÛŒÙ† ÙˆØ²Ù†â€ŒÙ‡Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´ÙˆØ¯

2. **Normalization Ù†Ø§Ø¯Ø±Ø³Øª Ø§Ø³Øª:**
   ```python
   confidence += min(0.4, ema_separation * 100)
   ```
   - Ø¶Ø±Ø¨ Ø¯Ø± 100 Ù…Ù…Ú©Ù† Ø§Ø³Øª Ù…Ù‚Ø¯Ø§Ø± Ø±Ø§ Ø®ÛŒÙ„ÛŒ Ø¨Ø²Ø±Ú¯ Ú©Ù†Ø¯
   - Ø¨Ù‡ØªØ± Ø§Ø³Øª Ø§Ø² percentile ÛŒØ§ z-score Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´ÙˆØ¯
   - Ù…Ø«Ø§Ù„ Ø¨Ù‡ØªØ±:
   ```python
   # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² percentile Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ
   ema_sep_percentile = self._get_percentile(ema_separation, historical_data)
   confidence += 0.4 * ema_sep_percentile
   ```

3. **ÙØ§Ú©ØªÙˆØ±Ù‡Ø§ÛŒ Ù…Ù‡Ù… Ø¯ÛŒÚ¯Ø± Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯:**
   - Volume (Ø­Ø¬Ù… Ù…Ø¹Ø§Ù…Ù„Ø§Øª)
   - Volatility (Ù†ÙˆØ³Ø§Ù†Ø§Øª)
   - Timeframe (ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…)
   - Ø§ÛŒÙ†â€ŒÙ‡Ø§ Ù‡Ù… Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ Ø¨Ø± confidence ØªØ£Ø«ÛŒØ± Ø¨Ú¯Ø°Ø§Ø±Ù†Ø¯

4. **Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªØ³Øª Ùˆ ØªÙ†Ø¸ÛŒÙ…:**
   - threshold Ù‡Ø§ÛŒ confidence (0.8ØŒ 0.5) arbitrary Ù‡Ø³ØªÙ†Ø¯
   - multiplier Ù‡Ø§ÛŒ confidence (1.1ØŒ 0.9) arbitrary Ù‡Ø³ØªÙ†Ø¯
   - Ø¨Ø§ÛŒØ¯ Ø§Ø² backtesting ØªØ¹ÛŒÛŒÙ† Ø´ÙˆÙ†Ø¯

---

### ğŸ“Š Ø®Ù„Ø§ØµÙ‡ ØªØ£Ø«ÛŒØ± Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª

| Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ | Ø§ÙˆÙ„ÙˆÛŒØª | ØªØ£Ø«ÛŒØ± Ø¨Ø± Ø¯Ù‚Øª | Ø³Ø®ØªÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ |
|---------|--------|-------------|-----------------|
| Ø§ØµÙ„Ø§Ø­ Ø¶Ø±Ø§ÛŒØ¨ Bonus/Penalty | ğŸ”´ Ø¨Ø§Ù„Ø§ | +15-20% | Ø¢Ø³Ø§Ù† |
| Ø§ØµÙ„Ø§Ø­ Phase Multiplier | ğŸŸ¡ Ù…ØªÙˆØ³Ø· | +5-10% | Ø¢Ø³Ø§Ù† |
| ØªØ´Ø®ÛŒØµ Ù‡ÙˆØ´Ù…Ù†Ø¯ Reversal | ğŸ”´ Ø¨Ø§Ù„Ø§ | +10-15% | Ù…ØªÙˆØ³Ø· |
| Ø¨Ù‡Ø¨ÙˆØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² EMA50 Slope | ğŸŸ¡ Ù…ØªÙˆØ³Ø· | +5-8% | Ø¢Ø³Ø§Ù† |
| Ø§ÙØ²ÙˆØ¯Ù† Confidence | ğŸŸ¢ Ù¾Ø§ÛŒÛŒÙ† | +3-5% | Ù…ØªÙˆØ³Ø· |

**ØªÙˆØµÛŒÙ‡:** Ø´Ø±ÙˆØ¹ Ø¨Ø§ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª 1 Ùˆ 3 (Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§Ù„Ø§)

---

### ğŸ§ª Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø¨Ø±Ø§ÛŒ ØªØ³Øª

Ù¾Ø³ Ø§Ø² Ø§Ø¹Ù…Ø§Ù„ ØªØºÛŒÛŒØ±Ø§ØªØŒ Ø¨Ø§ÛŒØ¯ Ù…ÙˆØ§Ø±Ø¯ Ø²ÛŒØ± ØªØ³Øª Ø´ÙˆÙ†Ø¯:

1. **Backtest Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ:**
   - Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù†ØªØ§ÛŒØ¬ Ù‚Ø¨Ù„ Ùˆ Ø¨Ø¹Ø¯ Ø§Ø² ØªØºÛŒÛŒØ±Ø§Øª
   - Ù…Ø­Ø§Ø³Ø¨Ù‡ Win Rate, Profit Factor, Maximum Drawdown

2. **ØªØ³Øª Ø±ÙˆÛŒ Ø³Ù†Ø§Ø±ÛŒÙˆÙ‡Ø§ÛŒ Ø®Ø§Øµ:**
   - Ø±ÙˆÙ†Ø¯ Ù‚ÙˆÛŒ ØµØ¹ÙˆØ¯ÛŒ/Ù†Ø²ÙˆÙ„ÛŒ
   - Ø¨Ø§Ø²Ø§Ø± range-bound
   - Ù†Ù‚Ø§Ø· reversal Ù…Ø¹ØªØ¨Ø±
   - Ù†Ù‚Ø§Ø· reversal Ú©Ø§Ø°Ø¨ (false reversal)

3. **A/B Testing:**
   - Ø§Ø¬Ø±Ø§ÛŒ Ù‡Ù…Ø²Ù…Ø§Ù† Ø³ÛŒØ³ØªÙ… Ù‚Ø¯ÛŒÙ… Ùˆ Ø¬Ø¯ÛŒØ¯
   - Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¯Ø± Ø´Ø±Ø§ÛŒØ· Ø¨Ø§Ø²Ø§Ø± Ù…Ø®ØªÙ„Ù

---

## Ù…Ø±Ø­Ù„Ù‡ 2: ØªØ­Ù„ÛŒÙ„ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ù…ÙˆÙ…Ù†ØªÙˆÙ… (RSI, Stochastic, MACD, MFI)

### âœ… Ù†Ú©Ø§Øª Ù…Ø«Ø¨Øª Ùˆ Ù…Ù†Ø·Ù‚ÛŒ

#### 1. Ø§Ø³ØªÙØ§Ø¯Ù‡ ØµØ­ÛŒØ­ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ØªØ£ÛŒÛŒØ¯ Ú©Ù†Ù†Ø¯Ù‡
- Momentum Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ Ø³ÛŒÚ¯Ù†Ø§Ù„ Ù…Ø³ØªÙ‚ÛŒÙ… ØªÙˆÙ„ÛŒØ¯ Ù†Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯
- ÙÙ‚Ø· Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ØªØ£ÛŒÛŒØ¯ Ú©Ù†Ù†Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
- Ø§ÛŒÙ† Ø±ÙˆÛŒÚ©Ø±Ø¯ Ø¯Ø±Ø³Øª Ø§Ø³Øª Ú†ÙˆÙ† momentum indicators lagging Ù‡Ø³ØªÙ†Ø¯

#### 2. Ø´Ø±Ø· Reversal Ø¯Ù‚ÛŒÙ‚ Ùˆ Ù…Ù†Ø·Ù‚ÛŒ
- Ø¨Ø±Ø§ÛŒ RSI oversold/overboughtØŒ ÙÙ‚Ø· Ø¢Ø³ØªØ§Ù†Ù‡ Ú©Ø§ÙÛŒ Ù†ÛŒØ³Øª
- Ú©Ø¯ Ú†Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± Ø´Ø±ÙˆØ¹ Ø¨Ù‡ Ø¨Ø§Ø²Ú¯Ø´Øª Ú©Ø±Ø¯Ù‡ Ø¨Ø§Ø´Ø¯
- `curr_rsi < 30 and curr_rsi > prev_rsi` âœ…

#### 3. ØªØ´Ø®ÛŒØµ ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡
- Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø¨Ø±Ø§ÛŒ ÛŒØ§ÙØªÙ† peaks Ùˆ valleys
- Ù…Ø­Ø§Ø³Ø¨Ù‡ strength Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø±ØµØ¯ ØªØºÛŒÛŒØ±Ø§Øª
- ÙÛŒÙ„ØªØ± Ú©ÛŒÙÛŒØª (divergence_sensitivity)
- ÙÛŒÙ„ØªØ± Ø²Ù…Ø§Ù†ÛŒ (ÙÙ‚Ø· 10 Ú©Ù†Ø¯Ù„ Ø§Ø®ÛŒØ±)

#### 4. Ø´Ø±Ø§ÛŒØ· Ø¯Ù‚ÛŒÙ‚ Stochastic Cross
- Ù†Ù‡ ÙÙ‚Ø· oversold/overbought Ø¨Ù„Ú©Ù‡ ØªÙ‚Ø§Ø·Ø¹ ÙˆØ§Ù‚Ø¹ÛŒ K Ùˆ D
- `curr_k > curr_d and prev_k <= prev_d` âœ…
- Ø§ÛŒÙ† Ø§Ø² false signals Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯

#### 5. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² MFI Ø¨Ø±Ø§ÛŒ ØªØ£ÛŒÛŒØ¯ Ø¨Ø§ Ø­Ø¬Ù…
- MFI ØªØ±Ú©ÛŒØ¨ÛŒ Ø§Ø² Ù‚ÛŒÙ…Øª Ùˆ Ø­Ø¬Ù… Ø§Ø³Øª
- Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ø§Ø² RSI Ø¯Ø± ØªØ´Ø®ÛŒØµ reversals
- ÙÙ‚Ø· Ø²Ù…Ø§Ù†ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ volume Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ø§Ø³Øª

#### 6. Caching Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Performance
- Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ ÙÙ‚Ø· ÛŒÚ© Ø¨Ø§Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
- Ø§Ø² cache Ø¨Ø±Ø§ÛŒ timeframe Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯

---

### âš ï¸ Ù…Ø´Ú©Ù„Ø§Øª Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡

#### Ù…Ø´Ú©Ù„ 1: ØªÙ†Ø§Ù‚Ø¶ Ø¯Ø± Ù…Ø³ØªÙ†Ø¯Ø§Øª Ùˆ Ú©Ø¯ - Ø§Ù…ØªÛŒØ§Ø²Ø§Øª ğŸš¨

**ØªÙˆØ¶ÛŒØ­ Ù…Ø´Ú©Ù„:**

Ø¯Ø± Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù‚Ø¯ÛŒÙ…ÛŒ signal.md (Ù‚Ø¨Ù„ Ø§Ø² Ø§ØµÙ„Ø§Ø­):
```
- RSI < 30 + Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø®Ø±ÛŒØ¯ â†’ +10 ØªØ§ +15 Ø§Ù…ØªÛŒØ§Ø²
- Stochastic Cross â†’ +5 ØªØ§ +10 Ø§Ù…ØªÛŒØ§Ø²
- ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ RSI â†’ +15 ØªØ§ +20 Ø§Ù…ØªÛŒØ§Ø²
```

Ø§Ù…Ø§ Ø¯Ø± Ú©Ø¯ ÙˆØ§Ù‚Ø¹ÛŒ (signal_generator.py:3610-3650):
```python
'rsi_oversold_reversal': 2.3
'stochastic_oversold_bullish_cross': 2.5
'rsi_bullish_divergence': 3.5 Ã— strength  # 0 ØªØ§ 3.5
```

**Ú†Ø±Ø§ Ø§ÛŒÙ† ØªÙ†Ø§Ù‚Ø¶ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´Øª:**

1. **Ù…Ø³ØªÙ†Ø¯Ø§Øª base scores Ø±Ø§ Ø°Ú©Ø± Ù†Ú©Ø±Ø¯Ù‡ Ø¨ÙˆØ¯:**
   - Ø§Ù…ØªÛŒØ§Ø²Ø§Øª 2.3, 2.5, 3.5 ÙÙ‚Ø· base scores Ù‡Ø³ØªÙ†Ø¯
   - Ø§ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø²Ø§Øª Ø¨Ø¹Ø¯Ø§Ù‹ scale Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ Ùˆ Ø¨Ø§ Ø¶Ø±Ø§ÛŒØ¨ Ø¯ÛŒÚ¯Ø± ØªØ±Ú©ÛŒØ¨ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
   - Ø¯Ø± ÙØ±Ù…ÙˆÙ„ Ù†Ù‡Ø§ÛŒÛŒ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¨Ù‡ 10-15 Ø¨Ø±Ø³Ù†Ø¯

2. **Ø¹Ø¯Ù… ØªÙˆØ¶ÛŒØ­ ÙØ±Ø¢ÛŒÙ†Ø¯ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ:**
   - Base scores Ø¬Ù…Ø¹ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
   - Ø¨Ø§ trend/alignment/regime multipliers Ø¶Ø±Ø¨ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
   - Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ (scale to 0-100)

**ØªØ£Ø«ÛŒØ±:**
- Ù…Ø³ØªÙ†Ø¯Ø§Øª Ú¯Ù…Ø±Ø§Ù‡â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø¨ÙˆØ¯
- ØªÙˆØ³Ø¹Ù‡â€ŒØ¯Ù‡Ù†Ø¯Ú¯Ø§Ù† Ø¬Ø¯ÛŒØ¯ confused Ù…ÛŒâ€ŒØ´Ø¯Ù†Ø¯
- **âœ… Ø­Ù„ Ø´Ø¯:** Ø¯Ø± Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¬Ø¯ÛŒØ¯ Ø§ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹ Ø¨Ù‡ ÙˆØ¶ÙˆØ­ ØªÙˆØ¶ÛŒØ­ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯

---

#### Ù…Ø´Ú©Ù„ 2: Ø¹Ø¯Ù… ØªÙˆØ¶ÛŒØ­ Ø´Ø±Ø§ÛŒØ· Ø¯Ù‚ÛŒÙ‚ Ø¯Ø± Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù‚Ø¯ÛŒÙ…ÛŒ âš ï¸

**ØªÙˆØ¶ÛŒØ­ Ù…Ø´Ú©Ù„:**

Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù‚Ø¯ÛŒÙ…ÛŒ ÙÙ‚Ø· Ù…ÛŒâ€ŒÚ¯ÙØª: "RSI < 30"

Ø§Ù…Ø§ Ú©Ø¯ Ø´Ø±Ø· Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ø¯Ø§Ø±Ø¯:
```python
if curr_rsi < 30 and curr_rsi > prev_rsi:
```

**ØªÙØ§ÙˆØª:**

| Ø­Ø§Ù„Øª | Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù‚Ø¯ÛŒÙ…ÛŒ | Ú©Ø¯ ÙˆØ§Ù‚Ø¹ÛŒ | Ù†ØªÛŒØ¬Ù‡ |
|------|--------------|----------|-------|
| RSI=25, prev=28 | âœ… (RSI < 30) | âŒ (Ù‡Ù†ÙˆØ² Ø¯Ø± Ø­Ø§Ù„ Ø³Ù‚ÙˆØ·) | Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù†Ø§Ù‚Øµ Ø¨ÙˆØ¯ |
| RSI=28, prev=25 | âœ… (RSI < 30) | âœ… (Ø´Ø±ÙˆØ¹ Ø¨Ø§Ø²Ú¯Ø´Øª) | Ø¯Ø±Ø³Øª |

**Ú†Ø±Ø§ Ø§ÛŒÙ† Ù…Ù‡Ù… Ø§Ø³Øª:**

- Ø§Ú¯Ø± ÙÙ‚Ø· RSI < 30 Ø±Ø§ Ú†Ú© Ú©Ù†ÛŒÙ…ØŒ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¯Ø± ÙˆØ³Ø· Ø³Ù‚ÙˆØ· ÙˆØ±ÙˆØ¯ Ú©Ù†ÛŒÙ…
- Ø´Ø±Ø· `curr_rsi > prev_rsi` ØªØ¶Ù…ÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ momentum Ø¯Ø± Ø­Ø§Ù„ ØªØºÛŒÛŒØ± Ø§Ø³Øª
- Ø§ÛŒÙ† Ø§Ø² **Catching a Falling Knife** Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯

**ØªØ£Ø«ÛŒØ±:**
- **âœ… Ø­Ù„ Ø´Ø¯:** Ø¯Ø± Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¬Ø¯ÛŒØ¯ Ø´Ø±Ø§ÛŒØ· Ø¯Ù‚ÛŒÙ‚ Ú©Ø¯ ØªÙˆØ¶ÛŒØ­ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯
- Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯

---

#### Ù…Ø´Ú©Ù„ 3: Stochastic Ø´Ø±Ø§ÛŒØ· Ù¾ÛŒÚ†ÛŒØ¯Ù‡â€ŒØªØ± Ø§Ø² Ù…Ø³ØªÙ†Ø¯Ø§Øª â“

**ØªÙˆØ¶ÛŒØ­ Ù…Ø´Ú©Ù„:**

Ú©Ø¯ ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø±Ø§ÛŒ Stochastic cross Ú†Ù‡Ø§Ø± Ø´Ø±Ø· Ø¯Ø§Ø±Ø¯:
```python
if curr_k < 20 and curr_d < 20 and curr_k > curr_d and prev_k <= prev_d:
```

Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù‚Ø¯ÛŒÙ…ÛŒ ÙÙ‚Ø· Ù…ÛŒâ€ŒÚ¯ÙØª: "Stochastic Cross Ø¯Ø± oversold"

**Ø´Ø±Ø§ÛŒØ· Ú©Ø§Ù…Ù„:**

1. `curr_k < 20` â†’ K Ø¯Ø± Ù†Ø§Ø­ÛŒÙ‡ oversold
2. `curr_d < 20` â†’ D Ø¯Ø± Ù†Ø§Ø­ÛŒÙ‡ oversold
3. `curr_k > curr_d` â†’ Ø§Ù„Ø§Ù† K Ø¨Ø§Ù„Ø§ÛŒ D Ø§Ø³Øª
4. `prev_k <= prev_d` â†’ Ù‚Ø¨Ù„Ø§Ù‹ K Ù¾Ø§ÛŒÛŒÙ† D Ø¨ÙˆØ¯

**Ú†Ø±Ø§ Ù‡Ù…Ù‡ Ø§ÛŒÙ† Ø´Ø±Ø§ÛŒØ· Ù„Ø§Ø²Ù… Ø§Ø³Øª:**

```python
# Ø³Ù†Ø§Ø±ÛŒÙˆ 1: ÙÙ‚Ø· Ø´Ø±Ø· 1 Ùˆ 2 âŒ
curr_k = 15, curr_d = 12  # Ù‡Ø± Ø¯Ùˆ oversold Ø§Ù…Ø§ K > D Ø§Ø² Ù‚Ø¨Ù„
prev_k = 18, prev_d = 14
# ØªÙ‚Ø§Ø·Ø¹ Ø¬Ø¯ÛŒØ¯ÛŒ Ù†Ø¯Ø§Ø±ÛŒÙ…! Ø³ÛŒÚ¯Ù†Ø§Ù„ false Ø§Ø³Øª

# Ø³Ù†Ø§Ø±ÛŒÙˆ 2: Ù‡Ù…Ù‡ Ø´Ø±Ø§ÛŒØ· âœ…
curr_k = 18, curr_d = 15  # K Ø¹Ø¨ÙˆØ± Ú©Ø±Ø¯
prev_k = 12, prev_d = 20
# ØªÙ‚Ø§Ø·Ø¹ ÙˆØ§Ù‚Ø¹ÛŒ! Ø³ÛŒÚ¯Ù†Ø§Ù„ Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª
```

**ØªØ£Ø«ÛŒØ±:**
- **âœ… Ø­Ù„ Ø´Ø¯:** Ø´Ø±Ø§ÛŒØ· Ø¯Ù‚ÛŒÙ‚ Ùˆ Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒ Ø¯Ø± Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¬Ø¯ÛŒØ¯ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯

---

#### Ù…Ø´Ú©Ù„ 4: MFI Ø¯Ø± Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù‚Ø¯ÛŒÙ…ÛŒ Ø°Ú©Ø± Ù†Ø´Ø¯Ù‡ Ø¨ÙˆØ¯ ğŸ“Š

**ØªÙˆØ¶ÛŒØ­ Ù…Ø´Ú©Ù„:**

Ø¯Ø± Ú©Ø¯ (signal_generator.py:3549-3644):
```python
# MFI Ù…Ø­Ø§Ø³Ø¨Ù‡ Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
mfi = talib.MFI(high, low, close, volume, timeperiod=14)
if curr_mfi < 20 and curr_mfi > prev_mfi:
    momentum_signals.append({
        'type': 'mfi_oversold_reversal',
        'score': 2.4
    })
```

Ø§Ù…Ø§ Ø¯Ø± Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù‚Ø¯ÛŒÙ…ÛŒ MFI Ø°Ú©Ø± Ù†Ø´Ø¯Ù‡ Ø¨ÙˆØ¯!

**Ú†Ø±Ø§ MFI Ù…Ù‡Ù… Ø§Ø³Øª:**

- **RSI:** ÙÙ‚Ø· Ù‚ÛŒÙ…Øª Ø±Ø§ Ù…ÛŒâ€ŒØ¨ÛŒÙ†Ø¯
- **MFI:** Ù‚ÛŒÙ…Øª + Ø­Ø¬Ù… Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø±Ø§ ØªØ±Ú©ÛŒØ¨ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
- **MFI Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ø§Ø³Øª:** Ú†ÙˆÙ† Ø­Ø¬Ù… Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø´Ø¯Øª Ø®Ø±ÛŒØ¯/ÙØ±ÙˆØ´ Ø±Ø§ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯

**Ù…Ø«Ø§Ù„:**

```python
# Ø³Ù†Ø§Ø±ÛŒÙˆ: Ù‚ÛŒÙ…Øª Ø¯Ø± Ø­Ø§Ù„ Ø³Ù‚ÙˆØ·
# RSI = 25 (oversold)
# MFI = 35 (Ù†Ù‡ oversold)

# ØªØ­Ù„ÛŒÙ„:
# - RSI Ù…ÛŒâ€ŒÚ¯ÙˆÛŒØ¯ oversold Ø§Ø³Øª
# - MFI Ù…ÛŒâ€ŒÚ¯ÙˆÛŒØ¯ Ù‡Ù†ÙˆØ² Ø­Ø¬Ù… ÙØ±ÙˆØ´ Ø¨Ø§Ù„Ø§ Ù†ÛŒØ³Øª
# - Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ Ù‡Ù†ÙˆØ² Ø³Ù‚ÙˆØ· Ø§Ø¯Ø§Ù…Ù‡ Ø¯Ø§Ø±Ø¯ (MFI Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ø§Ø³Øª)
```

**ØªØ£Ø«ÛŒØ±:**
- **âœ… Ø­Ù„ Ø´Ø¯:** MFI Ø¨Ù‡ Ø·ÙˆØ± Ú©Ø§Ù…Ù„ Ø¯Ø± Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¬Ø¯ÛŒØ¯ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
- ØªÙØ§ÙˆØª MFI Ùˆ RSI ØªÙˆØ¶ÛŒØ­ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯

---

#### Ù…Ø´Ú©Ù„ 5: Ø¬Ø²Ø¦ÛŒØ§Øª ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ù…Ø¨Ù‡Ù… Ø¨ÙˆØ¯ ğŸ”

**ØªÙˆØ¶ÛŒØ­ Ù…Ø´Ú©Ù„:**

Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù‚Ø¯ÛŒÙ…ÛŒ ÙÙ‚Ø· Ù…ÛŒâ€ŒÚ¯ÙØª: "ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ RSI â†’ +15 ØªØ§ +20 Ø§Ù…ØªÛŒØ§Ø²"

Ø§Ù…Ø§ Ø³Ø¤Ø§Ù„Ø§Øª Ø¨Ø¯ÙˆÙ† Ù¾Ø§Ø³Ø®:
- ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ú†Ú¯ÙˆÙ†Ù‡ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŸ
- strength Ú†Ú¯ÙˆÙ†Ù‡ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŸ
- Ú†Ø±Ø§ Ú¯Ø§Ù‡ÛŒ Ø§Ù…ØªÛŒØ§Ø² 2.1 Ùˆ Ú¯Ø§Ù‡ÛŒ 3.5 Ø§Ø³ØªØŸ
- ØªÙØ§ÙˆØª ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ù…Ø¹Ù…ÙˆÙ„ÛŒ Ùˆ hidden divergence Ú†ÛŒØ³ØªØŸ

**Ù…Ø­Ø§Ø³Ø¨Ù‡ Strength Ø¯Ø± Ú©Ø¯:**

```python
# signal_generator.py:2969-2971
price_change_pct = (p2_price - p1_price) / p1_price
ind_change_pct = (ind_p1_val - ind_p2_val) / ind_p1_val
div_strength = min(1.0, (price_change_pct + ind_change_pct) / 2 * 5)

# Ø§Ù…ØªÛŒØ§Ø² Ù†Ù‡Ø§ÛŒÛŒ
div_score = 3.5 * div_strength  # 0 ØªØ§ 3.5
```

**Ù…Ø«Ø§Ù„:**
```python
# ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ø¶Ø¹ÛŒÙ:
price_change = 2%, ind_change = 3%
strength = min(1.0, (0.02 + 0.03) / 2 * 5) = 0.125
score = 3.5 Ã— 0.125 = 0.44

# ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ù‚ÙˆÛŒ:
price_change = 10%, ind_change = 12%
strength = min(1.0, (0.10 + 0.12) / 2 * 5) = 0.55
score = 3.5 Ã— 0.55 = 1.93
```

**ØªØ£Ø«ÛŒØ±:**
- **âœ… Ø­Ù„ Ø´Ø¯:** ÙØ±Ø¢ÛŒÙ†Ø¯ Ú©Ø§Ù…Ù„ ØªØ´Ø®ÛŒØµ ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ø¯Ø± Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¬Ø¯ÛŒØ¯ ØªÙˆØ¶ÛŒØ­ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯
- Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø§ Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø¯Ù‚ÛŒÙ‚ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯

**âš ï¸ Ù†Ú©ØªÙ‡:** Ú©Ø¯ ÙØ¹Ù„ÛŒ ÙÙ‚Ø· ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ù…Ø¹Ù…ÙˆÙ„ÛŒ (Regular Divergence) Ø±Ø§ ØªØ´Ø®ÛŒØµ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ØŒ Ù†Ù‡ Hidden Divergence

---

#### Ù…Ø´Ú©Ù„ 6: Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ú©Ø§Ù…Ù„ ğŸ“

**ØªÙˆØ¶ÛŒØ­ Ù…Ø´Ú©Ù„:**

Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù‚Ø¯ÛŒÙ…ÛŒ ÙÙ‚Ø· ØªÙˆØ¶ÛŒØ­Ø§Øª Ù†Ø¸Ø±ÛŒ Ø¯Ø§Ø´ØªØŒ Ø¨Ø¯ÙˆÙ† Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ.

**Ú†Ø±Ø§ Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ù…Ù‡Ù… Ù‡Ø³ØªÙ†Ø¯:**

1. **Ø¯Ø±Ú© Ø¨Ù‡ØªØ±:** ØªÙˆØ³Ø¹Ù‡â€ŒØ¯Ù‡Ù†Ø¯Ù‡ Ù…ÛŒâ€ŒÙÙ‡Ù…Ø¯ Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ú†Ù‡ Ø¹Ø¯Ø¯ÛŒ Ø§Ø² Ú©Ø¬Ø§ Ù…ÛŒâ€ŒØ¢ÛŒØ¯
2. **ØªØ³Øª Ø¢Ø³Ø§Ù†â€ŒØªØ±:** Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ø®Øµ Ú©Ø¯ Ø±Ø§ ØªØ³Øª Ú©Ø±Ø¯
3. **Debug Ø³Ø±ÛŒØ¹â€ŒØªØ±:** ÙˆÙ‚ØªÛŒ Ù…Ø´Ú©Ù„ÛŒ Ù¾ÛŒØ´ Ù…ÛŒâ€ŒØ¢ÛŒØ¯ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø¨Ø§ Ù…Ø«Ø§Ù„ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ø±Ø¯

**ØªØ£Ø«ÛŒØ±:**
- **âœ… Ø­Ù„ Ø´Ø¯:** Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ú©Ø§Ù…Ù„ Ø¯Ø± Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¬Ø¯ÛŒØ¯ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯:
  - Ù…Ø«Ø§Ù„ RSI reversal
  - Ù…Ø«Ø§Ù„ Stochastic cross
  - Ù…Ø«Ø§Ù„ Ù…Ø­Ø§Ø³Ø¨Ù‡ divergence strength
  - Ù…Ø«Ø§Ù„ Ú©Ø§Ù…Ù„ Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø®Ø±ÛŒØ¯ Ø¨Ø§ momentum

---

### ğŸ’¡ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¨Ù‡Ø¨ÙˆØ¯

#### Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ 1: Ø§ÙØ²ÙˆØ¯Ù† Hidden Divergence Detection ğŸ¯

**Ù‡Ø¯Ù:** ØªØ´Ø®ÛŒØµ ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ù…Ø®ÙÛŒ (Hidden Divergence) Ú©Ù‡ Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡ Ø§Ø¯Ø§Ù…Ù‡ Ø±ÙˆÙ†Ø¯ Ø§Ø³Øª

**ØªÙØ§ÙˆØª Regular vs Hidden Divergence:**

| Ù†ÙˆØ¹ | Ù‚ÛŒÙ…Øª | Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± | Ù…Ø¹Ù†ÛŒ |
|-----|------|-----------|------|
| **Regular Bullish** | Lower Lows (LL) | Higher Lows (HL) | Ø¨Ø§Ø²Ú¯Ø´Øª Ø±ÙˆÙ†Ø¯ â†—ï¸ |
| **Hidden Bullish** | Higher Lows (HL) | Lower Lows (LL) | Ø§Ø¯Ø§Ù…Ù‡ Ø±ÙˆÙ†Ø¯ ØµØ¹ÙˆØ¯ÛŒ âœ… |
| **Regular Bearish** | Higher Highs (HH) | Lower Highs (LH) | Ø¨Ø§Ø²Ú¯Ø´Øª Ø±ÙˆÙ†Ø¯ â†˜ï¸ |
| **Hidden Bearish** | Lower Highs (LH) | Higher Highs (HH) | Ø§Ø¯Ø§Ù…Ù‡ Ø±ÙˆÙ†Ø¯ Ù†Ø²ÙˆÙ„ÛŒ âœ… |

**Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ú©Ø¯ Ø¬Ø¯ÛŒØ¯:**

```python
def _detect_hidden_divergence(self, price_series: pd.Series, indicator_series: pd.Series,
                               indicator_name: str, trend_direction: str) -> List[Dict[str, Any]]:
    """
    ØªØ´Ø®ÛŒØµ ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ù…Ø®ÙÛŒ (Hidden Divergence)

    Hidden Divergence Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡ Ø§Ø¯Ø§Ù…Ù‡ Ø±ÙˆÙ†Ø¯ Ø§Ø³ØªØŒ Ù†Ù‡ Ø¨Ø§Ø²Ú¯Ø´Øª Ø±ÙˆÙ†Ø¯.
    ÙÙ‚Ø· Ø¯Ø± Ø¬Ù‡Øª Ø±ÙˆÙ†Ø¯ Ø§ØµÙ„ÛŒ Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª.
    """
    signals = []

    if trend_direction not in ['bullish', 'bearish']:
        return signals  # Hidden divergence ÙÙ‚Ø· Ø¯Ø± Ø±ÙˆÙ†Ø¯ Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª

    period = min(len(price_series), len(indicator_series))
    if period < 20:
        return signals

    try:
        price_window = price_series.iloc[-period:]
        indicator_window = indicator_series.iloc[-period:]

        # ÛŒØ§ÙØªÙ† peaks Ùˆ valleys
        price_peaks_idx, price_valleys_idx = self.find_peaks_and_valleys(
            price_window.values, distance=5, prominence_factor=0.05, window_size=period
        )
        ind_peaks_idx, ind_valleys_idx = self.find_peaks_and_valleys(
            indicator_window.values, distance=5, prominence_factor=0.1, window_size=period
        )

        # Convert to absolute indices
        price_peaks_abs = price_window.index[price_peaks_idx].tolist() if len(price_peaks_idx) > 0 else []
        price_valleys_abs = price_window.index[price_valleys_idx].tolist() if len(price_valleys_idx) > 0 else []
        ind_peaks_abs = indicator_window.index[ind_peaks_idx].tolist() if len(ind_peaks_idx) > 0 else []
        ind_valleys_abs = indicator_window.index[ind_valleys_idx].tolist() if len(ind_valleys_idx) > 0 else []

        # ================ HIDDEN BULLISH DIVERGENCE (Ø¯Ø± Ø±ÙˆÙ†Ø¯ ØµØ¹ÙˆØ¯ÛŒ) ================
        # Ø´Ø±Ø·: Ù‚ÛŒÙ…Øª Higher Lows Ø§Ù…Ø§ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± Lower Lows
        if trend_direction == 'bullish' and len(price_valleys_abs) >= 2 and len(ind_valleys_abs) >= 2:
            for i in range(min(len(price_valleys_abs), 5) - 1):
                cur_idx = len(price_valleys_abs) - 1 - i
                prev_idx = cur_idx - 1

                if prev_idx < 0:
                    continue

                p1_idx = price_valleys_abs[prev_idx]
                p2_idx = price_valleys_abs[cur_idx]

                p1_price = price_window.loc[p1_idx]
                p2_price = price_window.loc[p2_idx]

                # Ù‚ÛŒÙ…Øª Ø¨Ø§ÛŒØ¯ Higher Low Ø¨Ø§Ø´Ø¯ (pullback Ø¯Ø± Ø±ÙˆÙ†Ø¯ ØµØ¹ÙˆØ¯ÛŒ)
                if p2_price <= p1_price:
                    continue

                # ÛŒØ§ÙØªÙ† valleys Ù…ØªÙ†Ø§Ø¸Ø± Ø¯Ø± Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±
                ind_p1_idx = self._find_closest_peak(ind_valleys_abs, p1_idx)
                ind_p2_idx = self._find_closest_peak(ind_valleys_abs, p2_idx)

                if ind_p1_idx is None or ind_p2_idx is None:
                    continue

                ind_p1_val = indicator_window.loc[ind_p1_idx]
                ind_p2_val = indicator_window.loc[ind_p2_idx]

                # Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± Ø¨Ø§ÛŒØ¯ Lower Low Ø¨Ø§Ø´Ø¯
                if ind_p2_val < ind_p1_val:
                    # âœ… Hidden Bullish Divergence
                    price_change_pct = (p2_price - p1_price) / p1_price
                    ind_change_pct = (ind_p1_val - ind_p2_val) / ind_p1_val
                    div_strength = min(1.0, (price_change_pct + ind_change_pct) / 2 * 5)

                    if div_strength >= self.divergence_sensitivity:
                        # Ø§Ù…ØªÛŒØ§Ø² Ú©Ù…ØªØ± Ø§Ø² regular divergence (Ú†ÙˆÙ† Ø§Ø¯Ø§Ù…Ù‡ Ø±ÙˆÙ†Ø¯ Ø§Ø³Øª Ù†Ù‡ Ø¨Ø§Ø²Ú¯Ø´Øª)
                        div_score = self.pattern_scores.get(
                            f"{indicator_name}_hidden_bullish_divergence", 2.5
                        ) * div_strength

                        signals.append({
                            'type': f'{indicator_name}_hidden_bullish_divergence',
                            'direction': 'bullish',
                            'score': div_score,
                            'strength': float(div_strength),
                            'signal_quality': 'continuation',  # Ø§Ø¯Ø§Ù…Ù‡ Ø±ÙˆÙ†Ø¯
                            'details': {
                                'price_p1': float(p1_price),
                                'price_p2': float(p2_price),
                                'ind_p1': float(ind_p1_val),
                                'ind_p2': float(ind_p2_val),
                                'price_change_pct': float(price_change_pct),
                                'ind_change_pct': float(ind_change_pct)
                            }
                        })

        # ================ HIDDEN BEARISH DIVERGENCE (Ø¯Ø± Ø±ÙˆÙ†Ø¯ Ù†Ø²ÙˆÙ„ÛŒ) ================
        # Ø´Ø±Ø·: Ù‚ÛŒÙ…Øª Lower Highs Ø§Ù…Ø§ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± Higher Highs
        if trend_direction == 'bearish' and len(price_peaks_abs) >= 2 and len(ind_peaks_abs) >= 2:
            for i in range(min(len(price_peaks_abs), 5) - 1):
                cur_idx = len(price_peaks_abs) - 1 - i
                prev_idx = cur_idx - 1

                if prev_idx < 0:
                    continue

                p1_idx = price_peaks_abs[prev_idx]
                p2_idx = price_peaks_abs[cur_idx]

                p1_price = price_window.loc[p1_idx]
                p2_price = price_window.loc[p2_idx]

                # Ù‚ÛŒÙ…Øª Ø¨Ø§ÛŒØ¯ Lower High Ø¨Ø§Ø´Ø¯ (pullback Ø¯Ø± Ø±ÙˆÙ†Ø¯ Ù†Ø²ÙˆÙ„ÛŒ)
                if p2_price >= p1_price:
                    continue

                # ÛŒØ§ÙØªÙ† peaks Ù…ØªÙ†Ø§Ø¸Ø± Ø¯Ø± Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±
                ind_p1_idx = self._find_closest_peak(ind_peaks_abs, p1_idx)
                ind_p2_idx = self._find_closest_peak(ind_peaks_abs, p2_idx)

                if ind_p1_idx is None or ind_p2_idx is None:
                    continue

                ind_p1_val = indicator_window.loc[ind_p1_idx]
                ind_p2_val = indicator_window.loc[ind_p2_idx]

                # Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± Ø¨Ø§ÛŒØ¯ Higher High Ø¨Ø§Ø´Ø¯
                if ind_p2_val > ind_p1_val:
                    # âœ… Hidden Bearish Divergence
                    price_change_pct = (p1_price - p2_price) / p1_price
                    ind_change_pct = (ind_p2_val - ind_p1_val) / ind_p1_val
                    div_strength = min(1.0, (price_change_pct + ind_change_pct) / 2 * 5)

                    if div_strength >= self.divergence_sensitivity:
                        div_score = self.pattern_scores.get(
                            f"{indicator_name}_hidden_bearish_divergence", 2.5
                        ) * div_strength

                        signals.append({
                            'type': f'{indicator_name}_hidden_bearish_divergence',
                            'direction': 'bearish',
                            'score': div_score,
                            'strength': float(div_strength),
                            'signal_quality': 'continuation',
                            'details': {
                                'price_p1': float(p1_price),
                                'price_p2': float(p2_price),
                                'ind_p1': float(ind_p1_val),
                                'ind_p2': float(ind_p2_val),
                                'price_change_pct': float(price_change_pct),
                                'ind_change_pct': float(ind_change_pct)
                            }
                        })

        # ÙÛŒÙ„ØªØ± Ø²Ù…Ø§Ù†ÛŒ
        recent_candle_limit = 10
        if len(signals) > 0 and len(price_window) > recent_candle_limit:
            recent_threshold = price_window.index[-recent_candle_limit]
            signals = [s for s in signals if s['index'] >= recent_threshold]

        return sorted(signals, key=lambda x: x.get('strength', 0), reverse=True)

    except Exception as e:
        logger.error(f"Error detecting hidden {indicator_name} divergence: {str(e)}", exc_info=True)
        return []

# Ø§Ø³ØªÙØ§Ø¯Ù‡ - Ú¯Ø²ÛŒÙ†Ù‡ 1: Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù¾Ø§Ø±Ø§Ù…ØªØ± trend Ø¨Ù‡ analyze_momentum_indicators:
def analyze_momentum_indicators(self, df: pd.DataFrame, trend_data: Optional[Dict] = None) -> Dict[str, Any]:
    # ... Ú©Ø¯ Ù‚Ø¨Ù„ÛŒ ...

    # ØªØ´Ø®ÛŒØµ regular divergence
    rsi_divergences = self._detect_divergence_generic(close_s, rsi_s, 'rsi')
    momentum_signals.extend(rsi_divergences)

    # ØªØ´Ø®ÛŒØµ hidden divergence (Ø¬Ø¯ÛŒØ¯) - ÙÙ‚Ø· Ø§Ú¯Ø± trend data Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯
    if trend_data:
        trend_direction = trend_data.get('trend', 'neutral')
        rsi_hidden_divs = self._detect_hidden_divergence(close_s, rsi_s, 'rsi', trend_direction)
        momentum_signals.extend(rsi_hidden_divs)

    # ... Ø§Ø¯Ø§Ù…Ù‡ Ú©Ø¯ ...

# Ø³Ù¾Ø³ Ø¯Ø± analyze_single_timeframe:
# ØªØºÛŒÛŒØ± line 4693 Ø§Ø²:
# analysis_data['momentum'] = self.analyze_momentum_indicators(df)
# Ø¨Ù‡:
# analysis_data['momentum'] = self.analyze_momentum_indicators(df, analysis_data.get('trend'))

# Ø§Ø³ØªÙØ§Ø¯Ù‡ - Ú¯Ø²ÛŒÙ†Ù‡ 2 (Ø¨Ù‡ØªØ±): Ø§Ø¬Ø±Ø§ Ù…Ø³ØªÙ‚ÛŒÙ… Ø¯Ø± analyze_single_timeframe Ø¨Ø¹Ø¯ Ø§Ø² momentum:
async def analyze_single_timeframe(self, symbol: str, timeframe: str, df: pd.DataFrame):
    # ... Ú©Ø¯ Ù‚Ø¨Ù„ÛŒ ØªØ§ line 4693 ...

    # 2. Analyze momentum
    analysis_data['momentum'] = self.analyze_momentum_indicators(df)

    # 2b. Detect hidden divergences (Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² trend data)
    if analysis_data.get('trend'):
        trend_direction = analysis_data['trend'].get('trend', 'neutral')
        close_s = pd.Series(df['close'].values)
        rsi_s = pd.Series(talib.RSI(df['close'].values, timeperiod=14))

        rsi_hidden_divs = self._detect_hidden_divergence(close_s, rsi_s, 'rsi', trend_direction)
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ momentum signals
        analysis_data['momentum']['signals'].extend(rsi_hidden_divs)
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ scores
        for sig in rsi_hidden_divs:
            if 'bullish' in sig.get('direction', ''):
                analysis_data['momentum']['bullish_score'] += sig['score']
            elif 'bearish' in sig.get('direction', ''):
                analysis_data['momentum']['bearish_score'] += sig['score']

    # ... Ø§Ø¯Ø§Ù…Ù‡ Ú©Ø¯ ...
```

**Ù…Ø²Ø§ÛŒØ§:**

- âœ… ØªØ´Ø®ÛŒØµ ÙØ±ØµØªâ€ŒÙ‡Ø§ÛŒ Ø§Ø¯Ø§Ù…Ù‡ Ø±ÙˆÙ†Ø¯ (pullback Ø¯Ø± Ø±ÙˆÙ†Ø¯ Ù‚ÙˆÛŒ)
- âœ… Ú©Ø§Ù‡Ø´ false reversals
- âœ… Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‚Øª Ø¯Ø± Ø±ÙˆÙ†Ø¯Ù‡Ø§ÛŒ Ù‚ÙˆÛŒ
- âœ… Ø§Ù…ØªÛŒØ§Ø² Ù…ØªÙØ§ÙˆØª Ø¨Ø±Ø§ÛŒ divergence types (regular vs hidden)

**Ú©Ø§Ø±Ø¨Ø±Ø¯:**

```python
# Ù…Ø«Ø§Ù„: Ø±ÙˆÙ†Ø¯ ØµØ¹ÙˆØ¯ÛŒ Ù‚ÙˆÛŒ BTC
# Ù‚ÛŒÙ…Øª: pullback Ù…ÙˆÙ‚Øª (Higher Low)
# RSI: Ú©Ù Ø¬Ø¯ÛŒØ¯ (Lower Low)
# = Hidden Bullish Divergence
# Ù…Ø¹Ù†ÛŒ: Ø±ÙˆÙ†Ø¯ ØµØ¹ÙˆØ¯ÛŒ Ø§Ø¯Ø§Ù…Ù‡ Ø¯Ø§Ø±Ø¯ØŒ ÙØ±ØµØª Ø®ÙˆØ¨ Ø¨Ø±Ø§ÛŒ Ø®Ø±ÛŒØ¯ âœ…

# Regular Divergence Ù…ÛŒâ€ŒÚ¯ÙØª: Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø±ÙˆÙ†Ø¯ Ù…Ø¹Ú©ÙˆØ³ Ø´ÙˆØ¯ âŒ
# Hidden Divergence Ù…ÛŒâ€ŒÚ¯ÙˆÛŒØ¯: Ø±ÙˆÙ†Ø¯ Ø§Ø¯Ø§Ù…Ù‡ Ø¯Ø§Ø±Ø¯ØŒ pullback ÙØ±ØµØª Ø§Ø³Øª âœ…
```

---

#### Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ 2: Ø§ÙØ²ÙˆØ¯Ù† Divergence Confidence Score ğŸ¯

**Ù‡Ø¯Ù:** ØªØ´Ø®ÛŒØµ Ú©ÛŒÙÛŒØª ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ø¨Ø§ confidence score

**Ù…Ø´Ú©Ù„ ÙØ¹Ù„ÛŒ:**

Ù‡Ù…Ù‡ ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒâ€ŒÙ‡Ø§ ÛŒÚ© Ú©ÛŒÙÛŒØª ÛŒÚ©Ø³Ø§Ù† Ù†Ø¯Ø§Ø±Ù†Ø¯:
- ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ø¯Ø± timeframe Ø¨Ø§Ù„Ø§ Ù‚ÙˆÛŒâ€ŒØªØ± Ø§Ø³Øª
- ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ø¯Ø± Ù†Ø§Ø­ÛŒÙ‡ S/R Ù…Ù‡Ù…â€ŒØªØ± Ø§Ø³Øª
- ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ù‡Ù…Ø±Ø§Ù‡ Ø¨Ø§ volume Ø¨Ø§Ù„Ø§ Ù…Ø¹ØªØ¨Ø±ØªØ± Ø§Ø³Øª

**Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ú©Ø¯ Ø¬Ø¯ÛŒØ¯:**

**Ù†Ú©ØªÙ‡ Ù…Ù‡Ù…:** Ø§ÛŒÙ† Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø¨Ù‡ Ø¯Ùˆ ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ù†ÛŒØ§Ø² Ø¯Ø§Ø±Ø¯ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ø§Ø¨ØªØ¯Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø´ÙˆÙ†Ø¯:

```python
# ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ 1: Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø²Ø¯ÛŒÚ©ÛŒ Ø¨Ù‡ Ø³Ø·ÙˆØ­ S/R
def _is_near_support_resistance(self, price: float, sr_levels: Dict, threshold_pct: float = 0.015) -> bool:
    """
    Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø²Ø¯ÛŒÚ©ÛŒ Ù‚ÛŒÙ…Øª Ø¨Ù‡ Ø³Ø·ÙˆØ­ support/resistance

    Args:
        price: Ù‚ÛŒÙ…Øª ÙØ¹Ù„ÛŒ
        sr_levels: dict Ø­Ø§ÙˆÛŒ 'support' Ùˆ 'resistance' lists
        threshold_pct: Ø¯Ø±ØµØ¯ Ø¢Ø³ØªØ§Ù†Ù‡ Ù†Ø²Ø¯ÛŒÚ©ÛŒ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶ 1.5%)

    Returns:
        True Ø§Ú¯Ø± Ù‚ÛŒÙ…Øª Ù†Ø²Ø¯ÛŒÚ© Ø³Ø·Ø­ S/R Ø¨Ø§Ø´Ø¯
    """
    supports = sr_levels.get('support', [])
    resistances = sr_levels.get('resistance', [])

    all_levels = supports + resistances

    for level in all_levels:
        if abs(price - level) / level <= threshold_pct:
            return True

    return False

# ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ 2: Ù…Ø­Ø§Ø³Ø¨Ù‡ confidence Ø¨Ø±Ø§ÛŒ ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ
def _calculate_divergence_confidence(self, div_details: Dict,
                                      timeframe: str,
                                      current_price: float,
                                      sr_levels: Dict,
                                      volume_series: Optional[pd.Series] = None,
                                      volume_window: int = 20) -> float:
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ confidence Ø¨Ø±Ø§ÛŒ ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ

    Args:
        div_details: Ø¬Ø²Ø¦ÛŒØ§Øª divergence Ø§Ø² _detect_divergence_generic
        timeframe: ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ… ÙØ¹Ù„ÛŒ
        current_price: Ù‚ÛŒÙ…Øª ÙØ¹Ù„ÛŒ
        sr_levels: Ø³Ø·ÙˆØ­ support/resistance Ø§Ø² detect_support_resistance
        volume_series: Ø³Ø±ÛŒ volume (optional)
        volume_window: ÙˆÛŒÙ†Ø¯Ùˆ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† volume

    Returns:
        0.0-1.0: confidence score
    """
    confidence = 0.0

    # 1. Ù‚Ø¯Ø±Øª divergence (40% ÙˆØ²Ù†)
    strength = div_details.get('strength', 0)
    confidence += min(0.4, strength * 0.4)

    # 2. ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ… (25% ÙˆØ²Ù†)
    timeframe_weights = {
        '5m': 0.15,
        '15m': 0.18,
        '1h': 0.22,
        '4h': 0.25   # Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ø§Ø¹ØªØ¨Ø§Ø±
    }
    confidence += timeframe_weights.get(timeframe, 0.18)

    # 3. Ù†Ø²Ø¯ÛŒÚ©ÛŒ Ø¨Ù‡ Ø³Ø·Ø­ S/R (20% ÙˆØ²Ù†)
    at_sr = self._is_near_support_resistance(current_price, sr_levels)
    if at_sr:
        confidence += 0.20

    # 4. ØªØ£ÛŒÛŒØ¯ Ø­Ø¬Ù… (15% ÙˆØ²Ù†)
    if volume_series is not None and len(volume_series) > volume_window:
        try:
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø­Ø¬Ù… Ø¯Ø± Ø¢Ø®Ø±ÛŒÙ† Ú©Ù†Ø¯Ù„â€ŒÙ‡Ø§
            recent_volume = volume_series.iloc[-5:].mean()  # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† 5 Ú©Ù†Ø¯Ù„ Ø§Ø®ÛŒØ±
            avg_volume = volume_series.rolling(volume_window).mean().iloc[-1]

            # Ø§Ú¯Ø± Ø­Ø¬Ù… Ø§Ø®ÛŒØ± Ø¨ÛŒØ´ØªØ± Ø§Ø² Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø¨Ø§Ø´Ø¯
            if recent_volume > avg_volume * 1.5:
                confidence += 0.15
            elif recent_volume > avg_volume:
                confidence += 0.08
        except Exception as e:
            logger.debug(f"Error calculating volume confidence: {e}")
            pass

    return min(1.0, confidence)

# Ø§Ø³ØªÙØ§Ø¯Ù‡: Ø¨Ø§ÛŒØ¯ Ø¯Ø± analyze_single_timeframe Ø¨Ø¹Ø¯ Ø§Ø² momentum analysis Ø§Ø¬Ø±Ø§ Ø´ÙˆØ¯
async def analyze_single_timeframe(self, symbol: str, timeframe: str, df: pd.DataFrame):
    # ... Ú©Ø¯ Ù‚Ø¨Ù„ÛŒ ØªØ§ line 4710 ...

    # 6. Detect support/resistance
    analysis_data['support_resistance'] = self.detect_support_resistance(df)

    # ... Ø¨Ù‚ÛŒÙ‡ analyses ...

    # Ø¨Ø¹Ø¯ Ø§Ø² ØªÙ…Ø§Ù… analysesØŒ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† confidence Ø¨Ù‡ divergence signals:
    if analysis_data.get('momentum') and analysis_data.get('support_resistance'):
        sr_levels = analysis_data['support_resistance'].get('details', {})
        current_price = df['close'].iloc[-1]
        volume_series = df.get('volume') if 'volume' in df.columns else None

        for signal in analysis_data['momentum']['signals']:
            # ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ divergence signals
            if 'divergence' in signal.get('type', ''):
                confidence = self._calculate_divergence_confidence(
                    signal,
                    timeframe,
                    current_price,
                    sr_levels,
                    volume_series
                )
                signal['confidence'] = confidence

                # ØªÙ†Ø¸ÛŒÙ… Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø±Ø§Ø³Ø§Ø³ confidence
                if confidence < 0.5:
                    signal['score'] *= 0.7
                elif confidence > 0.8:
                    signal['score'] *= 1.2

        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ bullish/bearish scores
        bullish_score = sum(s['score'] for s in analysis_data['momentum']['signals']
                          if 'bullish' in s.get('direction', s.get('type', '')))
        bearish_score = sum(s['score'] for s in analysis_data['momentum']['signals']
                          if 'bearish' in s.get('direction', s.get('type', '')))

        analysis_data['momentum']['bullish_score'] = round(bullish_score, 2)
        analysis_data['momentum']['bearish_score'] = round(bearish_score, 2)

    # ... Ø§Ø¯Ø§Ù…Ù‡ Ú©Ø¯ ...
```

**Ù…Ø²Ø§ÛŒØ§:**

- âœ… ÙÛŒÙ„ØªØ± ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒâ€ŒÙ‡Ø§ÛŒ Ø¶Ø¹ÛŒÙ
- âœ… Ø§ÙˆÙ„ÙˆÛŒØªâ€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ù‡ØªØ± Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§
- âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² context Ø¨ÛŒØ´ØªØ± (timeframe, S/R, volume)
- âœ… Ú©Ø§Ù‡Ø´ false positives

---

#### Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ 3: Ø§ÙØ²ÙˆØ¯Ù† MACD Histogram Divergence ğŸ“Š

**Ù‡Ø¯Ù:** ØªØ´Ø®ÛŒØµ ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ø¯Ø± MACD Histogram (Ø³Ø±ÛŒØ¹â€ŒØªØ± Ø§Ø² MACD Line)

**Ú†Ø±Ø§ MACD Histogram Ù…Ù‡Ù… Ø§Ø³Øª:**

- MACD Line ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ø±Ø§ Ø¨Ø§ ØªØ£Ø®ÛŒØ± Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯
- MACD Histogram ØªØºÛŒÛŒØ±Ø§Øª momentum Ø±Ø§ Ø²ÙˆØ¯ØªØ± Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯
- Histogram divergence = early warning signal

**Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ú©Ø¯ Ø¬Ø¯ÛŒØ¯:**

```python
# Ø¯Ø± analyze_momentum_indicators:

# Ù…Ø­Ø§Ø³Ø¨Ù‡ MACD histogram divergence
macd_hist_s = pd.Series(macd_hist)
macd_hist_divergences = self._detect_divergence_generic(close_s, macd_hist_s, 'macd_histogram')
momentum_signals.extend(macd_hist_divergences)
```

**ØªÙØ§ÙˆØª Ø¨Ø§ MACD Line Divergence:**

```python
# Ø³Ù†Ø§Ø±ÛŒÙˆ: Ø±ÙˆÙ†Ø¯ ØµØ¹ÙˆØ¯ÛŒ Ù†Ø²Ø¯ÛŒÚ© Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù†

# MACD Line: Ù‡Ù†ÙˆØ² divergence Ù†Ø¯Ø§Ø±Ø¯
# MACD Histogram: Ø´Ø±ÙˆØ¹ Ø¨Ù‡ Ú©Ø§Ù‡Ø´ Ú©Ø±Ø¯Ù‡ (early warning âœ…)

# Ù†ØªÛŒØ¬Ù‡:
# - Histogram Divergence = Ø§Ù…ØªÛŒØ§Ø² 2.5 Ã— strength
# - Ø²ÙˆØ¯ØªØ± Ø§Ø² MACD Line ØªØ´Ø®ÛŒØµ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯
# - Ù…ÙˆÙ‚Ø¹ÛŒØª Ø¨Ù‡ØªØ± Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ ÛŒØ§ ÙØ±ÙˆØ´
```

**Ø§Ù…ØªÛŒØ§Ø² Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**

```python
# config.yaml
macd_histogram_bullish_divergence: 2.8  # Ø¨Ø§Ù„Ø§ØªØ± Ø§Ø² RSI (Ú†ÙˆÙ† Ø²ÙˆØ¯ØªØ± ØªØ´Ø®ÛŒØµ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯)
macd_histogram_bearish_divergence: 2.8
```

---

#### Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ 4: Ø¨Ù‡Ø¨ÙˆØ¯ Ø´Ø±Ø· RSI Extreme Levels âš¡

**Ù‡Ø¯Ù:** Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø³Ø·ÙˆØ­ extreme Ø¨Ø±Ø§ÛŒ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù‚ÙˆÛŒâ€ŒØªØ±

**Ù…Ø´Ú©Ù„ ÙØ¹Ù„ÛŒ:**

ÙÙ‚Ø· Ø¯Ùˆ Ø³Ø·Ø­ Ø¯Ø§Ø±ÛŒÙ…:
- RSI < 30 = oversold
- RSI > 70 = overbought

Ø§Ù…Ø§ Ø³Ø·ÙˆØ­ extreme Ù‚ÙˆÛŒâ€ŒØªØ± Ù‡Ø³ØªÙ†Ø¯:
- RSI < 20 = extremely oversold (Ø®ÛŒÙ„ÛŒ Ù‚ÙˆÛŒâ€ŒØªØ±)
- RSI > 80 = extremely overbought

**Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ú©Ø¯ Ø¬Ø¯ÛŒØ¯:**

```python
# signal_generator.py: Ø¯Ø± analyze_momentum_indicators

# 3. RSI Oversold/Overbought Reversal Ø¨Ø§ Ø³Ø·ÙˆØ­ extreme
if curr_rsi < 20 and curr_rsi > prev_rsi:
    # Extremely oversold - Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø§Ù„Ø§ØªØ±
    momentum_signals.append({
        'type': 'rsi_extremely_oversold_reversal',
        'score': self.pattern_scores.get('rsi_extremely_oversold_reversal', 3.0),  # Ø¨Ø§Ù„Ø§ØªØ± Ø§Ø² 2.3
        'level': 'extreme'
    })
elif curr_rsi < 30 and curr_rsi > prev_rsi:
    # Normal oversold
    momentum_signals.append({
        'type': 'rsi_oversold_reversal',
        'score': self.pattern_scores.get('rsi_oversold_reversal', 2.3),
        'level': 'normal'
    })

# Ù…Ø´Ø§Ø¨Ù‡ Ø¨Ø±Ø§ÛŒ overbought
if curr_rsi > 80 and curr_rsi < prev_rsi:
    momentum_signals.append({
        'type': 'rsi_extremely_overbought_reversal',
        'score': self.pattern_scores.get('rsi_extremely_overbought_reversal', 3.0),
        'level': 'extreme'
    })
elif curr_rsi > 70 and curr_rsi < prev_rsi:
    momentum_signals.append({
        'type': 'rsi_overbought_reversal',
        'score': self.pattern_scores.get('rsi_overbought_reversal', 2.3),
        'level': 'normal'
    })
```

**Ø§Ù…ØªÛŒØ§Ø²Ø§Øª Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**

| Ø³Ø·Ø­ | Ø´Ø±Ø· | Ø§Ù…ØªÛŒØ§Ø² Ù¾Ø§ÛŒÙ‡ | Ø¯Ù„ÛŒÙ„ |
|-----|-----|-----------|------|
| Normal Oversold | RSI 20-30 | 2.3 | Ù…Ø¹Ù…ÙˆÙ„ÛŒ |
| Extreme Oversold | RSI < 20 | **3.0** | Ø®ÛŒÙ„ÛŒ Ù‚ÙˆÛŒ |
| Normal Overbought | RSI 70-80 | 2.3 | Ù…Ø¹Ù…ÙˆÙ„ÛŒ |
| Extreme Overbought | RSI > 80 | **3.0** | Ø®ÛŒÙ„ÛŒ Ù‚ÙˆÛŒ |

**Ù…Ø²Ø§ÛŒØ§:**

- âœ… ØªØ´Ø®ÛŒØµ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù‚ÙˆÛŒâ€ŒØªØ±
- âœ… Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø§Ù„Ø§ØªØ± Ø¨Ø±Ø§ÛŒ extreme levels
- âœ… Ú©Ø§Ù‡Ø´ false signals (Ú†ÙˆÙ† extreme Ù†Ø§Ø¯Ø±ØªØ± Ø§ØªÙØ§Ù‚ Ù…ÛŒâ€ŒØ§ÙØªØ¯)

---

#### Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ 5: Ø§ÙØ²ÙˆØ¯Ù† Multiple Timeframe Momentum Confirmation ğŸ¯

**Ù‡Ø¯Ù:** ØªØ£ÛŒÛŒØ¯ momentum Ø¯Ø± Ú†Ù†Ø¯ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…

**Ù…Ø´Ú©Ù„ ÙØ¹Ù„ÛŒ:**

momentum ÙÙ‚Ø· Ø¯Ø± ÛŒÚ© ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ… Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯.
Ø§Ú¯Ø± Ø¯Ø± Ù‡Ù…Ù‡ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…â€ŒÙ‡Ø§ momentum ÛŒÚ©Ø³Ø§Ù† Ø¨Ø§Ø´Ø¯ â†’ Ø³ÛŒÚ¯Ù†Ø§Ù„ Ù‚ÙˆÛŒâ€ŒØªØ±

**Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ú©Ø¯ Ø¬Ø¯ÛŒØ¯:**

```python
def _calculate_mtf_momentum_score(self, all_timeframes_data: Dict) -> Dict[str, Any]:
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² momentum Ø¯Ø± Ú†Ù†Ø¯ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…

    Args:
        all_timeframes_data: {
            '5m': {'momentum': {...}},
            '15m': {'momentum': {...}},
            '1h': {'momentum': {...}},
            '4h': {'momentum': {...}}
        }

    Returns:
        {
            'mtf_bullish_score': float,
            'mtf_bearish_score': float,
            'mtf_alignment': float (0-1),
            'mtf_momentum_multiplier': float
        }
    """
    timeframe_weights = {
        '5m': 0.15,
        '15m': 0.20,
        '1h': 0.30,
        '4h': 0.35
    }

    weighted_bullish = 0
    weighted_bearish = 0

    for tf, weight in timeframe_weights.items():
        if tf in all_timeframes_data:
            momentum = all_timeframes_data[tf].get('momentum', {})
            bullish = momentum.get('bullish_score', 0)
            bearish = momentum.get('bearish_score', 0)

            weighted_bullish += bullish * weight
            weighted_bearish += bearish * weight

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ alignment
    total = weighted_bullish + weighted_bearish
    if total > 0:
        alignment = abs(weighted_bullish - weighted_bearish) / total
    else:
        alignment = 0

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ multiplier
    if alignment > 0.8:
        multiplier = 1.3  # Ù‡Ù…Ù‡ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…â€ŒÙ‡Ø§ ÛŒÚ©Ø³Ø§Ù†
    elif alignment > 0.6:
        multiplier = 1.15
    elif alignment > 0.4:
        multiplier = 1.0
    else:
        multiplier = 0.85  # ØªØ¶Ø§Ø¯ Ø¨ÛŒÙ† ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…â€ŒÙ‡Ø§

    return {
        'mtf_bullish_score': round(weighted_bullish, 2),
        'mtf_bearish_score': round(weighted_bearish, 2),
        'mtf_alignment': round(alignment, 3),
        'mtf_momentum_multiplier': multiplier
    }

# Ø§Ø³ØªÙØ§Ø¯Ù‡: Ø¨Ø§ÛŒØ¯ Ø¯Ø± calculate_multi_timeframe_score Ø§Ø¶Ø§ÙÙ‡ Ø´ÙˆØ¯
# Ù…Ø­Ù„: signal_generator.py Ø¯Ø± ØªØ§Ø¨Ø¹ calculate_multi_timeframe_score
# Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ analysis_results Ø§Ø² Ù‡Ù…Ù‡ timeframes Ø±Ø§ Ø¯Ø±ÛŒØ§ÙØª Ù…ÛŒâ€ŒÚ©Ù†Ø¯

def calculate_multi_timeframe_score(self, symbol: str,
                                     analysis_results: Dict[str, Dict],
                                     base_signals: Dict[str, Dict],
                                     timeframes_data: Dict[str, pd.DataFrame]) -> Dict[str, Any]:
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø² Ú†Ù†Ø¯ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…

    Args:
        analysis_results: Ù†ØªØ§ÛŒØ¬ ØªØ­Ù„ÛŒÙ„ Ø§Ø² analyze_single_timeframe Ø¨Ø±Ø§ÛŒ Ù‡Ø± timeframe
                         Ù…Ø«Ø§Ù„: {'1h': {'momentum': {...}, 'trend': {...}}, '4h': {...}}
    """
    # ... Ú©Ø¯ Ù‚Ø¨Ù„ÛŒ ...

    # Ø§ÙØ²ÙˆØ¯Ù† MTF Momentum Confirmation
    mtf_momentum = self._calculate_mtf_momentum_score(analysis_results)

    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² multiplier Ø¨Ø±Ø§ÛŒ ØªÙ†Ø¸ÛŒÙ… structure_score
    # structure_score *= mtf_momentum['mtf_momentum_multiplier']

    # ÛŒØ§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Ù†ØªØ§ÛŒØ¬ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ø¬Ø§Ù‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø±:
    score_result['mtf_momentum_alignment'] = mtf_momentum['mtf_alignment']
    score_result['mtf_momentum_multiplier'] = mtf_momentum['mtf_momentum_multiplier']

    # ... Ø§Ø¯Ø§Ù…Ù‡ Ú©Ø¯ ...

    return score_result
```

**Ù…Ø«Ø§Ù„:**

```python
# Ø³Ù†Ø§Ø±ÛŒÙˆ 1: Ù‡Ù…Ù‡ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…â€ŒÙ‡Ø§ bullish momentum
{
    '5m':  {'bullish_score': 8, 'bearish_score': 2},
    '15m': {'bullish_score': 9, 'bearish_score': 1},
    '1h':  {'bullish_score': 7, 'bearish_score': 3},
    '4h':  {'bullish_score': 8, 'bearish_score': 2}
}
# mtf_alignment = 0.85 â†’ multiplier = 1.3 âœ…

# Ø³Ù†Ø§Ø±ÛŒÙˆ 2: ØªØ¶Ø§Ø¯ Ø¨ÛŒÙ† ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…â€ŒÙ‡Ø§
{
    '5m':  {'bullish_score': 8, 'bearish_score': 2},
    '15m': {'bullish_score': 7, 'bearish_score': 3},
    '1h':  {'bullish_score': 3, 'bearish_score': 7},  # Ù…Ø®Ø§Ù„Ù!
    '4h':  {'bullish_score': 2, 'bearish_score': 8}   # Ù…Ø®Ø§Ù„Ù!
}
# mtf_alignment = 0.3 â†’ multiplier = 0.85 âŒ
```

**Ù…Ø²Ø§ÛŒØ§:**

- âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‚Ø¯Ø±Øª momentum Ø¯Ø± Ú†Ù†Ø¯ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…
- âœ… ØªØ£ÛŒÛŒØ¯ Ù‚ÙˆÛŒâ€ŒØªØ± Ø¨Ø§ Ù‡Ù…Ø±Ø§Ø³ØªØ§ÛŒÛŒ
- âœ… Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†Ø§Ù‚Ø¶

---

### ğŸ“Š Ø®Ù„Ø§ØµÙ‡ ØªØ£Ø«ÛŒØ± Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª

| Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ | Ø§ÙˆÙ„ÙˆÛŒØª | ØªØ£Ø«ÛŒØ± Ø¨Ø± Ø¯Ù‚Øª | Ø³Ø®ØªÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ |
|---------|--------|-------------|-----------------|
| Ø§ÙØ²ÙˆØ¯Ù† Hidden Divergence | ğŸŸ¡ Ù…ØªÙˆØ³Ø· | +8-12% | Ù…ØªÙˆØ³Ø· |
| Ø§ÙØ²ÙˆØ¯Ù† Divergence Confidence | ğŸ”´ Ø¨Ø§Ù„Ø§ | +10-15% | Ù…ØªÙˆØ³Ø· |
| MACD Histogram Divergence | ğŸŸ¢ Ù¾Ø§ÛŒÛŒÙ† | +3-5% | Ø¢Ø³Ø§Ù† |
| RSI Extreme Levels | ğŸŸ¡ Ù…ØªÙˆØ³Ø· | +5-8% | Ø¢Ø³Ø§Ù† |
| MTF Momentum Confirmation | ğŸ”´ Ø¨Ø§Ù„Ø§ | +12-18% | Ù…ØªÙˆØ³Ø·-Ø³Ø®Øª |

**ØªÙˆØµÛŒÙ‡:** Ø´Ø±ÙˆØ¹ Ø¨Ø§ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª 2 Ùˆ 5 (Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§Ù„Ø§)

---

### ğŸ§ª Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø¨Ø±Ø§ÛŒ ØªØ³Øª

Ù¾Ø³ Ø§Ø² Ø§Ø¹Ù…Ø§Ù„ ØªØºÛŒÛŒØ±Ø§ØªØŒ Ø¨Ø§ÛŒØ¯ Ù…ÙˆØ§Ø±Ø¯ Ø²ÛŒØ± ØªØ³Øª Ø´ÙˆÙ†Ø¯:

1. **Backtest Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ:**
   - Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù†ØªØ§ÛŒØ¬ Ù‚Ø¨Ù„ Ùˆ Ø¨Ø¹Ø¯ Ø§Ø² ØªØºÛŒÛŒØ±Ø§Øª
   - Ù…Ø­Ø§Ø³Ø¨Ù‡ Win Rate Ø¨Ø±Ø§ÛŒ divergence signals
   - Ø¨Ø±Ø±Ø³ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¯Ø± ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù

2. **ØªØ³Øª Hidden Divergence:**
   - ØªØ³Øª Ø¯Ø± Ø±ÙˆÙ†Ø¯Ù‡Ø§ÛŒ Ù‚ÙˆÛŒ
   - Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ Regular Divergence
   - Ø¨Ø±Ø±Ø³ÛŒ false positives

3. **ØªØ³Øª Confidence Score:**
   - Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ù‚Øª confidence predictions
   - Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ confidence Ø¨Ø§ Ù†ØªÛŒØ¬Ù‡ Ù…Ø¹Ø§Ù…Ù„Ù‡
   - Ø¢Ø³ØªØ§Ù†Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù†

4. **A/B Testing:**
   - Ø§Ø¬Ø±Ø§ÛŒ Ù‡Ù…Ø²Ù…Ø§Ù† Ø³ÛŒØ³ØªÙ… Ù‚Ø¯ÛŒÙ… Ùˆ Ø¬Ø¯ÛŒØ¯
   - Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¯Ø± Ø´Ø±Ø§ÛŒØ· Ø¨Ø§Ø²Ø§Ø± Ù…Ø®ØªÙ„Ù
   - ØªØ­Ù„ÛŒÙ„ Ù†ØªØ§ÛŒØ¬ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù†ÙˆØ¹ divergence

---

## 3. ØªØ­Ù„ÛŒÙ„ Ø­Ø¬Ù… Ù…Ø¹Ø§Ù…Ù„Ø§Øª (Volume Analysis)

**ğŸ“ Ú©Ø¯ Ù…Ø±Ø¬Ø¹:** `signal_generator.py:1658-1717` - ØªØ§Ø¨Ø¹ `analyze_volume_trend()`

### Ù…Ø´Ú©Ù„Ø§Øª Ùˆ Ù…Ø­Ø¯ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ ÙØ¹Ù„ÛŒ

#### âŒ Ù…Ø´Ú©Ù„ 1: Ø¹Ø¯Ù… ØªÙÚ©ÛŒÚ© Ø¨ÛŒÙ† Climax Volume (Ø§ÙˆØ¬) Ùˆ Breakout Volume (Ø´Ú©Ø³Øª)

**Ù…Ø´Ú©Ù„ ÙØ¹Ù„ÛŒ:**
```python
# signal_generator.py:1687-1689
if current_ratio > self.volume_multiplier_threshold * 2.0:
    results['trend'] = 'strongly_increasing'
    results['pattern'] = 'climax_volume'
```

- Ù‡Ù… **Ø­Ø¬Ù… Ø§ÙˆØ¬ (exhaustion)** Ùˆ Ù‡Ù… **Ø­Ø¬Ù… Ø´Ú©Ø³Øª Ø³Ø·Ø­ (breakout)** Ù‡Ø± Ø¯Ùˆ Ø¨Ø§ ratio Ø¨Ø§Ù„Ø§ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
- ØªÙØ§ÙˆØª Ø¨ÛŒÙ† Ø§ÛŒÙ† Ø¯Ùˆ Ù…Ù‡Ù… Ø§Ø³Øª:
  - **Climax Volume:** Ø§ÙˆØ¬ Ø­Ø±Ú©ØªØŒ Ø§Ø­ØªÙ…Ø§Ù„ Ø¨Ø±Ú¯Ø´Øª Ø¨Ø§Ù„Ø§ (Ù…Ù†ÙÛŒ)
  - **Breakout Volume:** Ø´Ú©Ø³Øª Ø³Ø·Ø­ Ù…Ù‡Ù…ØŒ Ø§Ø¯Ø§Ù…Ù‡ Ø­Ø±Ú©Øª (Ù…Ø«Ø¨Øª)

**Ø±Ø§Ù‡ Ø­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**

**Ù†Ú©ØªÙ‡ Ù…Ù‡Ù…:** Ø§ÛŒÙ† Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ùˆ ØªØºÛŒÛŒØ± execution order Ø¯Ø§Ø±Ø¯.

**ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²:**

```python
# ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ 1: Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø²Ø¯ÛŒÚ©ÛŒ Ø¨Ù‡ Ø³Ø·Ø­
def _is_near_level(self, price: float, levels: List[float], threshold_pct: float = 0.02) -> bool:
    """
    Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø²Ø¯ÛŒÚ©ÛŒ Ù‚ÛŒÙ…Øª Ø¨Ù‡ Ø³Ø·ÙˆØ­ (S/R)

    Args:
        price: Ù‚ÛŒÙ…Øª ÙØ¹Ù„ÛŒ
        levels: Ù„ÛŒØ³Øª Ø³Ø·ÙˆØ­ (support ÛŒØ§ resistance)
        threshold_pct: Ø¯Ø±ØµØ¯ Ø¢Ø³ØªØ§Ù†Ù‡ Ù†Ø²Ø¯ÛŒÚ©ÛŒ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶ 2%)

    Returns:
        True Ø§Ú¯Ø± Ù‚ÛŒÙ…Øª Ù†Ø²Ø¯ÛŒÚ© ÛŒÚ©ÛŒ Ø§Ø² Ø³Ø·ÙˆØ­ Ø¨Ø§Ø´Ø¯
    """
    for level in levels:
        if abs(price - level) / level <= threshold_pct:
            return True
    return False

# ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ 2: Ø¨Ø±Ø±Ø³ÛŒ Ù‚Ø¯Ø±Øª momentum
def _check_momentum_strength(self, momentum_data: Dict) -> bool:
    """
    Ø¨Ø±Ø±Ø³ÛŒ Ù‚Ø¯Ø±Øª momentum

    Args:
        momentum_data: Ù†ØªØ§ÛŒØ¬ Ø§Ø² analyze_momentum_indicators

    Returns:
        True Ø§Ú¯Ø± momentum Ù‚ÙˆÛŒ Ø¨Ø§Ø´Ø¯
    """
    if not momentum_data or momentum_data.get('status') != 'ok':
        return False

    bullish_score = momentum_data.get('bullish_score', 0)
    bearish_score = momentum_data.get('bearish_score', 0)

    # momentum Ù‚ÙˆÛŒ: Ø§Ø®ØªÙ„Ø§Ù Ø§Ù…ØªÛŒØ§Ø²Ù‡Ø§ Ø¨ÛŒØ´ØªØ± Ø§Ø² 3
    return abs(bullish_score - bearish_score) > 3.0

# ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ: ØªØ´Ø®ÛŒØµ pattern Ø­Ø¬Ù… Ø¨Ø§Ù„Ø§
def classify_high_volume_pattern(self, df: pd.DataFrame, current_ratio: float,
                                 trend_data: Dict, sr_levels: Dict,
                                 momentum_data: Dict) -> str:
    """
    ØªØ´Ø®ÛŒØµ Ù†ÙˆØ¹ Ø§Ù„Ú¯ÙˆÛŒ Ø­Ø¬Ù… Ø¨Ø§Ù„Ø§

    Args:
        df: DataFrame Ù‚ÛŒÙ…Øª
        current_ratio: Ù†Ø³Ø¨Øª Ø­Ø¬Ù… ÙØ¹Ù„ÛŒ
        trend_data: Ù†ØªØ§ÛŒØ¬ detect_trend
        sr_levels: Ø³Ø·ÙˆØ­ S/R Ø§Ø² detect_support_resistance
        momentum_data: Ù†ØªØ§ÛŒØ¬ analyze_momentum_indicators

    Returns:
        'breakout_volume': Ø­Ø¬Ù… Ø´Ú©Ø³Øª Ø³Ø·Ø­ - Ù…Ø«Ø¨Øª
        'climax_volume': Ø­Ø¬Ù… Ø§ÙˆØ¬ - Ø§Ø­ØªÙ…Ø§Ù„ Ø¨Ø±Ú¯Ø´Øª
        'trending_volume': Ø§Ø¯Ø§Ù…Ù‡ Ø±ÙˆÙ†Ø¯ Ù‚ÙˆÛŒ
        'spike': Ø§ÙØ²Ø§ÛŒØ´ Ù…Ø¹Ù…ÙˆÙ„ÛŒ Ø­Ø¬Ù…
    """

    if current_ratio > self.volume_multiplier_threshold * 2.0:
        # Ø¨Ø±Ø±Ø³ÛŒ Ø¢ÛŒØ§ Ø¯Ø± Ù†Ø²Ø¯ÛŒÚ©ÛŒ Ø³Ø·ÙˆØ­ Ù…Ù‡Ù… Ù‡Ø³ØªÛŒÙ…
        current_price = df['close'].iloc[-1]

        # Ú†Ú© Ú©Ø±Ø¯Ù† Ø´Ú©Ø³Øª Ø³Ø·Ø­
        near_resistance = self._is_near_level(current_price, sr_levels.get('resistance', []))
        near_support = self._is_near_level(current_price, sr_levels.get('support', []))

        # Ø¨Ø±Ø±Ø³ÛŒ momentum Ùˆ Ø±ÙˆÙ†Ø¯
        is_trending = abs(trend_data.get('strength', 0)) >= 2
        momentum_strong = self._check_momentum_strength(momentum_data)

        # ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯Ùˆ
        if (near_resistance or near_support) and is_trending and momentum_strong:
            # Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ breakout Ø¨Ø§ Ø­Ø¬Ù… Ø¨Ø§Ù„Ø§
            return 'breakout_volume'
        elif momentum_strong and is_trending:
            # Ø§Ø¯Ø§Ù…Ù‡ Ø±ÙˆÙ†Ø¯ Ù‚ÙˆÛŒ Ø¨Ø§ Ø­Ø¬Ù… Ø¨Ø§Ù„Ø§
            return 'trending_volume'
        else:
            # Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ exhaustion/climax
            return 'climax_volume'

    elif current_ratio > self.volume_multiplier_threshold * 1.5:
        return 'spike'

    return 'normal'
```

**Ù†Ø­ÙˆÙ‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ - Ú¯Ø²ÛŒÙ†Ù‡ 1: ØªØºÛŒÛŒØ± analyze_volume_trend:**

```python
def analyze_volume_trend(self, df: pd.DataFrame, window: int = 20,
                        trend_data: Optional[Dict] = None,
                        sr_levels: Optional[Dict] = None,
                        momentum_data: Optional[Dict] = None) -> Dict[str, Any]:
    """Analyze volume trend with context awareness"""
    results = {'status': 'ok', 'current_ratio': 1.0, 'trend': 'neutral', 'pattern': 'normal'}

    # ... Ú©Ø¯ Ù‚Ø¨Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ current_ratio ...

    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² classify_high_volume_pattern Ø§Ú¯Ø± context Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯
    if trend_data and sr_levels and momentum_data:
        pattern = self.classify_high_volume_pattern(
            df, current_ratio, trend_data, sr_levels, momentum_data
        )
        results['pattern'] = pattern
    else:
        # fallback Ø¨Ù‡ logic Ù‚Ø¯ÛŒÙ…ÛŒ
        if current_ratio > self.volume_multiplier_threshold * 2.0:
            results['pattern'] = 'climax_volume'
        # ... Ø¨Ù‚ÛŒÙ‡ Ø´Ø±Ø§ÛŒØ· ...

    return results

# ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ø¯Ø± analyze_single_timeframe:
# ØªØºÛŒÛŒØ± line 4697 Ø§Ø²:
# analysis_data['volume'] = self.analyze_volume_trend(df)
# Ø¨Ù‡:
# analysis_data['volume'] = self.analyze_volume_trend(
#     df,
#     trend_data=analysis_data.get('trend'),
#     sr_levels=analysis_data.get('support_resistance', {}).get('details'),
#     momentum_data=analysis_data.get('momentum')
# )
```

**Ù†Ø­ÙˆÙ‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ - Ú¯Ø²ÛŒÙ†Ù‡ 2 (Ø¨Ù‡ØªØ±): Post-processing Ø¯Ø± analyze_single_timeframe:**

```python
async def analyze_single_timeframe(self, symbol: str, timeframe: str, df: pd.DataFrame):
    # ... Ú©Ø¯ Ù‚Ø¨Ù„ÛŒ ØªØ§ line 4710 ...

    # 6. Detect support/resistance
    analysis_data['support_resistance'] = self.detect_support_resistance(df)

    # ... Ø¨Ù‚ÛŒÙ‡ analyses ...

    # Post-processing: Refine volume pattern Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² context
    if (analysis_data.get('volume', {}).get('pattern') == 'climax_volume' and
        analysis_data.get('trend') and
        analysis_data.get('support_resistance') and
        analysis_data.get('momentum')):

        refined_pattern = self.classify_high_volume_pattern(
            df,
            analysis_data['volume']['current_ratio'],
            analysis_data['trend'],
            analysis_data['support_resistance'].get('details', {}),
            analysis_data['momentum']
        )
        analysis_data['volume']['pattern'] = refined_pattern
        analysis_data['volume']['refined'] = True

    return analysis_data
```

**ØªÙˆØµÛŒÙ‡:** Ú¯Ø²ÛŒÙ†Ù‡ 2 Ø¨Ù‡ØªØ± Ø§Ø³Øª Ú†ÙˆÙ†:
- âœ… Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªØºÛŒÛŒØ± signature ØªØ§Ø¨Ø¹ analyze_volume_trend Ù†Ø¯Ø§Ø±Ø¯
- âœ… backward compatible Ø§Ø³Øª
- âœ… ÙÙ‚Ø· climax_volume Ø±Ø§ refine Ù…ÛŒâ€ŒÚ©Ù†Ø¯

**Ù…Ø²Ø§ÛŒØ§:**
- ØªÙÚ©ÛŒÚ© Ø¯Ù‚ÛŒÙ‚ Ø¨ÛŒÙ† Ø­Ø¬Ù… Ù…Ø«Ø¨Øª (breakout) Ùˆ Ù…Ù†ÙÛŒ (climax)
- ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ø¨Ù‡ØªØ± Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø§Ø¹ØªØ¨Ø§Ø± Ø³ÛŒÚ¯Ù†Ø§Ù„
- Ú©Ø§Ù‡Ø´ false signals Ø¯Ø± Ù†Ø²Ø¯ÛŒÚ©ÛŒ exhaustion points

---

#### âŒ Ù…Ø´Ú©Ù„ 2: Ø¹Ø¯Ù… ØªØ­Ù„ÛŒÙ„ Volume Price Trend (VPT)

**Ù…Ø´Ú©Ù„ ÙØ¹Ù„ÛŒ:**
- ÙÙ‚Ø· **Ù†Ø³Ø¨Øª Ø­Ø¬Ù… (ratio)** Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
- **Ø±Ø§Ø¨Ø·Ù‡ Ø­Ø¬Ù… Ø¨Ø§ Ø¬Ù‡Øª Ù‚ÛŒÙ…Øª** Ø¨Ø±Ø±Ø³ÛŒ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯
- Ù…Ø«Ø§Ù„: Ø­Ø¬Ù… Ø¨Ø§Ù„Ø§ Ø¯Ø± Ú©Ù†Ø¯Ù„ Ù‚Ø±Ù…Ø² vs Ú©Ù†Ø¯Ù„ Ø³Ø¨Ø² ØªÙØ§ÙˆØª Ø¯Ø§Ø±Ø¯

**Ø±Ø§Ù‡ Ø­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**

```python
def calculate_volume_price_trend(self, df: pd.DataFrame, window: int = 20) -> Dict[str, Any]:
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ VPT (Volume Price Trend)

    VPT = VPT_Ù‚Ø¨Ù„ÛŒ + (Ø­Ø¬Ù… Ã— (ØªØºÛŒÛŒØ±_Ù‚ÛŒÙ…Øª / Ù‚ÛŒÙ…Øª_Ù‚Ø¨Ù„ÛŒ))
    """

    results = {}

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØºÛŒÛŒØ± Ù‚ÛŒÙ…Øª Ø¯Ø±ØµØ¯ÛŒ
    price_change_pct = df['close'].pct_change()

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ VPT
    vpt = (df['volume'] * price_change_pct).cumsum()

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø±ÙˆÙ†Ø¯ VPT
    vpt_sma = vpt.rolling(window=window).mean()
    current_vpt = vpt.iloc[-1]
    avg_vpt = vpt_sma.iloc[-1]

    # Ø¨Ø±Ø±Ø³ÛŒ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ VPT Ø¨Ø§ Ù‚ÛŒÙ…Øª
    price_trend = 'bullish' if df['close'].iloc[-1] > df['close'].iloc[-window] else 'bearish'
    vpt_trend = 'bullish' if current_vpt > avg_vpt else 'bearish'

    # Volume-Price Alignment
    vp_aligned = (price_trend == vpt_trend)

    results['vpt'] = current_vpt
    results['vpt_trend'] = vpt_trend
    results['price_trend'] = price_trend
    results['vp_alignment'] = vp_aligned
    results['vpt_strength'] = abs(current_vpt - avg_vpt) / abs(avg_vpt) if avg_vpt != 0 else 0

    return results
```

**Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ:**

```python
vpt_data = self.calculate_volume_price_trend(df)

if vpt_data['vp_alignment']:
    # Ø­Ø¬Ù… Ùˆ Ù‚ÛŒÙ…Øª Ù‡Ù…Ø§Ù‡Ù†Ú¯ Ù‡Ø³ØªÙ†Ø¯
    volume_quality_factor = 1.0 + (vpt_data['vpt_strength'] * 0.3)
else:
    # Ø­Ø¬Ù… Ùˆ Ù‚ÛŒÙ…Øª Ù†Ø§Ù‡Ù…Ø§Ù‡Ù†Ú¯ - Ù‡Ø´Ø¯Ø§Ø± ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ
    volume_quality_factor = max(0.7, 1.0 - (vpt_data['vpt_strength'] * 0.2))
```

**Ù…Ø²Ø§ÛŒØ§:**
- Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ø­Ø¬Ù…-Ù‚ÛŒÙ…Øª
- Ú©ÛŒÙÛŒØª Ø¨Ù‡ØªØ± Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ø­Ø¬Ù…
- ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø­Ø¬Ù…â€ŒÙ‡Ø§ÛŒ Ú¯Ù…Ø±Ø§Ù‡â€ŒÚ©Ù†Ù†Ø¯Ù‡

---

#### âŒ Ù…Ø´Ú©Ù„ 3: Ø¹Ø¯Ù… ØªØ´Ø®ÛŒØµ Volume Accumulation/Distribution

**Ù…Ø´Ú©Ù„ ÙØ¹Ù„ÛŒ:**
- ÙÙ‚Ø· Ø­Ø¬Ù… **Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ (instantaneous)** Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯
- **Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ØªØ¬Ù…Ø¹ÛŒ Ø­Ø¬Ù…** Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
- Ù…Ø«Ø§Ù„: Ø§ÙØ²Ø§ÛŒØ´ ØªØ¯Ø±ÛŒØ¬ÛŒ Ø­Ø¬Ù… Ù‚Ø¨Ù„ Ø§Ø² Ø­Ø±Ú©Øª Ø¨Ø²Ø±Ú¯

**Ø±Ø§Ù‡ Ø­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**

**ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²:**

```python
# ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ 1: Ø´Ù…Ø§Ø±Ø´ Ø§ÙØ²Ø§ÛŒØ´ Ù…ØªÙˆØ§Ù„ÛŒ
def _count_consecutive_increase(self, series: pd.Series, threshold: float = 0.0) -> int:
    """
    Ø´Ù…Ø§Ø±Ø´ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù†Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ØªÙˆØ§Ù„ÛŒ Ú©Ù‡ Ù…Ù‚Ø¯Ø§Ø± Ø¯Ø± Ø­Ø§Ù„ Ø§ÙØ²Ø§ÛŒØ´ Ø§Ø³Øª

    Args:
        series: Ø³Ø±ÛŒ Ø¯Ø§Ø¯Ù‡ (Ù…Ø«Ù„Ø§Ù‹ net_volume_cumsum)
        threshold: Ø­Ø¯Ø§Ù‚Ù„ ØªØºÛŒÛŒØ± Ø¨Ø±Ø§ÛŒ Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ† Ø§ÙØ²Ø§ÛŒØ´

    Returns:
        ØªØ¹Ø¯Ø§Ø¯ Ú©Ù†Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ØªÙˆØ§Ù„ÛŒ Ø§ÙØ²Ø§ÛŒØ´ÛŒ Ø§Ø² Ø§Ù†ØªÙ‡Ø§
    """
    count = 0
    for i in range(len(series) - 1, 0, -1):
        if series.iloc[i] > series.iloc[i-1] + threshold:
            count += 1
        else:
            break
    return count

# ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ 2: Ø´Ù…Ø§Ø±Ø´ Ú©Ø§Ù‡Ø´ Ù…ØªÙˆØ§Ù„ÛŒ
def _count_consecutive_decrease(self, series: pd.Series, threshold: float = 0.0) -> int:
    """
    Ø´Ù…Ø§Ø±Ø´ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù†Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ØªÙˆØ§Ù„ÛŒ Ú©Ù‡ Ù…Ù‚Ø¯Ø§Ø± Ø¯Ø± Ø­Ø§Ù„ Ú©Ø§Ù‡Ø´ Ø§Ø³Øª

    Args:
        series: Ø³Ø±ÛŒ Ø¯Ø§Ø¯Ù‡ (Ù…Ø«Ù„Ø§Ù‹ net_volume_cumsum)
        threshold: Ø­Ø¯Ø§Ù‚Ù„ ØªØºÛŒÛŒØ± Ø¨Ø±Ø§ÛŒ Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ† Ú©Ø§Ù‡Ø´

    Returns:
        ØªØ¹Ø¯Ø§Ø¯ Ú©Ù†Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ØªÙˆØ§Ù„ÛŒ Ú©Ø§Ù‡Ø´ÛŒ Ø§Ø² Ø§Ù†ØªÙ‡Ø§
    """
    count = 0
    for i in range(len(series) - 1, 0, -1):
        if series.iloc[i] < series.iloc[i-1] - threshold:
            count += 1
        else:
            break
    return count

# ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ: ØªØ´Ø®ÛŒØµ Accumulation/Distribution
def detect_volume_accumulation_distribution(self, df: pd.DataFrame,
                                            window: int = 20) -> Dict[str, Any]:
    """
    ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Accumulation (ØªØ¬Ù…Ø¹) Ùˆ Distribution (ØªÙˆØ²ÛŒØ¹)

    Args:
        df: DataFrame Ù‚ÛŒÙ…Øª Ø¨Ø§ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ open, close, volume
        window: ØªØ¹Ø¯Ø§Ø¯ Ú©Ù†Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ø§Øª (Ù¾ÛŒØ´â€ŒÙØ±Ø¶ 20)

    Returns:
        {
            'pattern': 'accumulation' | 'distribution' | 'surge' | 'neutral',
            'strength': float (0-1),
            'duration': int (ØªØ¹Ø¯Ø§Ø¯ Ú©Ù†Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ØªÙˆØ§Ù„ÛŒ)
        }
    """

    results = {'pattern': 'neutral', 'strength': 0, 'duration': 0}

    if len(df) < window + 1:
        return results

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø­Ø¬Ù… Ù†Ø³Ø¨ÛŒ Ø¯Ø± Ù‡Ø± Ú©Ù†Ø¯Ù„
    vol_sma = df['volume'].rolling(window=window).mean()
    vol_ratio = df['volume'] / vol_sma

    # Ø¨Ø±Ø±Ø³ÛŒ Ø±ÙˆÙ†Ø¯ Ù‚ÛŒÙ…Øª
    price_direction = np.sign(df['close'] - df['open'])

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Net Volume
    # Ø­Ø¬Ù… Ù…Ø«Ø¨Øª: Ú©Ù†Ø¯Ù„ ØµØ¹ÙˆØ¯ÛŒØŒ Ø­Ø¬Ù… Ù…Ù†ÙÛŒ: Ú©Ù†Ø¯Ù„ Ù†Ø²ÙˆÙ„ÛŒ
    net_volume = df['volume'] * price_direction
    net_volume_cumsum = net_volume.rolling(window=window).sum()

    # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ accumulation/distribution
    recent_net_vol = net_volume_cumsum.iloc[-window:]

    # Accumulation: Ø­Ø¬Ù… Ø®Ø±ÛŒØ¯ Ø¨ÛŒØ´ØªØ± Ø§Ø² ÙØ±ÙˆØ´ (net volume Ù…Ø«Ø¨Øª Ùˆ Ø±Ùˆ Ø¨Ù‡ Ø§ÙØ²Ø§ÛŒØ´)
    if recent_net_vol.iloc[-1] > 0 and recent_net_vol.is_monotonic_increasing:
        results['pattern'] = 'accumulation'
        results['strength'] = abs(recent_net_vol.iloc[-1]) / df['volume'].iloc[-window:].sum()
        results['duration'] = self._count_consecutive_increase(recent_net_vol)

    # Distribution: Ø­Ø¬Ù… ÙØ±ÙˆØ´ Ø¨ÛŒØ´ØªØ± Ø§Ø² Ø®Ø±ÛŒØ¯ (net volume Ù…Ù†ÙÛŒ Ùˆ Ø±Ùˆ Ø¨Ù‡ Ú©Ø§Ù‡Ø´)
    elif recent_net_vol.iloc[-1] < 0 and recent_net_vol.is_monotonic_decreasing:
        results['pattern'] = 'distribution'
        results['strength'] = abs(recent_net_vol.iloc[-1]) / df['volume'].iloc[-window:].sum()
        results['duration'] = self._count_consecutive_decrease(recent_net_vol)

    # Volume Surge: Ø§ÙØ²Ø§ÛŒØ´ Ù†Ø§Ú¯Ù‡Ø§Ù†ÛŒ Ø¨Ø¯ÙˆÙ† Ø§Ù„Ú¯Ùˆ
    elif vol_ratio.iloc[-1] > self.volume_multiplier_threshold * 1.5:
        results['pattern'] = 'surge'
        results['strength'] = vol_ratio.iloc[-1]

    return results
```

**Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ø³ÛŒÚ¯Ù†Ø§Ù„:**

```python
accum_dist = self.detect_volume_accumulation_distribution(df)

if direction == 'long' and accum_dist['pattern'] == 'accumulation':
    # Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø®Ø±ÛŒØ¯ Ø¨Ø§ Ø§Ù„Ú¯ÙˆÛŒ ØªØ¬Ù…Ø¹ - Ø¨Ø³ÛŒØ§Ø± Ù‚ÙˆÛŒ
    score_multiplier = 1.0 + (accum_dist['strength'] * 0.5)

elif direction == 'long' and accum_dist['pattern'] == 'distribution':
    # Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø®Ø±ÛŒØ¯ Ø¨Ø§ Ø§Ù„Ú¯ÙˆÛŒ ØªÙˆØ²ÛŒØ¹ - Ø¶Ø¹ÛŒÙ/Ø±Ø¯
    score_multiplier = 0.6

elif direction == 'short' and accum_dist['pattern'] == 'distribution':
    # Ø³ÛŒÚ¯Ù†Ø§Ù„ ÙØ±ÙˆØ´ Ø¨Ø§ Ø§Ù„Ú¯ÙˆÛŒ ØªÙˆØ²ÛŒØ¹ - Ø¨Ø³ÛŒØ§Ø± Ù‚ÙˆÛŒ
    score_multiplier = 1.0 + (accum_dist['strength'] * 0.5)
```

**Ù…Ø²Ø§ÛŒØ§:**
- Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ÙØ´Ø§Ø± Ø®Ø±ÛŒØ¯/ÙØ±ÙˆØ´ Ù‚Ø¨Ù„ Ø§Ø² Ø­Ø±Ú©Øª Ø§ØµÙ„ÛŒ
- ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø®Ù„Ø§Ù Ø¬Ø±ÛŒØ§Ù†
- Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ù‡ØªØ± Ø­Ø±Ú©Ø§Øª Ø¨Ø²Ø±Ú¯

---

#### âŒ Ù…Ø´Ú©Ù„ 4: Ø¹Ø¯Ù… ØªØ·Ø¨ÛŒÙ‚ Ø¢Ø³ØªØ§Ù†Ù‡ Ø­Ø¬Ù… Ø¨Ø§ Ù†ÙˆØ³Ø§Ù†Ø§Øª Ø¨Ø§Ø²Ø§Ø±

**Ù…Ø´Ú©Ù„ ÙØ¹Ù„ÛŒ:**
```python
# signal_generator.py:1472
self.volume_multiplier_threshold = 1.3  # Ø«Ø§Ø¨Øª
```

- Ø¢Ø³ØªØ§Ù†Ù‡ Ø­Ø¬Ù… **Ø«Ø§Ø¨Øª (1.3)** Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ø´Ø±Ø§ÛŒØ· Ø¨Ø§Ø²Ø§Ø±
- Ø¯Ø± Ø¨Ø§Ø²Ø§Ø±Ù‡Ø§ÛŒ Ø¢Ø±Ø§Ù…: 1.3 Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø®ÛŒÙ„ÛŒ Ù¾Ø§ÛŒÛŒÙ† Ø¨Ø§Ø´Ø¯
- Ø¯Ø± Ø¨Ø§Ø²Ø§Ø±Ù‡Ø§ÛŒ Ù¾Ø± Ù†ÙˆØ³Ø§Ù†: 1.3 Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø®ÛŒÙ„ÛŒ Ø¨Ø§Ù„Ø§ Ø¨Ø§Ø´Ø¯

**Ø±Ø§Ù‡ Ø­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**

```python
def calculate_adaptive_volume_threshold(self, df: pd.DataFrame,
                                        market_regime: str,
                                        window: int = 20) -> float:
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ø³ØªØ§Ù†Ù‡ Ø­Ø¬Ù… Ø§Ù†Ø·Ø¨Ø§Ù‚ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø±Ú˜ÛŒÙ… Ø¨Ø§Ø²Ø§Ø±
    """

    base_threshold = 1.3

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†ÙˆØ³Ø§Ù†Ø§Øª Ø­Ø¬Ù…
    vol_std = df['volume'].rolling(window=window).std().iloc[-1]
    vol_mean = df['volume'].rolling(window=window).mean().iloc[-1]
    vol_cv = vol_std / vol_mean if vol_mean > 0 else 0  # Coefficient of Variation

    # ØªØ·Ø¨ÛŒÙ‚ Ø¨Ø§ Ø±Ú˜ÛŒÙ… Ø¨Ø§Ø²Ø§Ø±
    regime_adjustments = {
        'strong_trend': -0.1,      # Ø¯Ø± Ø±ÙˆÙ†Ø¯ Ù‚ÙˆÛŒØŒ Ø¢Ø³ØªØ§Ù†Ù‡ Ú©Ù…ØªØ±ÛŒ Ú©Ø§ÙÛŒ Ø§Ø³Øª
        'weak_trend': 0.0,
        'range': 0.2,              # Ø¯Ø± Ø±Ù†Ø¬ØŒ Ø­Ø¬Ù… Ø¨Ø§Ù„Ø§ØªØ±ÛŒ Ù†ÛŒØ§Ø² Ø§Ø³Øª
        'tight_range': 0.3,
        'choppy': 0.25,
        'breakout': -0.15,         # Ø¯Ø± breakoutØŒ Ø­Ø¬Ù… Ø·Ø¨ÛŒØ¹ÛŒ Ø§Ø³Øª
        'volatile': 0.15,
        'trending_range': 0.1,
        'transition': 0.05
    }

    regime_adj = regime_adjustments.get(market_regime, 0.0)

    # ØªØ·Ø¨ÛŒÙ‚ Ø¨Ø§ Ù†ÙˆØ³Ø§Ù†Ø§Øª Ø­Ø¬Ù…
    # Ø§Ú¯Ø± Ø­Ø¬Ù… Ø®ÛŒÙ„ÛŒ Ù…ØªØºÛŒØ± Ø§Ø³Øª (CV Ø¨Ø§Ù„Ø§)ØŒ Ø¢Ø³ØªØ§Ù†Ù‡ Ø±Ø§ Ø¨Ø§Ù„Ø§ Ø¨Ø¨Ø±ÛŒÙ…
    volatility_adj = min(0.3, vol_cv * 0.5)

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ø³ØªØ§Ù†Ù‡ Ù†Ù‡Ø§ÛŒÛŒ
    adaptive_threshold = base_threshold + regime_adj + volatility_adj

    # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Ø¨Ø§Ø²Ù‡ Ù…Ø¹Ù‚ÙˆÙ„
    adaptive_threshold = max(1.1, min(2.0, adaptive_threshold))

    return adaptive_threshold
```

**Ø§Ø³ØªÙØ§Ø¯Ù‡ - Ú¯Ø²ÛŒÙ†Ù‡ 1: Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù¾Ø§Ø±Ø§Ù…ØªØ± regime (ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯):**

```python
def analyze_volume_trend(self, df: pd.DataFrame, window: int = 20,
                        market_regime: Optional[str] = None) -> Dict[str, Any]:
    """Analyze volume trend with adaptive threshold"""
    results = {'status': 'ok', ...}

    # ... Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ù‚Ø¨Ù„ÛŒ ...

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ø³ØªØ§Ù†Ù‡ Ø§Ù†Ø·Ø¨Ø§Ù‚ÛŒ Ø§Ú¯Ø± regime Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯
    if market_regime:
        adaptive_threshold = self.calculate_adaptive_volume_threshold(df, market_regime)
    else:
        adaptive_threshold = self.volume_multiplier_threshold  # fallback Ø¨Ù‡ Ø«Ø§Ø¨Øª

    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¢Ø³ØªØ§Ù†Ù‡ Ø§Ù†Ø·Ø¨Ø§Ù‚ÛŒ
    is_confirmed_by_volume = current_ratio > adaptive_threshold

    return results

# ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ø¯Ø± analyze_single_timeframe:
# âš ï¸ Ù…Ø´Ú©Ù„: regime detection Ø¨Ø¹Ø¯ Ø§Ø² volume analysis Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯ (line 4733)
# Ø±Ø§Ù‡ Ø­Ù„: Ø§Ù†ØªÙ‚Ø§Ù„ regime detection Ø¨Ù‡ Ù‚Ø¨Ù„ Ø§Ø² volume ÛŒØ§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú¯Ø²ÛŒÙ†Ù‡ 2
```

**Ø§Ø³ØªÙØ§Ø¯Ù‡ - Ú¯Ø²ÛŒÙ†Ù‡ 2 (Ø³Ø§Ø¯Ù‡â€ŒØªØ±): Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² trend strength Ø¨Ù‡ Ø¬Ø§ÛŒ regime:**

```python
def calculate_adaptive_volume_threshold(self, df: pd.DataFrame,
                                        trend_strength: Optional[int] = None,
                                        window: int = 20) -> float:
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ø³ØªØ§Ù†Ù‡ Ø­Ø¬Ù… Ø§Ù†Ø·Ø¨Ø§Ù‚ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ trend strength Ùˆ volume volatility

    Args:
        df: DataFrame Ù‚ÛŒÙ…Øª
        trend_strength: Ù‚Ø¯Ø±Øª Ø±ÙˆÙ†Ø¯ Ø§Ø² detect_trend (-3 ØªØ§ +3)
        window: ØªØ¹Ø¯Ø§Ø¯ Ú©Ù†Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ø§Øª

    Returns:
        Ø¢Ø³ØªØ§Ù†Ù‡ Ø§Ù†Ø·Ø¨Ø§Ù‚ÛŒ (1.1 ØªØ§ 2.0)
    """

    base_threshold = 1.3

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†ÙˆØ³Ø§Ù†Ø§Øª Ø­Ø¬Ù…
    vol_std = df['volume'].rolling(window=window).std().iloc[-1]
    vol_mean = df['volume'].rolling(window=window).mean().iloc[-1]
    vol_cv = vol_std / vol_mean if vol_mean > 0 else 0  # Coefficient of Variation

    # ØªØ·Ø¨ÛŒÙ‚ Ø¨Ø§ Ù‚Ø¯Ø±Øª Ø±ÙˆÙ†Ø¯ (Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯)
    trend_adj = 0.0
    if trend_strength is not None:
        abs_strength = abs(trend_strength)
        if abs_strength >= 3:
            trend_adj = -0.15  # Ø±ÙˆÙ†Ø¯ Ø®ÛŒÙ„ÛŒ Ù‚ÙˆÛŒ - Ø¢Ø³ØªØ§Ù†Ù‡ Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ±
        elif abs_strength >= 2:
            trend_adj = -0.05  # Ø±ÙˆÙ†Ø¯ Ù‚ÙˆÛŒ
        elif abs_strength <= 1:
            trend_adj = 0.2    # Ø±ÙˆÙ†Ø¯ Ø¶Ø¹ÛŒÙ/Ø±Ù†Ø¬ - Ø¢Ø³ØªØ§Ù†Ù‡ Ø¨Ø§Ù„Ø§ØªØ±

    # ØªØ·Ø¨ÛŒÙ‚ Ø¨Ø§ Ù†ÙˆØ³Ø§Ù†Ø§Øª Ø­Ø¬Ù…
    # Ø§Ú¯Ø± Ø­Ø¬Ù… Ø®ÛŒÙ„ÛŒ Ù…ØªØºÛŒØ± Ø§Ø³Øª (CV Ø¨Ø§Ù„Ø§)ØŒ Ø¢Ø³ØªØ§Ù†Ù‡ Ø±Ø§ Ø¨Ø§Ù„Ø§ Ø¨Ø¨Ø±ÛŒÙ…
    volatility_adj = min(0.3, vol_cv * 0.5)

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ø³ØªØ§Ù†Ù‡ Ù†Ù‡Ø§ÛŒÛŒ
    adaptive_threshold = base_threshold + trend_adj + volatility_adj

    # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Ø¨Ø§Ø²Ù‡ Ù…Ø¹Ù‚ÙˆÙ„
    adaptive_threshold = max(1.1, min(2.0, adaptive_threshold))

    return adaptive_threshold

# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± analyze_volume_trend:
def analyze_volume_trend(self, df: pd.DataFrame, window: int = 20,
                        trend_data: Optional[Dict] = None) -> Dict[str, Any]:
    """Analyze volume trend with adaptive threshold"""
    results = {'status': 'ok', ...}

    # ... Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ù‚Ø¨Ù„ÛŒ ...

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ø³ØªØ§Ù†Ù‡ Ø§Ù†Ø·Ø¨Ø§Ù‚ÛŒ
    trend_strength = trend_data.get('strength', None) if trend_data else None
    adaptive_threshold = self.calculate_adaptive_volume_threshold(df, trend_strength)

    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¢Ø³ØªØ§Ù†Ù‡ Ø§Ù†Ø·Ø¨Ø§Ù‚ÛŒ
    is_confirmed_by_volume = current_ratio > adaptive_threshold
    results['adaptive_threshold'] = adaptive_threshold  # Ø°Ø®ÛŒØ±Ù‡ Ø¨Ø±Ø§ÛŒ debugging

    return results

# ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ø¯Ø± analyze_single_timeframe (line 4697):
# analysis_data['volume'] = self.analyze_volume_trend(df, trend_data=analysis_data.get('trend'))
```

**ØªÙˆØµÛŒÙ‡:** Ú¯Ø²ÛŒÙ†Ù‡ 2 Ø¨Ù‡ØªØ± Ø§Ø³Øª Ú†ÙˆÙ†:
- âœ… trend_data Ù‚Ø¨Ù„ Ø§Ø² volume Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³Øª (line 4689)
- âœ… Ø³Ø§Ø¯Ù‡â€ŒØªØ± Ùˆ Ú©Ù…ØªØ± ÙˆØ§Ø¨Ø³ØªÙ‡ Ø¨Ù‡ regime detection
- âœ… backward compatible Ø¨Ø§ fallback Ø¨Ù‡ threshold Ø«Ø§Ø¨Øª

**Ù…Ø²Ø§ÛŒØ§:**
- Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ Ø´Ø±Ø§ÛŒØ· Ù…Ø®ØªÙ„Ù Ø¨Ø§Ø²Ø§Ø±
- Ú©Ø§Ù‡Ø´ false positives Ø¯Ø± Ø¨Ø§Ø²Ø§Ø±Ù‡Ø§ÛŒ Ø¢Ø±Ø§Ù…
- Ø§ÙØ²Ø§ÛŒØ´ Ø­Ø³Ø§Ø³ÛŒØª Ø¯Ø± Ø´Ø±Ø§ÛŒØ· Ù…Ù†Ø§Ø³Ø¨

---

#### âŒ Ù…Ø´Ú©Ù„ 5: Ø¹Ø¯Ù… ØªØ­Ù„ÛŒÙ„ Volume Momentum

**Ù…Ø´Ú©Ù„ ÙØ¹Ù„ÛŒ:**
- ÙÙ‚Ø· **Ø­Ø¬Ù… Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ** Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯
- **ØªØºÛŒÛŒØ±Ø§Øª Ø­Ø¬Ù… (Volume Momentum)** Ø¨Ø±Ø±Ø³ÛŒ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯
- Ø³Ø±Ø¹Øª Ø§ÙØ²Ø§ÛŒØ´/Ú©Ø§Ù‡Ø´ Ø­Ø¬Ù… Ø§Ù‡Ù…ÛŒØª Ø¯Ø§Ø±Ø¯

**Ø±Ø§Ù‡ Ø­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**

```python
def calculate_volume_momentum(self, df: pd.DataFrame, periods: List[int] = [5, 10, 20]) -> Dict[str, Any]:
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ Momentum Ø­Ø¬Ù… Ø¯Ø± Ø¨Ø§Ø²Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
    """

    results = {}
    vol_series = df['volume']

    for period in periods:
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØºÛŒÛŒØ± Ø¯Ø±ØµØ¯ÛŒ Ø­Ø¬Ù…
        vol_change_pct = vol_series.pct_change(period).iloc[-1]

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´ØªØ§Ø¨ Ø­Ø¬Ù… (Volume Acceleration)
        vol_roc = vol_series.pct_change(period)
        vol_acceleration = vol_roc.diff(period).iloc[-1]

        results[f'vol_momentum_{period}'] = vol_change_pct
        results[f'vol_acceleration_{period}'] = vol_acceleration

    # Ø¨Ø±Ø±Ø³ÛŒ Ù‡Ù…Ú¯Ø±Ø§ÛŒÛŒ momentumâ€ŒÙ‡Ø§
    all_positive = all(results[f'vol_momentum_{p}'] > 0 for p in periods)
    all_negative = all(results[f'vol_momentum_{p}'] < 0 for p in periods)

    results['momentum_aligned'] = all_positive or all_negative
    results['momentum_direction'] = 'increasing' if all_positive else 'decreasing' if all_negative else 'mixed'

    # Ø¨Ø±Ø±Ø³ÛŒ Ø´ØªØ§Ø¨ (Ø¢ÛŒØ§ momentum Ø¯Ø± Ø­Ø§Ù„ Ø§ÙØ²Ø§ÛŒØ´ Ø§Ø³ØªØŸ)
    accelerating = all(results[f'vol_acceleration_{p}'] > 0 for p in periods)
    results['is_accelerating'] = accelerating

    return results
```

**Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ:**

```python
vol_momentum = self.calculate_volume_momentum(df)

if vol_momentum['momentum_aligned'] and vol_momentum['is_accelerating']:
    # Ø­Ø¬Ù… Ø¯Ø± Ø­Ø§Ù„ Ø§ÙØ²Ø§ÛŒØ´ Ø¨Ø§ Ø´ØªØ§Ø¨ - Ø¨Ø³ÛŒØ§Ø± Ù‚ÙˆÛŒ
    volume_momentum_factor = 1.5
elif vol_momentum['momentum_aligned']:
    # Ø­Ø¬Ù… Ø¯Ø± Ø­Ø§Ù„ Ø§ÙØ²Ø§ÛŒØ´/Ú©Ø§Ù‡Ø´ ÛŒÚ©Ù†ÙˆØ§Ø®Øª - Ù‚ÙˆÛŒ
    volume_momentum_factor = 1.2
else:
    # momentum Ù…Ø®ØªÙ„Ø· - Ù…Ø¹Ù…ÙˆÙ„ÛŒ
    volume_momentum_factor = 1.0
```

**Ù…Ø²Ø§ÛŒØ§:**
- Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø²ÙˆØ¯Ù‡Ù†Ú¯Ø§Ù… ØªØºÛŒÛŒØ±Ø§Øª Ø­Ø¬Ù…
- ØªØ´Ø®ÛŒØµ Ù‚Ø¯Ø±Øª Ø±ÙˆÙ†Ø¯ Ø­Ø¬Ù…ÛŒ
- Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ù‡ØªØ± Ø§Ø¯Ø§Ù…Ù‡ Ø­Ø±Ú©Øª

---

### ğŸ“‹ Ø®Ù„Ø§ØµÙ‡ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª

| # | Ù…Ø´Ú©Ù„ | Ø±Ø§Ù‡ Ø­Ù„ | Ø§ÙˆÙ„ÙˆÛŒØª | ØªØ£Ø«ÛŒØ± Ø¨Ø± Ø¯Ù‚Øª |
|---|------|--------|---------|--------------|
| 1 | Ø¹Ø¯Ù… ØªÙÚ©ÛŒÚ© Climax vs Breakout | ØªØ­Ù„ÛŒÙ„ context (Ø³Ø·ÙˆØ­ØŒ Ø±ÙˆÙ†Ø¯) | ğŸ”´ Ø¨Ø§Ù„Ø§ | +15% |
| 2 | Ø¹Ø¯Ù… ØªØ­Ù„ÛŒÙ„ VPT | Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Volume-Price Trend | ğŸŸ¡ Ù…ØªÙˆØ³Ø· | +10% |
| 3 | Ø¹Ø¯Ù… ØªØ´Ø®ÛŒØµ Accumulation/Distribution | ØªØ­Ù„ÛŒÙ„ Net Volume ØªØ¬Ù…Ø¹ÛŒ | ğŸ”´ Ø¨Ø§Ù„Ø§ | +20% |
| 4 | Ø¢Ø³ØªØ§Ù†Ù‡ Ø«Ø§Ø¨Øª | Ø¢Ø³ØªØ§Ù†Ù‡ Ø§Ù†Ø·Ø¨Ø§Ù‚ÛŒ Ø¨Ø§ Ø±Ú˜ÛŒÙ… Ø¨Ø§Ø²Ø§Ø± | ğŸŸ¡ Ù…ØªÙˆØ³Ø· | +12% |
| 5 | Ø¹Ø¯Ù… Volume Momentum | Ù…Ø­Ø§Ø³Ø¨Ù‡ momentum Ùˆ acceleration | ğŸŸ¢ Ù¾Ø§ÛŒÛŒÙ† | +8% |

**ØªØ£Ø«ÛŒØ± Ú©Ù„ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª:** Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‚Øª Ø­Ø¯ÙˆØ¯ **+35-45%** Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ø­Ø¬Ù…

---

### ğŸ”¬ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª ØªØ³Øª Ùˆ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ

1. **Backtesting Volume Patterns:**
   - ØªØ³Øª Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù‡Ø± Ø§Ù„Ú¯ÙˆÛŒ Ø­Ø¬Ù…ÛŒ (spike, climax, breakout, accumulation)
   - Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù†Ø±Ø® Ù…ÙˆÙÙ‚ÛŒØª Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ Ø¨Ø§/Ø¨Ø¯ÙˆÙ† ØªØ£ÛŒÛŒØ¯ Ø­Ø¬Ù…ÛŒ
   - ØªØ­Ù„ÛŒÙ„ Ø¯Ø± timeframeâ€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù

2. **A/B Testing Thresholds:**
   - ØªØ³Øª Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù (1.2, 1.3, 1.5, adaptive)
   - Ø§Ù†Ø¯Ø§Ø²Ù‡â€ŒÚ¯ÛŒØ±ÛŒ ØªØ£Ø«ÛŒØ± Ø¨Ø± precision/recall
   - ÛŒØ§ÙØªÙ† Ø¢Ø³ØªØ§Ù†Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø±Ú˜ÛŒÙ… Ø¨Ø§Ø²Ø§Ø±

3. **Volume-Signal Correlation:**
   - ØªØ­Ù„ÛŒÙ„ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø¨ÛŒÙ† Ø­Ø¬Ù… Ùˆ Ù…ÙˆÙÙ‚ÛŒØª Ø³ÛŒÚ¯Ù†Ø§Ù„
   - Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø±Ø§ÛŒØ·ÛŒ Ú©Ù‡ Ø­Ø¬Ù… Ø¨Ø§Ù„Ø§ Ù…ÙÛŒØ¯/Ù…Ø¶Ø± Ø§Ø³Øª
   - Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙˆØ²Ù† Ø­Ø¬Ù… Ø¯Ø± scoring

4. **Multi-Timeframe Volume:**
   - ØªØ³Øª Ø§Ù‡Ù…ÛŒØª ØªØ£ÛŒÛŒØ¯ Ø­Ø¬Ù…ÛŒ Ø¯Ø± timeframeâ€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
   - Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙˆØ²Ù† Ù‡Ø± timeframe
   - ØªØ­Ù„ÛŒÙ„ divergence Ø­Ø¬Ù…ÛŒ Ø¨ÛŒÙ† timeframeâ€ŒÙ‡Ø§

---

## 4. ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒØ´Ø±ÙØªÙ‡ MACD

**ğŸ“ Ú©Ø¯ Ù…Ø±Ø¬Ø¹:** `signal_generator.py:4534-4645` - ØªØ§Ø¨Ø¹ `_analyze_macd()`

### Ù…Ø´Ú©Ù„Ø§Øª Ùˆ Ù…Ø­Ø¯ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ ÙØ¹Ù„ÛŒ

#### âŒ Ù…Ø´Ú©Ù„ 1: Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø«Ø§Ø¨Øª MACD Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ø´Ø±Ø§ÛŒØ·

**Ù…Ø´Ú©Ù„ ÙØ¹Ù„ÛŒ:**
```python
# signal_generator.py:4566
dif, dea, hist = talib.MACD(close, fastperiod=12, slowperiod=26, signalperiod=9)
```

- Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ **Ø«Ø§Ø¨Øª (12, 26, 9)** Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ø´Ø±Ø§ÛŒØ· Ø¨Ø§Ø²Ø§Ø± Ùˆ Ù‡Ù…Ù‡ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…â€ŒÙ‡Ø§
- Ø§ÛŒÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø± Ø³Ù‡Ø§Ù… Ø¯Ù‡Ù‡ 1970 Ø·Ø±Ø§Ø­ÛŒ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯
- Ø¯Ø± Ø¨Ø§Ø²Ø§Ø±Ù‡Ø§ÛŒ Ú©Ø±ÛŒÙ¾ØªÙˆ Ø¨Ø§ Ù†ÙˆØ³Ø§Ù†Ø§Øª Ø¨Ø§Ù„Ø§ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¨Ù‡ÛŒÙ†Ù‡ Ù†Ø¨Ø§Ø´Ù†Ø¯
- ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…ØªÙØ§ÙˆØª Ø¯Ø§Ø±Ù†Ø¯

**Ø±Ø§Ù‡ Ø­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**

```python
def calculate_adaptive_macd_parameters(self, df: pd.DataFrame,
                                       timeframe: str,
                                       market_regime: str) -> Tuple[int, int, int]:
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø§Ù†Ø·Ø¨Ø§Ù‚ÛŒ MACD Ø¨Ø± Ø§Ø³Ø§Ø³ timeframe Ùˆ market regime
    """

    # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡ Ø¨Ø± Ø§Ø³Ø§Ø³ timeframe
    base_params = {
        '1m':  (8,  17, 6),   # Ø³Ø±ÛŒØ¹â€ŒØªØ± Ø¨Ø±Ø§ÛŒ timeframe Ú©ÙˆØªØ§Ù‡
        '5m':  (10, 21, 7),
        '15m': (12, 26, 9),   # Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯
        '1h':  (14, 30, 10),
        '4h':  (16, 34, 11),
        '1d':  (19, 39, 12)   # Ú©Ù†Ø¯ØªØ± Ø¨Ø±Ø§ÛŒ timeframe Ø¨Ù„Ù†Ø¯
    }

    fast, slow, signal = base_params.get(timeframe, (12, 26, 9))

    # ØªØ·Ø¨ÛŒÙ‚ Ø¨Ø§ Ø±Ú˜ÛŒÙ… Ø¨Ø§Ø²Ø§Ø±
    if market_regime in ['choppy', 'range', 'tight_range']:
        # Ø¯Ø± Ø¨Ø§Ø²Ø§Ø±Ù‡Ø§ÛŒ Ø±Ù†Ø¬ØŒ MACD Ø³Ø±ÛŒØ¹â€ŒØªØ± Ø¨Ø§Ø´Ø¯
        fast = max(6, int(fast * 0.75))
        slow = max(12, int(slow * 0.75))
        signal = max(5, int(signal * 0.75))

    elif market_regime in ['strong_trend', 'trending']:
        # Ø¯Ø± Ø±ÙˆÙ†Ø¯Ù‡Ø§ÛŒ Ù‚ÙˆÛŒØŒ MACD Ú©Ù†Ø¯ØªØ± Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„ØªØ± Ù†ÙˆÛŒØ²
        fast = int(fast * 1.2)
        slow = int(slow * 1.2)
        signal = int(signal * 1.2)

    # ØªØ·Ø¨ÛŒÙ‚ Ø¨Ø§ Ù†ÙˆØ³Ø§Ù†Ø§Øª
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ ATR Ø¯Ø±ØµØ¯ÛŒ
    atr = talib.ATR(df['high'].values, df['low'].values, df['close'].values, timeperiod=14)
    if len(atr) > 0 and not np.isnan(atr[-1]):
        atr_percent = (atr[-1] / df['close'].iloc[-1]) * 100
        if atr_percent > 5.0:  # Ù†ÙˆØ³Ø§Ù†Ø§Øª Ø¨Ø§Ù„Ø§
            fast = int(fast * 1.15)
            slow = int(slow * 1.15)
        elif atr_percent < 1.5:  # Ù†ÙˆØ³Ø§Ù†Ø§Øª Ù¾Ø§ÛŒÛŒÙ†
            fast = int(fast * 0.85)
            slow = int(slow * 0.85)

    return (fast, slow, signal)
```

**Ø§Ø³ØªÙØ§Ø¯Ù‡:**

```python
# Ø¯Ø± ØªØ§Ø¨Ø¹ _analyze_macd
timeframe = df.attrs.get('timeframe', '15m')

# Ø¯Ø±ÛŒØ§ÙØª Ø±Ú˜ÛŒÙ… Ø¨Ø§Ø²Ø§Ø±
if self.regime_detector and self.regime_detector.enabled:
    regime_result = self.regime_detector.detect_regime(df)
    market_regime = regime_result.get('regime', 'unknown')
else:
    market_regime = 'unknown'

fast, slow, signal = self.calculate_adaptive_macd_parameters(df, timeframe, market_regime)

# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø§Ù†Ø·Ø¨Ø§Ù‚ÛŒ
dif, dea, hist = talib.MACD(close, fastperiod=fast, slowperiod=slow, signalperiod=signal)
```

**Ù…Ø²Ø§ÛŒØ§:**
- Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ Ø´Ø±Ø§ÛŒØ· Ù…Ø®ØªÙ„Ù Ø¨Ø§Ø²Ø§Ø±
- Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± timeframe
- Ú©Ø§Ù‡Ø´ false signals Ø¯Ø± Ø¨Ø§Ø²Ø§Ø±Ù‡Ø§ÛŒ choppy
- Ø§ÙØ²Ø§ÛŒØ´ sensitivity Ø¯Ø± Ø²Ù…Ø§Ù† Ù…Ù†Ø§Ø³Ø¨

---

#### âŒ Ù…Ø´Ú©Ù„ 2: Ø¹Ø¯Ù… ÙÛŒÙ„ØªØ± Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Market Type

**Ù…Ø´Ú©Ù„ ÙØ¹Ù„ÛŒ:**
```python
# signal_generator.py:4577
market_type = self._detect_macd_market_type(dif, hist, ema20, ema50)
# ÙˆÙ„ÛŒ Ø§ÛŒÙ† market_type ÙÙ‚Ø· Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯!
```

- Market Type Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯ ÙˆÙ„ÛŒ Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯
- Ø¯Ø± `X_transition` ÛŒØ§ `B_bullish_correction` Ù†Ø¨Ø§ÛŒØ¯ Ø³ÛŒÚ¯Ù†Ø§Ù„ Ù‚ÙˆÛŒ ØµØ§Ø¯Ø± Ø´ÙˆØ¯

**Ø±Ø§Ù‡ Ø­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**

```python
def filter_macd_signals_by_market_type(self, signals: List[Dict], market_type: str) -> List[Dict]:
    """
    ÙÛŒÙ„ØªØ± Ùˆ ØªØ¹Ø¯ÛŒÙ„ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ MACD Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ Ø¨Ø§Ø²Ø§Ø±
    """

    # Ø¶Ø±Ø§ÛŒØ¨ ØªØ¹Ø¯ÛŒÙ„ Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ market type
    market_type_multipliers = {
        'A_bullish_strong': {
            'bullish_signals': 1.3,   # Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØµØ¹ÙˆØ¯ÛŒ Ù‚ÙˆÛŒâ€ŒØªØ±
            'bearish_signals': 0.5    # Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù†Ø²ÙˆÙ„ÛŒ Ø¶Ø¹ÛŒÙâ€ŒØªØ±
        },
        'B_bullish_correction': {
            'bullish_signals': 0.7,   # Ù…Ù†ØªØ¸Ø± Ù¾Ø§ÛŒØ§Ù† Ø§ØµÙ„Ø§Ø­
            'bearish_signals': 0.8
        },
        'C_bearish_strong': {
            'bullish_signals': 0.5,
            'bearish_signals': 1.3
        },
        'D_bearish_rebound': {
            'bullish_signals': 0.8,
            'bearish_signals': 0.7
        },
        'X_transition': {
            'bullish_signals': 0.4,   # Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ Ø¨ÛŒâ€ŒØ§Ø¹ØªØ¨Ø§Ø±
            'bearish_signals': 0.4
        }
    }

    multipliers = market_type_multipliers.get(market_type,
                                               {'bullish_signals': 1.0, 'bearish_signals': 1.0})

    filtered_signals = []
    for signal in signals:
        direction = signal.get('direction', 'neutral')

        # ØªØ¹Ø¯ÛŒÙ„ Ø§Ù…ØªÛŒØ§Ø²
        if direction == 'bullish':
            signal['score'] *= multipliers['bullish_signals']
            signal['market_type_adjusted'] = True
        elif direction == 'bearish':
            signal['score'] *= multipliers['bearish_signals']
            signal['market_type_adjusted'] = True

        # Ø­Ø°Ù Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø®ÛŒÙ„ÛŒ Ø¶Ø¹ÛŒÙ
        if signal['score'] >= 0.5:
            filtered_signals.append(signal)

    return filtered_signals
```

**Ø§Ø³ØªÙØ§Ø¯Ù‡:**

```python
# Ø¯Ø± ØªØ§Ø¨Ø¹ _analyze_macd
all_signals = macd_crosses + dif_behavior + hist_analysis + macd_divergence

# ÙÛŒÙ„ØªØ± Ø¨Ø± Ø§Ø³Ø§Ø³ market type
all_signals = self.filter_macd_signals_by_market_type(all_signals, market_type)
```

**Ù…Ø«Ø§Ù„:**
```
Market Type: X_transition (Ø¨Ø§Ø²Ø§Ø± Ø¯Ø± Ø­Ø§Ù„ ØªØºÛŒÛŒØ± Ø¬Ù‡Øª)
Ø³ÛŒÚ¯Ù†Ø§Ù„: macd_gold_cross_below_zero Ø¨Ø§ Ø§Ù…ØªÛŒØ§Ø² 2.5

Ø¨Ø¹Ø¯ Ø§Ø² ÙÛŒÙ„ØªØ±:
score = 2.5 Ã— 0.4 = 1.0 (Ú©Ø§Ù‡Ø´ 60%)
```

**Ù…Ø²Ø§ÛŒØ§:**
- Ú©Ø§Ù‡Ø´ false signals Ø¯Ø± market types Ù†Ø§Ù…Ù†Ø§Ø³Ø¨
- Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‚Øª Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ Ø¯Ø± Ø´Ø±Ø§ÛŒØ· Ù…Ù†Ø§Ø³Ø¨
- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¨Ù‡ØªØ± Ø§Ø² market type Ú©Ù‡ Ù‚Ø¨Ù„Ø§Ù‹ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´Ø¯

---

#### âŒ Ù…Ø´Ú©Ù„ 3: Ø¹Ø¯Ù… Ù…Ø­Ø§Ø³Ø¨Ù‡ Strength Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§

**Ù…Ø´Ú©Ù„ ÙØ¹Ù„ÛŒ:**
```python
# ÙÙ‚Ø· MACD crosses Ø¯Ø§Ø±Ø§ÛŒ strength Ù‡Ø³ØªÙ†Ø¯
cross_strength = min(1.0, abs(dif - dea) * 5)

# Ø¨Ù‚ÛŒÙ‡ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ (histogram, divergence, trendline) strength Ù†Ø¯Ø§Ø±Ù†Ø¯
```

- ÙÙ‚Ø· ØªÙ‚Ø§Ø·Ø¹â€ŒÙ‡Ø§ strength Ø¯Ø§Ø±Ù†Ø¯ØŒ Ø¨Ù‚ÛŒÙ‡ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ Ø«Ø§Ø¨Øª Ù‡Ø³ØªÙ†Ø¯
- ÛŒÚ© `macd_hist_bottom_divergence` Ø¶Ø¹ÛŒÙ Ù‡Ù…Ø§Ù† Ø§Ù…ØªÛŒØ§Ø² ÛŒÚ© divergence Ù‚ÙˆÛŒ Ø±Ø§ Ø¯Ø§Ø±Ø¯

**Ø±Ø§Ù‡ Ø­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**

```python
def calculate_signal_strength(self, signal_type: str, df: pd.DataFrame,
                              dif: pd.Series, dea: pd.Series,
                              hist: pd.Series) -> float:
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‚Ø¯Ø±Øª Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ Ø¢Ù†
    """

    if 'divergence' in signal_type:
        # Ù‚Ø¯Ø±Øª ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ÙØ§ØµÙ„Ù‡ Ø¨ÛŒÙ† Ù‚Ù„Ù‡â€ŒÙ‡Ø§/Ø¯Ø±Ù‡â€ŒÙ‡Ø§
        return self._calculate_divergence_strength(df, dif)

    elif 'trendline_break' in signal_type:
        # Ù‚Ø¯Ø±Øª Ø´Ú©Ø³Øª Ø®Ø· Ø±ÙˆÙ†Ø¯ Ø¨Ø± Ø§Ø³Ø§Ø³ momentum
        dif_momentum = abs(dif.iloc[-1] - dif.iloc[-5])
        return min(1.0, dif_momentum * 2)

    elif 'hist_shrink' in signal_type or 'hist_pull' in signal_type:
        # Ù‚Ø¯Ø±Øª Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù‚Ù„Ù‡/Ø¯Ø±Ù‡ histogram
        hist_peak_value = abs(hist.iloc[-1])
        hist_avg = hist.abs().mean()
        return min(1.0, hist_peak_value / hist_avg if hist_avg > 0 else 0.5)

    elif 'zero_cross' in signal_type:
        # Ù‚Ø¯Ø±Øª Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø±Ø¹Øª Ø¹Ø¨ÙˆØ± Ø§Ø² ØµÙØ±
        dif_change = abs(dif.iloc[-1] - dif.iloc[-3])
        return min(1.0, dif_change * 3)

    else:
        return 1.0  # Ù¾ÛŒØ´â€ŒÙØ±Ø¶


def _calculate_divergence_strength(self, df: pd.DataFrame, indicator: pd.Series) -> float:
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‚Ø¯Ø±Øª ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ
    """
    # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¯Ùˆ Ù‚Ù„Ù‡/Ø¯Ø±Ù‡ Ø§Ø®ÛŒØ±
    peaks, valleys = self.find_peaks_and_valleys(
        indicator.values,
        distance=self.macd_peak_detection_settings['distance'],
        prominence_factor=self.macd_peak_detection_settings['prominence_factor']
    )

    if len(peaks) >= 2:
        # ÙØ§ØµÙ„Ù‡ Ù‚ÛŒÙ…ØªÛŒ Ø¨ÛŒÙ† Ù‚Ù„Ù‡â€ŒÙ‡Ø§
        price_change = abs(df['close'].iloc[peaks[-1]] - df['close'].iloc[peaks[-2]])

        # ÙØ§ØµÙ„Ù‡ indicator Ø¨ÛŒÙ† Ù‚Ù„Ù‡â€ŒÙ‡Ø§
        indicator_change = abs(indicator.iloc[peaks[-1]] - indicator.iloc[peaks[-2]])

        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
        price_change_pct = price_change / df['close'].iloc[peaks[-2]]
        indicator_change_pct = indicator_change / abs(indicator.iloc[peaks[-2]]) if indicator.iloc[peaks[-2]] != 0 else 0

        # Ù‡Ø±Ú†Ù‡ divergence Ø¨ÛŒØ´ØªØ±ØŒ Ù‚ÙˆÛŒâ€ŒØªØ±
        divergence_gap = abs(price_change_pct - indicator_change_pct)

        return min(1.0, divergence_gap * 10)

    return 0.5  # Ù¾ÛŒØ´â€ŒÙØ±Ø¶
```

**Ø§Ø³ØªÙØ§Ø¯Ù‡:**

```python
# Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø³ÛŒÚ¯Ù†Ø§Ù„
for signal in all_signals:
    if 'strength' not in signal:
        signal['strength'] = self.calculate_signal_strength(
            signal['type'], df, dif, dea, hist
        )
        signal['score'] *= signal['strength']
```

**Ù…Ø²Ø§ÛŒØ§:**
- Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù‚ÙˆÛŒ Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø§Ù„Ø§ØªØ±ØŒ Ø¶Ø¹ÛŒÙâ€ŒÙ‡Ø§ Ø§Ù…ØªÛŒØ§Ø² Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ±
- ØªÙÚ©ÛŒÚ© Ø¨Ù‡ØªØ± Ø¨ÛŒÙ† Ú©ÛŒÙÛŒØª Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§
- Ú©Ø§Ù‡Ø´ ØªØ£Ø«ÛŒØ± Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¶Ø¹ÛŒÙ

---

#### âŒ Ù…Ø´Ú©Ù„ 4: Ø§Ù„Ú¯ÙˆÛŒ Kill Long Bin Ø¨Ø¯ÙˆÙ† Ù…Ø¹Ú©ÙˆØ³ (Kill Short Bin)

**Ù…Ø´Ú©Ù„ ÙØ¹Ù„ÛŒ:**
```python
# signal_generator.py:3481-3494
# ÙÙ‚Ø· Kill Long Bin Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ (Ø¨ÛŒÙ† Ø¯Ùˆ Ø¯Ø±Ù‡ Ù‡Ù…ÛŒØ´Ù‡ Ù…Ù†ÙÛŒ)
# Kill Short Bin (Ø¨ÛŒÙ† Ø¯Ùˆ Ù‚Ù„Ù‡ Ù‡Ù…ÛŒØ´Ù‡ Ù…Ø«Ø¨Øª) ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯
```

- ÙÙ‚Ø· Ø§Ù„Ú¯ÙˆÛŒ bearish (Kill Long) Ù¾ÛŒØ§Ø¯Ù‡ Ø´Ø¯Ù‡
- Ø§Ù„Ú¯ÙˆÛŒ Ù…Ø¹Ú©ÙˆØ³ Ø¨Ø±Ø§ÛŒ Ø³ÛŒÚ¯Ù†Ø§Ù„ bullish ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯

**Ø±Ø§Ù‡ Ø­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**

```python
def detect_macd_bins(self, hist: pd.Series, dates_index: pd.Index) -> List[Dict[str, Any]]:
    """
    Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Kill Long Bin Ùˆ Kill Short Bin
    """
    signals = []

    peaks_iloc, valleys_iloc = self.find_peaks_and_valleys(
        hist.values,
        distance=self.macd_peak_detection_settings['distance'],
        prominence_factor=self.macd_peak_detection_settings['prominence_factor']
    )

    # Kill Long Bin: Ø¨ÛŒÙ† Ø¯Ùˆ Ø¯Ø±Ù‡ Ù‡Ù…ÛŒØ´Ù‡ Ù…Ù†ÙÛŒ (ÙØ´Ø§Ø± ÙØ±ÙˆØ´ Ù…Ø¯Ø§ÙˆÙ…)
    if len(valleys_iloc) >= 2:
        for i in range(len(valleys_iloc) - 1):
            v1_rel, v2_rel = valleys_iloc[i], valleys_iloc[i + 1]
            v1_abs, v2_abs = dates_index[v1_rel], dates_index[v2_rel]

            if hist.iloc[v1_rel] < 0 and hist.iloc[v2_rel] < 0:
                hist_between = hist.iloc[v1_rel: v2_rel + 1]

                if not hist_between.empty and hist_between.max() < 0:
                    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‚Ø¯Ø±Øª Ø¨Ø± Ø§Ø³Ø§Ø³ Ø·ÙˆÙ„ bin Ùˆ Ø¹Ù…Ù‚
                    bin_length = v2_rel - v1_rel
                    bin_depth = abs(hist_between.mean())
                    strength = min(1.0, (bin_length * bin_depth) / 10)

                    signals.append({
                        'type': 'macd_hist_kill_long_bin',
                        'direction': 'bearish',
                        'index': v2_abs,
                        'date': v2_abs,
                        'score': self.pattern_scores.get('macd_hist_kill_long_bin', 2.0) * strength,
                        'strength': strength,
                        'details': {
                            'bin_length': bin_length,
                            'bin_depth': float(bin_depth)
                        }
                    })

    # Kill Short Bin: Ø¨ÛŒÙ† Ø¯Ùˆ Ù‚Ù„Ù‡ Ù‡Ù…ÛŒØ´Ù‡ Ù…Ø«Ø¨Øª (ÙØ´Ø§Ø± Ø®Ø±ÛŒØ¯ Ù…Ø¯Ø§ÙˆÙ…)
    if len(peaks_iloc) >= 2:
        for i in range(len(peaks_iloc) - 1):
            p1_rel, p2_rel = peaks_iloc[i], peaks_iloc[i + 1]
            p1_abs, p2_abs = dates_index[p1_rel], dates_index[p2_rel]

            if hist.iloc[p1_rel] > 0 and hist.iloc[p2_rel] > 0:
                hist_between = hist.iloc[p1_rel: p2_rel + 1]

                if not hist_between.empty and hist_between.min() > 0:
                    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‚Ø¯Ø±Øª
                    bin_length = p2_rel - p1_rel
                    bin_height = hist_between.mean()
                    strength = min(1.0, (bin_length * bin_height) / 10)

                    signals.append({
                        'type': 'macd_hist_kill_short_bin',
                        'direction': 'bullish',
                        'index': p2_abs,
                        'date': p2_abs,
                        'score': self.pattern_scores.get('macd_hist_kill_short_bin', 2.0) * strength,
                        'strength': strength,
                        'details': {
                            'bin_length': bin_length,
                            'bin_height': float(bin_height)
                        }
                    })

    return signals
```

**Ù…Ø²Ø§ÛŒØ§:**
- ØªÙ‚Ø§Ø±Ù† Ø¯Ø± Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ØµØ¹ÙˆØ¯ÛŒ Ùˆ Ù†Ø²ÙˆÙ„ÛŒ
- Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ÙØ´Ø§Ø± Ø®Ø±ÛŒØ¯ Ù…Ø¯Ø§ÙˆÙ… (Kill Short Bin)
- Ù…Ø­Ø§Ø³Ø¨Ù‡ strength Ø¨Ø± Ø§Ø³Ø§Ø³ Ø·ÙˆÙ„ Ùˆ Ø¹Ù…Ù‚ bin

---

#### âŒ Ù…Ø´Ú©Ù„ 5: Ø¹Ø¯Ù… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² MACD Zero-Lag ÛŒØ§ MACD Leader

**Ù…Ø´Ú©Ù„ ÙØ¹Ù„ÛŒ:**
```python
# MACD Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ ØªØ£Ø®ÛŒØ± (lag) Ø¯Ø§Ø±Ø¯
dif, dea, hist = talib.MACD(close, fastperiod=12, slowperiod=26, signalperiod=9)
```

- MACD Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² EMA Ø¯Ø§Ø±Ø§ÛŒ lag Ø§Ø³Øª
- Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ Ø¨Ø§ ØªØ£Ø®ÛŒØ± ØµØ§Ø¯Ø± Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
- Ù†Ø³Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡â€ŒØªØ± Ø¨Ø§ lag Ú©Ù…ØªØ± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ù†Ø¯

**Ø±Ø§Ù‡ Ø­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**

```python
def calculate_zero_lag_macd(self, close: np.ndarray,
                           fast: int = 12, slow: int = 26,
                           signal: int = 9) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ Zero-Lag MACD

    Zero-Lag MACD Ø§Ø² EMA + lag compensation Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    """

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ EMA Ù…Ø¹Ù…ÙˆÙ„ÛŒ
    ema_fast = talib.EMA(close, timeperiod=fast)
    ema_slow = talib.EMA(close, timeperiod=slow)

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ lag compensation
    # Lag â‰ˆ (period - 1) / 2
    lag_fast = (fast - 1) / 2
    lag_slow = (slow - 1) / 2

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ EMA Ø¯ÙˆØ¨Ø§Ø± Ø¨Ø±Ø§ÛŒ ØªØ®Ù…ÛŒÙ† lag
    ema_fast_2 = talib.EMA(ema_fast, timeperiod=fast)
    ema_slow_2 = talib.EMA(ema_slow, timeperiod=slow)

    # Zero-Lag EMA = 2*EMA - EMA(EMA)
    zlema_fast = 2 * ema_fast - ema_fast_2
    zlema_slow = 2 * ema_slow - ema_slow_2

    # Zero-Lag DIF
    zl_dif = zlema_fast - zlema_slow

    # Zero-Lag Signal
    zl_signal = talib.EMA(zl_dif, timeperiod=signal)
    zl_signal_2 = talib.EMA(zl_signal, timeperiod=signal)
    zl_dea = 2 * zl_signal - zl_signal_2

    # Zero-Lag Histogram
    zl_hist = zl_dif - zl_dea

    return zl_dif, zl_dea, zl_hist


def calculate_macd_leader(self, close: np.ndarray,
                          fast: int = 12, slow: int = 26) -> np.ndarray:
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ MACD Leader (Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©Ù†Ù†Ø¯Ù‡)

    MACD Leader = DIF + (DIF - DIF_Ù‚Ø¨Ù„ÛŒ)
    """

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ MACD Ù…Ø¹Ù…ÙˆÙ„ÛŒ
    dif, _, _ = talib.MACD(close, fastperiod=fast, slowperiod=slow, signalperiod=9)

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ momentum DIF
    dif_momentum = np.diff(dif, prepend=dif[0])

    # MACD Leader
    macd_leader = dif + dif_momentum

    return macd_leader
```

**Ø§Ø³ØªÙØ§Ø¯Ù‡ ØªØ±Ú©ÛŒØ¨ÛŒ:**

```python
def _analyze_macd_advanced(self, df: pd.DataFrame) -> Dict[str, Any]:
    """
    ØªØ­Ù„ÛŒÙ„ MACD Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‡Ø± Ø¯Ùˆ Ù†Ø³Ø®Ù‡ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ùˆ Zero-Lag
    """

    close = df['close'].values

    # MACD Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ (Ø¨Ø±Ø§ÛŒ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØªØ£ÛŒÛŒØ¯ Ø´Ø¯Ù‡)
    std_dif, std_dea, std_hist = talib.MACD(close, 12, 26, 9)

    # Zero-Lag MACD (Ø¨Ø±Ø§ÛŒ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø²ÙˆØ¯Ù‡Ù†Ú¯Ø§Ù…)
    zl_dif, zl_dea, zl_hist = self.calculate_zero_lag_macd(close, 12, 26, 9)

    # MACD Leader (Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ)
    macd_leader = self.calculate_macd_leader(close, 12, 26)

    # ØªØ±Ú©ÛŒØ¨ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§
    signals = []

    # 1. Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ (ÙˆØ²Ù† Ø¨Ø§Ù„Ø§ - ØªØ£ÛŒÛŒØ¯ Ø´Ø¯Ù‡)
    std_signals = self._detect_macd_signals(std_dif, std_dea, std_hist, df.index)
    for sig in std_signals:
        sig['source'] = 'standard'
        sig['weight'] = 1.0
        signals.append(sig)

    # 2. Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Zero-Lag (ÙˆØ²Ù† Ù…ØªÙˆØ³Ø· - Ø²ÙˆØ¯Ù‡Ù†Ú¯Ø§Ù…)
    zl_signals = self._detect_macd_signals(zl_dif, zl_dea, zl_hist, df.index)
    for sig in zl_signals:
        sig['source'] = 'zero_lag'
        sig['weight'] = 0.7  # ÙˆØ²Ù† Ú©Ù…ØªØ± Ú†ÙˆÙ† Ù…Ù…Ú©Ù† Ø§Ø³Øª false signal Ø¨Ø§Ø´Ø¯
        sig['score'] *= 0.7
        signals.append(sig)

    # 3. Ø³ÛŒÚ¯Ù†Ø§Ù„ Leader (ÙˆØ²Ù† Ù¾Ø§ÛŒÛŒÙ† - Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ)
    if len(macd_leader) >= 2:
        # cross Ø¨Ø§ zero
        if macd_leader[-2] < 0 and macd_leader[-1] > 0:
            signals.append({
                'type': 'macd_leader_cross_up',
                'direction': 'bullish',
                'score': 1.5,
                'source': 'leader',
                'weight': 0.5
            })

    return {'signals': signals, 'standard': std_dif, 'zero_lag': zl_dif, 'leader': macd_leader}
```

**Ù…Ø²Ø§ÛŒØ§:**
- Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø²ÙˆØ¯ØªØ± Ø¨Ø§ Zero-Lag MACD
- ØªØ£ÛŒÛŒØ¯ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ Ø¨Ø§ MACD Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯
- Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§ MACD Leader
- Ú©Ø§Ù‡Ø´ lag Ø¯Ø± ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ

---

#### âŒ Ù…Ø´Ú©Ù„ 6: Ø¹Ø¯Ù… validation Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ MACD Ø¨Ø§ Price Action

**Ù…Ø´Ú©Ù„ ÙØ¹Ù„ÛŒ:**
- Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ MACD Ø¨Ø¯ÙˆÙ† Ø¨Ø±Ø±Ø³ÛŒ price action ØµØ§Ø¯Ø± Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
- Ù…Ù…Ú©Ù† Ø§Ø³Øª MACD ØµØ¹ÙˆØ¯ÛŒ Ø¨Ø§Ø´Ø¯ ÙˆÙ„ÛŒ Ù‚ÛŒÙ…Øª Ø¯Ø± Ø­Ø§Ù„ Ø´Ú©Ø³Øª support Ø¨Ø§Ø´Ø¯

**Ø±Ø§Ù‡ Ø­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**

```python
def _is_near_support(self, price: float, sr_levels: Dict, threshold_pct: float = 0.02) -> bool:
    """Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø²Ø¯ÛŒÚ©ÛŒ Ù‚ÛŒÙ…Øª Ø¨Ù‡ Ø³Ø·ÙˆØ­ support"""
    supports = sr_levels.get('support', [])
    for support in supports:
        if abs(price - support) / support <= threshold_pct:
            return True
    return False

def _is_near_resistance(self, price: float, sr_levels: Dict, threshold_pct: float = 0.02) -> bool:
    """Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø²Ø¯ÛŒÚ©ÛŒ Ù‚ÛŒÙ…Øª Ø¨Ù‡ Ø³Ø·ÙˆØ­ resistance"""
    resistances = sr_levels.get('resistance', [])
    for resistance in resistances:
        if abs(price - resistance) / resistance <= threshold_pct:
            return True
    return False

def is_volume_confirmed(self, df: pd.DataFrame, threshold: float = 1.2) -> bool:
    """Ø¨Ø±Ø±Ø³ÛŒ ØªØ£ÛŒÛŒØ¯ Ø­Ø¬Ù… Ø¨Ø±Ø§ÛŒ Ø­Ø±Ú©Øª ÙØ¹Ù„ÛŒ"""
    if len(df) < 20:
        return False
    current_volume = df['volume'].iloc[-1]
    avg_volume = df['volume'].iloc[-20:].mean()
    return current_volume > avg_volume * threshold

def validate_macd_with_price_action(self, macd_signals: List[Dict],
                                     df: pd.DataFrame,
                                     sr_levels: Dict) -> List[Dict]:
    """
    Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ MACD Ø¨Ø§ price action
    """

    validated_signals = []
    current_price = df['close'].iloc[-1]

    for signal in macd_signals:
        direction = signal['direction']
        validation_score = 1.0

        # 1. Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø²Ø¯ÛŒÚ©ÛŒ Ø¨Ù‡ Ø³Ø·ÙˆØ­ S/R
        if direction == 'bullish':
            # Ø¢ÛŒØ§ Ù†Ø²Ø¯ÛŒÚ© support Ù‡Ø³ØªÛŒÙ…ØŸ
            near_support = self._is_near_support(current_price, sr_levels)
            if near_support:
                validation_score *= 1.3  # ØªÙ‚ÙˆÛŒØª

            # Ø¢ÛŒØ§ Ù†Ø²Ø¯ÛŒÚ© resistance Ù‡Ø³ØªÛŒÙ…ØŸ
            near_resistance = self._is_near_resistance(current_price, sr_levels)
            if near_resistance:
                validation_score *= 0.6  # ØªØ¶Ø¹ÛŒÙ

        elif direction == 'bearish':
            near_resistance = self._is_near_resistance(current_price, sr_levels)
            if near_resistance:
                validation_score *= 1.3

            near_support = self._is_near_support(current_price, sr_levels)
            if near_support:
                validation_score *= 0.6

        # 2. Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ Ø§Ø®ÛŒØ±
        recent_candles = df.tail(3)
        bullish_candles = sum(1 for _, row in recent_candles.iterrows() if row['close'] > row['open'])

        if direction == 'bullish' and bullish_candles >= 2:
            validation_score *= 1.2  # Ù‡Ù…Ø§Ù‡Ù†Ú¯ÛŒ price action
        elif direction == 'bearish' and bullish_candles <= 1:
            validation_score *= 1.2

        # 3. Ø¨Ø±Ø±Ø³ÛŒ Ø­Ø¬Ù…
        volume_confirmed = self.is_volume_confirmed(df)
        if volume_confirmed:
            validation_score *= 1.15
        else:
            validation_score *= 0.85

        # Ø§Ø¹Ù…Ø§Ù„ validation
        signal['score'] *= validation_score
        signal['validation_score'] = validation_score
        signal['validated'] = True

        validated_signals.append(signal)

    return validated_signals
```

**Ù…Ø²Ø§ÛŒØ§:**
- ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø®Ù„Ø§Ù price action
- ØªÙ‚ÙˆÛŒØª Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù‡Ù…Ø§Ù‡Ù†Ú¯ Ø¨Ø§ S/R
- ØªØ±Ú©ÛŒØ¨ MACD Ø¨Ø§ Ø­Ø¬Ù… Ùˆ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ

---

### ğŸ“‹ Ø®Ù„Ø§ØµÙ‡ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª

| # | Ù…Ø´Ú©Ù„ | Ø±Ø§Ù‡ Ø­Ù„ | Ø§ÙˆÙ„ÙˆÛŒØª | ØªØ£Ø«ÛŒØ± Ø¨Ø± Ø¯Ù‚Øª |
|---|------|--------|---------|--------------|
| 1 | Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø«Ø§Ø¨Øª MACD | Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø§Ù†Ø·Ø¨Ø§Ù‚ÛŒ Ø¨Ø§ timeframe Ùˆ regime | ğŸŸ¡ Ù…ØªÙˆØ³Ø· | +10% |
| 2 | Ø¹Ø¯Ù… ÙÛŒÙ„ØªØ± Ø¨Ø§ Market Type | ØªØ¹Ø¯ÛŒÙ„ Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ market type | ğŸ”´ Ø¨Ø§Ù„Ø§ | +18% |
| 3 | ÙÙ‚Ø¯Ø§Ù† Strength Ø¯Ø± Ù‡Ù…Ù‡ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ | Ù…Ø­Ø§Ø³Ø¨Ù‡ strength Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ù… Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ | ğŸŸ¡ Ù…ØªÙˆØ³Ø· | +12% |
| 4 | ÙÙ‚Ø· Kill Long Bin | Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Kill Short Bin | ğŸŸ¢ Ù¾Ø§ÛŒÛŒÙ† | +5% |
| 5 | Lag Ø¨Ø§Ù„Ø§ÛŒ MACD | Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Zero-Lag MACD | ğŸ”´ Ø¨Ø§Ù„Ø§ | +15% |
| 6 | Ø¹Ø¯Ù… validation Ø¨Ø§ Price Action | Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø¨Ø§ S/R Ùˆ candles | ğŸŸ¡ Ù…ØªÙˆØ³Ø· | +14% |

**ØªØ£Ø«ÛŒØ± Ú©Ù„ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª:** Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‚Øª Ø­Ø¯ÙˆØ¯ **+45-55%** Ø¯Ø± ØªØ­Ù„ÛŒÙ„ MACD

---

### ğŸ”¬ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª ØªØ³Øª Ùˆ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ

1. **A/B Testing MACD Parameters:**
   - Ù…Ù‚Ø§ÛŒØ³Ù‡ MACD Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ vs Ø§Ù†Ø·Ø¨Ø§Ù‚ÛŒ
   - ØªØ³Øª Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¨Ø±Ø§ÛŒ Ù‡Ø± timeframe
   - Ø§Ù†Ø¯Ø§Ø²Ù‡â€ŒÚ¯ÛŒØ±ÛŒ win rate Ùˆ profit factor

2. **Backtesting Market Type Filtering:**
   - Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ø§/Ø¨Ø¯ÙˆÙ† ÙÛŒÙ„ØªØ± market type
   - ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù‡Ø´ false signals Ø¯Ø± X_transition
   - Ø§Ù†Ø¯Ø§Ø²Ù‡â€ŒÚ¯ÛŒØ±ÛŒ ØªØ£Ø«ÛŒØ± Ø¯Ø± Ù‡Ø± market type

3. **Zero-Lag vs Standard MACD:**
   - Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø³Ø±Ø¹Øª Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§
   - ØªØ­Ù„ÛŒÙ„ false positives Ø¯Ø± Zero-Lag
   - ÛŒØ§ÙØªÙ† ÙˆØ²Ù† Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ±Ú©ÛŒØ¨ Ø¯Ùˆ Ù†Ø³Ø®Ù‡

4. **Validation Impact:**
   - ØªØ³Øª ØªØ£Ø«ÛŒØ± validation Ø¨Ø§ price action
   - Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ validated vs non-validated
   - ØªØ­Ù„ÛŒÙ„ Ø¯Ø± Ø´Ø±Ø§ÛŒØ· Ù…Ø®ØªÙ„Ù Ø¨Ø§Ø²Ø§Ø±

---

## 5. ØªØ­Ù„ÛŒÙ„ Price Action (Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ Ùˆ ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ÙÙ†ÛŒ)

**ğŸ“ Ú©Ø¯ Ù…Ø±Ø¬Ø¹:** `signal_generator.py:3867-4014` - ØªØ§Ø¨Ø¹ `analyze_price_action()`

### Ù…Ø´Ú©Ù„Ø§Øª Ùˆ Ù…Ø­Ø¯ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ ÙØ¹Ù„ÛŒ

#### âŒ Ù…Ø´Ú©Ù„ 1: Ø¹Ø¯Ù… Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Context Ù…Ø­Ù„ Ø§Ù„Ú¯Ùˆ

**Ù…Ø´Ú©Ù„ ÙØ¹Ù„ÛŒ:**
```python
# signal_generator.py:1931-1945
# Ø§Ù„Ú¯ÙˆÙ‡Ø§ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ ÙˆÙ„ÛŒ context Ù…Ø­Ù„ÛŒ (S/RØŒ Ø±ÙˆÙ†Ø¯) Ø¨Ø±Ø±Ø³ÛŒ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯
patterns_found.append({
    'type': pattern_name,
    'direction': pattern_direction,
    'score': pattern_score
})
```

- Ø§Ù„Ú¯Ùˆ Ø¯Ø± Ù†Ø²Ø¯ÛŒÚ©ÛŒ Support/Resistance Ø¨Ø±Ø±Ø³ÛŒ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯
- Ø§Ù„Ú¯Ùˆ Ø¯Ø± Ø¬Ù‡Øª Ø±ÙˆÙ†Ø¯ ÛŒØ§ Ø®Ù„Ø§Ù Ø±ÙˆÙ†Ø¯ Ú†Ú© Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯
- Ú©ÛŒÙÛŒØª Ø§Ù„Ú¯Ùˆ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø­Ù„ Ù‚Ø±Ø§Ø±Ú¯ÛŒØ±ÛŒ ØªÙ†Ø¸ÛŒÙ… Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯

**Ø±Ø§Ù‡ Ø­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**

```python
def evaluate_pattern_context(self, pattern: Dict, df: pd.DataFrame,
                             sr_levels: Dict, trend_data: Dict) -> Dict:
    """
    Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ context Ù…Ø­Ù„ÛŒ Ø§Ù„Ú¯Ùˆ Ùˆ ØªÙ†Ø¸ÛŒÙ… Ø§Ù…ØªÛŒØ§Ø²
    """

    pattern_type = pattern['type']
    pattern_direction = pattern['direction']
    current_price = df['close'].iloc[-1]

    context_score = 1.0
    context_notes = []

    # 1. Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø²Ø¯ÛŒÚ©ÛŒ Ø¨Ù‡ Ø³Ø·ÙˆØ­ S/R
    support_levels = sr_levels.get('support_levels', [])
    resistance_levels = sr_levels.get('resistance_levels', [])

    is_near_support = any(abs(current_price - s['price']) / current_price < 0.01
                          for s in support_levels)
    is_near_resistance = any(abs(current_price - r['price']) / current_price < 0.01
                             for r in resistance_levels)

    # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø¨Ø±Ú¯Ø´ØªÛŒ Ø¯Ø± Ù…Ø­Ù„ Ù…Ù†Ø§Ø³Ø¨
    is_reversal_pattern = pattern_type in ['hammer', 'morning_star', 'evening_star',
                                           'shooting_star', 'head_and_shoulders']

    if is_reversal_pattern:
        if pattern_direction == 'bullish' and is_near_support:
            context_score *= 1.5  # Ø§Ù„Ú¯ÙˆÛŒ Ø¨Ø±Ú¯Ø´Øª ØµØ¹ÙˆØ¯ÛŒ Ø¯Ø± support = Ø¹Ø§Ù„ÛŒ
            context_notes.append('bullish_reversal_at_support')
        elif pattern_direction == 'bearish' and is_near_resistance:
            context_score *= 1.5  # Ø§Ù„Ú¯ÙˆÛŒ Ø¨Ø±Ú¯Ø´Øª Ù†Ø²ÙˆÙ„ÛŒ Ø¯Ø± resistance = Ø¹Ø§Ù„ÛŒ
            context_notes.append('bearish_reversal_at_resistance')
        elif pattern_direction == 'bullish' and is_near_resistance:
            context_score *= 0.5  # Ø§Ù„Ú¯ÙˆÛŒ Ø¨Ø±Ú¯Ø´Øª ØµØ¹ÙˆØ¯ÛŒ Ø¯Ø± resistance = Ø¶Ø¹ÛŒÙ
            context_notes.append('bullish_reversal_at_resistance_weak')
        elif pattern_direction == 'bearish' and is_near_support:
            context_score *= 0.5  # Ø§Ù„Ú¯ÙˆÛŒ Ø¨Ø±Ú¯Ø´Øª Ù†Ø²ÙˆÙ„ÛŒ Ø¯Ø± support = Ø¶Ø¹ÛŒÙ
            context_notes.append('bearish_reversal_at_support_weak')

    # 2. Ø¨Ø±Ø±Ø³ÛŒ Ù‡Ù…Ø³ÙˆÛŒÛŒ Ø¨Ø§ Ø±ÙˆÙ†Ø¯
    trend_direction = trend_data.get('trend', 'neutral')
    trend_strength = abs(trend_data.get('strength', 0))

    is_continuation_pattern = pattern_type in ['bull_flag', 'bear_flag',
                                               'ascending_triangle', 'descending_triangle']

    if is_continuation_pattern:
        # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø§Ø¯Ø§Ù…Ù‡â€ŒØ¯Ù‡Ù†Ø¯Ù‡ Ø¨Ø§ÛŒØ¯ Ø¨Ø§ Ø±ÙˆÙ†Ø¯ Ù‡Ù…Ø³Ùˆ Ø¨Ø§Ø´Ù†Ø¯
        if (pattern_direction == 'bullish' and trend_direction == 'bullish') or \
           (pattern_direction == 'bearish' and trend_direction == 'bearish'):
            context_score *= (1.0 + trend_strength * 0.2)
            context_notes.append('continuation_with_trend')
        else:
            context_score *= 0.6
            context_notes.append('continuation_against_trend_weak')

    # 3. Ø¨Ø±Ø±Ø³ÛŒ Ø­Ø¬Ù…
    if 'volume' in df.columns:
        recent_volume = df['volume'].iloc[-3:].mean()
        avg_volume = df['volume'].iloc[-30:-3].mean()
        volume_ratio = recent_volume / avg_volume if avg_volume > 0 else 1.0

        if volume_ratio > 1.5:
            context_score *= 1.2
            context_notes.append('confirmed_by_volume')
        elif volume_ratio < 0.7:
            context_score *= 0.85
            context_notes.append('weak_volume')

    return {
        'context_score': context_score,
        'context_notes': context_notes,
        'is_near_support': is_near_support,
        'is_near_resistance': is_near_resistance,
        'trend_aligned': (pattern_direction == trend_direction)
    }
```

**Ø§Ø³ØªÙØ§Ø¯Ù‡:**

```python
# Ø¯Ø± ØªØ§Ø¨Ø¹ analyze_price_action
for pattern in patterns_found:
    context = self.evaluate_pattern_context(pattern, df, sr_levels, trend_data)
    pattern['score'] *= context['context_score']
    pattern['context'] = context
```

**Ù…Ø²Ø§ÛŒØ§:**
- Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù‚ÙˆÛŒ Ø¯Ø± Ù…Ø­Ù„ Ù…Ù†Ø§Ø³Ø¨ Ø§Ù…ØªÛŒØ§Ø² Ø¨ÛŒØ´ØªØ±
- Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø¶Ø¹ÛŒÙ Ø¯Ø± Ù…Ø­Ù„ Ù†Ø§Ù…Ù†Ø§Ø³Ø¨ ÙÛŒÙ„ØªØ± Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
- Ú©Ø§Ù‡Ø´ false signals

---

#### âŒ Ù…Ø´Ú©Ù„ 2: Pattern Quality Ø¨Ø±Ø§ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ØªÚ©-Ú©Ù†Ø¯Ù„ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯

**Ù…Ø´Ú©Ù„ ÙØ¹Ù„ÛŒ:**
```python
# signal_generator.py:1931-1936
# ÙÙ‚Ø· pattern_strength Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
pattern_strength = min(1.0, abs(pattern_value) / 100)
```

- Ú©ÛŒÙÛŒØª Ø§Ù„Ú¯Ùˆ ÙÙ‚Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ pattern_value Ø§Ø³Øª
- Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¨Ø¯Ù†Ù‡ØŒ Ø³Ø§ÛŒÙ‡â€ŒÙ‡Ø§ØŒ Ùˆ Ù†Ø³Ø¨Øªâ€ŒÙ‡Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯
- Ù‡Ù…Ù‡ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ÛŒÚ© Ù†ÙˆØ¹ Ø§Ù…ØªÛŒØ§Ø² ÛŒÚ©Ø³Ø§Ù†ÛŒ Ø¯Ø§Ø±Ù†Ø¯

**Ø±Ø§Ù‡ Ø­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**

```python
def calculate_candle_pattern_quality(self, df: pd.DataFrame, pattern_type: str) -> float:
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©ÛŒÙÛŒØª Ø§Ù„Ú¯ÙˆÛŒ Ø´Ù…Ø¹ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ù†Ø¯Ù„
    """

    last_candle = df.iloc[-1]
    open_p = last_candle['open']
    high_p = last_candle['high']
    low_p = last_candle['low']
    close_p = last_candle['close']

    body = abs(close_p - open_p)
    total_range = high_p - low_p
    upper_shadow = high_p - max(open_p, close_p)
    lower_shadow = min(open_p, close_p) - low_p

    quality = 1.0

    # Ú©ÛŒÙÛŒØª Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ Ø§Ù„Ú¯Ùˆ
    if pattern_type == 'hammer':
        # Hammer Ø¨Ø§ÛŒØ¯: Ø¨Ø¯Ù†Ù‡ Ú©ÙˆÚ†Ú©ØŒ Ø³Ø§ÛŒÙ‡ Ù¾Ø§ÛŒÛŒÙ† Ø¨Ù„Ù†Ø¯ØŒ Ø³Ø§ÛŒÙ‡ Ø¨Ø§Ù„Ø§ Ú©ÙˆÚ†Ú©
        body_ratio = body / total_range if total_range > 0 else 0
        lower_shadow_ratio = lower_shadow / total_range if total_range > 0 else 0
        upper_shadow_ratio = upper_shadow / total_range if total_range > 0 else 0

        # Ú©ÛŒÙÛŒØª Ø¨Ø§Ù„Ø§: Ø¨Ø¯Ù†Ù‡ Ú©ÙˆÚ†Ú© (< 30%) Ùˆ Ø³Ø§ÛŒÙ‡ Ù¾Ø§ÛŒÛŒÙ† Ø¨Ù„Ù†Ø¯ (> 60%)
        if body_ratio < 0.3 and lower_shadow_ratio > 0.6 and upper_shadow_ratio < 0.1:
            quality = 1.0
        elif body_ratio < 0.4 and lower_shadow_ratio > 0.5:
            quality = 0.8
        else:
            quality = 0.5

    elif pattern_type == 'shooting_star':
        # Shooting Star: Ø¨Ø¯Ù†Ù‡ Ú©ÙˆÚ†Ú©ØŒ Ø³Ø§ÛŒÙ‡ Ø¨Ø§Ù„Ø§ Ø¨Ù„Ù†Ø¯ØŒ Ø³Ø§ÛŒÙ‡ Ù¾Ø§ÛŒÛŒÙ† Ú©ÙˆÚ†Ú©
        body_ratio = body / total_range if total_range > 0 else 0
        upper_shadow_ratio = upper_shadow / total_range if total_range > 0 else 0
        lower_shadow_ratio = lower_shadow / total_range if total_range > 0 else 0

        if body_ratio < 0.3 and upper_shadow_ratio > 0.6 and lower_shadow_ratio < 0.1:
            quality = 1.0
        elif body_ratio < 0.4 and upper_shadow_ratio > 0.5:
            quality = 0.8
        else:
            quality = 0.5

    elif pattern_type == 'doji':
        # Doji: Ø¨Ø¯Ù†Ù‡ Ø®ÛŒÙ„ÛŒ Ú©ÙˆÚ†Ú©
        body_ratio = body / total_range if total_range > 0 else 0

        if body_ratio < 0.05:  # Ø¨Ø¯Ù†Ù‡ Ú©Ù…ØªØ± Ø§Ø² 5%
            quality = 1.0
        elif body_ratio < 0.1:
            quality = 0.7
        else:
            quality = 0.4

    elif pattern_type == 'engulfing':
        # Engulfing: Ú©Ù†Ø¯Ù„ Ø¯ÙˆÙ… Ø¨Ø§ÛŒØ¯ Ú©Ù†Ø¯Ù„ Ø§ÙˆÙ„ Ø±Ø§ Ú©Ø§Ù…Ù„Ø§Ù‹ Ø¨Ù¾ÙˆØ´Ø§Ù†Ø¯
        if len(df) < 2:
            return 0.5

        prev_candle = df.iloc[-2]
        prev_body = abs(prev_candle['close'] - prev_candle['open'])

        engulf_ratio = body / prev_body if prev_body > 0 else 1.0

        if engulf_ratio > 1.5:  # Ú©Ù†Ø¯Ù„ 50% Ø¨Ø²Ø±Ú¯ØªØ±
            quality = 1.0
        elif engulf_ratio > 1.2:
            quality = 0.8
        elif engulf_ratio > 1.0:
            quality = 0.6
        else:
            quality = 0.3

    elif pattern_type == 'marubozu':
        # Marubozu: Ø¨Ø¯ÙˆÙ† Ø³Ø§ÛŒÙ‡ ÛŒØ§ Ø³Ø§ÛŒÙ‡ Ø®ÛŒÙ„ÛŒ Ú©ÙˆÚ†Ú©
        shadow_ratio = (upper_shadow + lower_shadow) / total_range if total_range > 0 else 0

        if shadow_ratio < 0.05:  # Ø³Ø§ÛŒÙ‡ Ú©Ù…ØªØ± Ø§Ø² 5%
            quality = 1.0
        elif shadow_ratio < 0.1:
            quality = 0.7
        else:
            quality = 0.4

    # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ú©Ù†Ø¯Ù„ Ù†Ø³Ø¨Øª Ø¨Ù‡ ATR
    if len(df) >= 14:
        atr = talib.ATR(df['high'].values, df['low'].values, df['close'].values, timeperiod=14)
        if not np.isnan(atr[-1]):
            candle_size_ratio = total_range / atr[-1]
            if candle_size_ratio > 1.5:  # Ú©Ù†Ø¯Ù„ Ø¨Ø²Ø±Ú¯ØªØ± Ø§Ø² ATR
                quality *= 1.2
            elif candle_size_ratio < 0.5:  # Ú©Ù†Ø¯Ù„ Ø®ÛŒÙ„ÛŒ Ú©ÙˆÚ†Ú©
                quality *= 0.7

    return min(1.0, quality)
```

**Ø§Ø³ØªÙØ§Ø¯Ù‡:**

```python
# Ø¯Ø± ØªØ§Ø¨Ø¹ detect_candlestick_patterns
pattern_quality = self.calculate_candle_pattern_quality(df, pattern_name)
pattern_score = base_score * pattern_strength * pattern_quality
```

**Ù…Ø²Ø§ÛŒØ§:**
- ØªÙÚ©ÛŒÚ© Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø¨Ø§ Ú©ÛŒÙÛŒØª Ø¨Ø§Ù„Ø§ Ø§Ø² Ù¾Ø§ÛŒÛŒÙ†
- Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±
- Ú©Ø§Ù‡Ø´ false positives

---

#### âŒ Ù…Ø´Ú©Ù„ 3: Ø¹Ø¯Ù… ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù‚ÛŒÙ…ØªÛŒ Ù…Ù‡Ù… (Double Top/BottomØŒ Cup & Handle)

**Ù…Ø´Ú©Ù„ ÙØ¹Ù„ÛŒ:**
```python
# signal_generator.py:1955-1975
# ÙÙ‚Ø· H&SØŒ TriangleØŒ Flag Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
```

- Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ Ø¯ÛŒÚ¯Ø± Ù…Ø§Ù†Ù†Ø¯ Double Top/Bottom Ù†ÛŒØ³ØªÙ†Ø¯
- Cup & Handle Ú©Ù‡ Ø§Ù„Ú¯ÙˆÛŒ Ø§Ø¯Ø§Ù…Ù‡â€ŒØ¯Ù‡Ù†Ø¯Ù‡ Ù‚ÙˆÛŒ Ø§Ø³Øª ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯
- Rising/Falling Wedge Ù†ÛŒØ³Øª

**Ø±Ø§Ù‡ Ø­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**

```python
async def _detect_double_top_bottom(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
    """
    ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Double Top Ùˆ Double Bottom
    """
    patterns = []
    if len(df) < 30:
        return patterns

    closes = df['close'].values
    highs = df['high'].values
    lows = df['low'].values

    peaks, valleys = self.find_peaks_and_valleys(closes, distance=5, prominence_factor=0.05)

    # Double Top
    if len(peaks) >= 2:
        for i in range(len(peaks) - 1):
            peak1_idx = peaks[i]
            peak2_idx = peaks[i + 1]

            peak1_price = highs[peak1_idx]
            peak2_price = highs[peak2_idx]

            # Ø¯Ùˆ Ù‚Ù„Ù‡ Ø¨Ø§ÛŒØ¯ ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ Ù‡Ù…â€ŒØ³Ø·Ø­ Ø¨Ø§Ø´Ù†Ø¯ (< 2% Ø§Ø®ØªÙ„Ø§Ù)
            price_diff_pct = abs(peak2_price - peak1_price) / peak1_price

            if price_diff_pct < 0.02:
                # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¯Ø±Ù‡ Ø¨ÛŒÙ† Ø¯Ùˆ Ù‚Ù„Ù‡ (neckline)
                valleys_between = [v for v in valleys if v > peak1_idx and v < peak2_idx]

                if valleys_between:
                    valley_idx = valleys_between[0]
                    neckline_price = lows[valley_idx]

                    # Ø¨Ø±Ø±Ø³ÛŒ Ø´Ú©Ø³Øª neckline
                    current_price = closes[-1]
                    breakout_confirmed = current_price < neckline_price

                    # Ù…Ø­Ø§Ø³Ø¨Ù‡ target
                    pattern_height = peak1_price - neckline_price
                    price_target = neckline_price - pattern_height

                    # Ù…Ø­Ø§Ø³Ø¨Ù‡ quality
                    time_gap = peak2_idx - peak1_idx
                    pattern_quality = (1.0 - price_diff_pct) * min(1.0, time_gap / 20)

                    patterns.append({
                        'type': 'double_top',
                        'direction': 'bearish',
                        'index': peak2_idx,
                        'breakout_confirmed': breakout_confirmed,
                        'neckline_price': float(neckline_price),
                        'price_target': float(price_target),
                        'pattern_quality': round(pattern_quality, 2),
                        'score': self.pattern_scores.get('double_top', 3.5) * pattern_quality
                    })

    # Double Bottom (Ù‡Ù…Ø§Ù† Ù…Ù†Ø·Ù‚ Ø¨Ø§ valleys)
    if len(valleys) >= 2:
        for i in range(len(valleys) - 1):
            valley1_idx = valleys[i]
            valley2_idx = valleys[i + 1]

            valley1_price = lows[valley1_idx]
            valley2_price = lows[valley2_idx]

            price_diff_pct = abs(valley2_price - valley1_price) / valley1_price

            if price_diff_pct < 0.02:
                peaks_between = [p for p in peaks if p > valley1_idx and p < valley2_idx]

                if peaks_between:
                    peak_idx = peaks_between[0]
                    neckline_price = highs[peak_idx]

                    current_price = closes[-1]
                    breakout_confirmed = current_price > neckline_price

                    pattern_height = neckline_price - valley1_price
                    price_target = neckline_price + pattern_height

                    time_gap = valley2_idx - valley1_idx
                    pattern_quality = (1.0 - price_diff_pct) * min(1.0, time_gap / 20)

                    patterns.append({
                        'type': 'double_bottom',
                        'direction': 'bullish',
                        'index': valley2_idx,
                        'breakout_confirmed': breakout_confirmed,
                        'neckline_price': float(neckline_price),
                        'price_target': float(price_target),
                        'pattern_quality': round(pattern_quality, 2),
                        'score': self.pattern_scores.get('double_bottom', 3.5) * pattern_quality
                    })

    return patterns


async def _detect_cup_and_handle(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
    """
    ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÛŒ Cup and Handle
    """
    patterns = []
    if len(df) < 50:
        return patterns

    closes = df['close'].values
    lows = df['low'].values

    # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Cup (ÛŒÚ© Ø¯Ø±Ù‡ Ø¨Ø²Ø±Ú¯ U-shape)
    _, valleys = self.find_peaks_and_valleys(closes, distance=10, prominence_factor=0.05)

    if len(valleys) < 1:
        return patterns

    # Ø¨Ø±Ø±Ø³ÛŒ Ø¢Ø®Ø±ÛŒÙ† Ø¯Ø±Ù‡ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ú©Ù cup
    cup_bottom_idx = valleys[-1]

    # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø´Ø±ÙˆØ¹ Ùˆ Ù¾Ø§ÛŒØ§Ù† cup (Ø¨Ø§ÛŒØ¯ Ù‚Ø¨Ù„ Ùˆ Ø¨Ø¹Ø¯ Ø§Ø² Ú©Ù ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ Ù‡Ù…â€ŒØ³Ø·Ø­ Ø¨Ø§Ø´Ù†Ø¯)
    window_before = 20
    window_after = 15

    if cup_bottom_idx < window_before or cup_bottom_idx + window_after >= len(closes):
        return patterns

    left_rim = closes[cup_bottom_idx - window_before]
    right_rim = closes[cup_bottom_idx + window_after]
    cup_bottom = lows[cup_bottom_idx]

    rim_diff_pct = abs(right_rim - left_rim) / left_rim

    # Ù„Ø¨Ù‡â€ŒÙ‡Ø§ÛŒ cup Ø¨Ø§ÛŒØ¯ ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ Ù‡Ù…â€ŒØ³Ø·Ø­ Ø¨Ø§Ø´Ù†Ø¯
    if rim_diff_pct > 0.05:
        return patterns

    # Handle Ø¨Ø§ÛŒØ¯ Ø§ØµÙ„Ø§Ø­ Ú©ÙˆÚ†Ú© Ø¨Ø¹Ø¯ Ø§Ø² right rim Ø¨Ø§Ø´Ø¯
    handle_start_idx = cup_bottom_idx + window_after
    handle_window = 10

    if handle_start_idx + handle_window >= len(closes):
        return patterns

    handle_prices = closes[handle_start_idx:handle_start_idx + handle_window]
    handle_low = handle_prices.min()
    handle_depth = (right_rim - handle_low) / right_rim

    # Handle Ø¨Ø§ÛŒØ¯ Ø§ØµÙ„Ø§Ø­ Ú©ÙˆÚ†Ú© Ø¨Ø§Ø´Ø¯ (< 15%)
    if handle_depth > 0.15 or handle_depth < 0.03:
        return patterns

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ target
    cup_depth = right_rim - cup_bottom
    price_target = right_rim + cup_depth

    pattern_quality = (1.0 - rim_diff_pct) * (1.0 - handle_depth / 0.15)

    patterns.append({
        'type': 'cup_and_handle',
        'direction': 'bullish',
        'index': handle_start_idx + handle_window - 1,
        'rim_price': float(right_rim),
        'price_target': float(price_target),
        'pattern_quality': round(pattern_quality, 2),
        'score': self.pattern_scores.get('cup_and_handle', 3.8) * pattern_quality
    })

    return patterns
```

**Ø§Ø³ØªÙØ§Ø¯Ù‡:**

```python
# Ø¯Ø± ØªØ§Ø¨Ø¹ _detect_multi_candle_patterns
double_patterns = await self._detect_double_top_bottom(df)
if double_patterns:
    patterns.extend(double_patterns)

cup_handle = await self._detect_cup_and_handle(df)
if cup_handle:
    patterns.extend(cup_handle)
```

**Ù…Ø²Ø§ÛŒØ§:**
- Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ Ø§Ø¶Ø§ÙÛŒ
- Ø§ÙØ²Ø§ÛŒØ´ ØªÙ†ÙˆØ¹ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§
- Double Top/Bottom Ø§Ø² Ù…Ø­Ø¨ÙˆØ¨â€ŒØªØ±ÛŒÙ† Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ù‡Ø³ØªÙ†Ø¯

---

#### âŒ Ù…Ø´Ú©Ù„ 4: Bollinger Bands ÙÙ‚Ø· Break Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯

**Ù…Ø´Ú©Ù„ ÙØ¹Ù„ÛŒ:**
```python
# signal_generator.py:3936-3947
# ÙÙ‚Ø· upper_break Ùˆ lower_break Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
if current_close > current_upper:
    signals.append({'type': 'bollinger_upper_break'})
```

- BB Bounce (Ø¨Ø±Ú¯Ø´Øª Ø§Ø² Ø¨Ø§Ù†Ø¯Ù‡Ø§) Ø¨Ø±Ø±Ø³ÛŒ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯
- BB Walk (Ø­Ø±Ú©Øª Ø±ÙˆÛŒ Ø¨Ø§Ù†Ø¯) ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯
- ØªØºÛŒÛŒØ±Ø§Øª width Ø¯Ø± Ø·ÙˆÙ„ Ø²Ù…Ø§Ù† Ø±Ø¯ÛŒØ§Ø¨ÛŒ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯

**Ø±Ø§Ù‡ Ø­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**

```python
def analyze_bollinger_bands_advanced(self, df: pd.DataFrame, upper, middle, lower) -> List[Dict]:
    """
    ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Bollinger Bands
    """
    signals = []

    if len(df) < 5:
        return signals

    current_close = df['close'].iloc[-1]
    prev_close = df['close'].iloc[-2]

    current_upper = upper[-1]
    current_middle = middle[-1]
    current_lower = lower[-1]
    prev_upper = upper[-2]
    prev_lower = lower[-2]

    # 1. BB Bounce (Ø¨Ø±Ú¯Ø´Øª Ø§Ø² Ø¨Ø§Ù†Ø¯)
    # Ù‚ÛŒÙ…Øª Ø¨Ù‡ Ø¨Ø§Ù†Ø¯ Ø±Ø³ÛŒØ¯ Ùˆ Ø¨Ø±Ú¯Ø´Øª
    if prev_close <= prev_lower and current_close > current_lower:
        # Bounce Ø§Ø² Ø¨Ø§Ù†Ø¯ Ù¾Ø§ÛŒÛŒÙ† (ØµØ¹ÙˆØ¯ÛŒ)
        bounce_strength = (current_close - current_lower) / (current_middle - current_lower)
        signals.append({
            'type': 'bollinger_lower_bounce',
            'direction': 'bullish',
            'score': self.pattern_scores.get('bollinger_lower_bounce', 2.3) * bounce_strength
        })

    elif prev_close >= prev_upper and current_close < current_upper:
        # Bounce Ø§Ø² Ø¨Ø§Ù†Ø¯ Ø¨Ø§Ù„Ø§ (Ù†Ø²ÙˆÙ„ÛŒ)
        bounce_strength = (current_upper - current_close) / (current_upper - current_middle)
        signals.append({
            'type': 'bollinger_upper_bounce',
            'direction': 'bearish',
            'score': self.pattern_scores.get('bollinger_upper_bounce', 2.3) * bounce_strength
        })

    # 2. BB Walk (Ø­Ø±Ú©Øª Ù…Ø¯Ø§ÙˆÙ… Ø±ÙˆÛŒ Ø¨Ø§Ù†Ø¯)
    # Ù‚ÛŒÙ…Øª Ø¨Ø±Ø§ÛŒ Ú†Ù†Ø¯ Ú©Ù†Ø¯Ù„ Ù…ØªÙˆØ§Ù„ÛŒ Ø±ÙˆÛŒ ÛŒØ§ Ù†Ø²Ø¯ÛŒÚ© Ø¨Ø§Ù†Ø¯ Ù…ÛŒâ€ŒÙ…Ø§Ù†Ø¯
    recent_closes = df['close'].iloc[-5:]
    recent_uppers = upper[-5:]
    recent_lowers = lower[-5:]

    # Upper Walk
    upper_touches = sum(1 for i, close in enumerate(recent_closes)
                       if close >= recent_uppers[i] * 0.98)
    if upper_touches >= 3:
        signals.append({
            'type': 'bollinger_upper_walk',
            'direction': 'bullish',  # Ø§Ø¯Ø§Ù…Ù‡ Ø±ÙˆÙ†Ø¯ ØµØ¹ÙˆØ¯ÛŒ Ù‚ÙˆÛŒ
            'score': self.pattern_scores.get('bollinger_upper_walk', 2.7)
        })

    # Lower Walk
    lower_touches = sum(1 for i, close in enumerate(recent_closes)
                       if close <= recent_lowers[i] * 1.02)
    if lower_touches >= 3:
        signals.append({
            'type': 'bollinger_lower_walk',
            'direction': 'bearish',  # Ø§Ø¯Ø§Ù…Ù‡ Ø±ÙˆÙ†Ø¯ Ù†Ø²ÙˆÙ„ÛŒ Ù‚ÙˆÛŒ
            'score': self.pattern_scores.get('bollinger_lower_walk', 2.7)
        })

    # 3. BB Expansion (Ø§Ù†Ø¨Ø³Ø§Ø· Ø¨Ø§Ù†Ø¯Ù‡Ø§)
    if len(df) >= 20:
        recent_widths = [(upper[i] - lower[i]) / middle[i]
                        for i in range(-20, 0) if middle[i] > 0]
        avg_width = np.mean(recent_widths)
        current_width = (current_upper - current_lower) / current_middle

        # Ø¨Ø§Ù†Ø¯Ù‡Ø§ Ø¯Ø± Ø­Ø§Ù„ Ø§Ù†Ø¨Ø³Ø§Ø· (Ù†ÙˆØ³Ø§Ù†Ø§Øª Ø§ÙØ²Ø§ÛŒØ´ ÛŒØ§ÙØªÙ‡)
        if current_width > avg_width * 1.3:
            signals.append({
                'type': 'bollinger_expansion',
                'direction': 'neutral',
                'score': self.pattern_scores.get('bollinger_expansion', 1.8)
            })

        # Ø¨Ø§Ù†Ø¯Ù‡Ø§ Ø¯Ø± Ø­Ø§Ù„ Ø§Ù†Ù‚Ø¨Ø§Ø¶ (Ù†ÙˆØ³Ø§Ù†Ø§Øª Ú©Ø§Ù‡Ø´ ÛŒØ§ÙØªÙ‡)
        elif current_width < avg_width * 0.7:
            signals.append({
                'type': 'bollinger_contraction',
                'direction': 'neutral',
                'score': self.pattern_scores.get('bollinger_contraction', 1.5)
            })

    # 4. Middle Band Cross
    # Ø¹Ø¨ÙˆØ± Ø§Ø² Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú© (middle band)
    if prev_close < middle[-2] and current_close > current_middle:
        signals.append({
            'type': 'bollinger_middle_cross_up',
            'direction': 'bullish',
            'score': self.pattern_scores.get('bollinger_middle_cross_up', 2.0)
        })
    elif prev_close > middle[-2] and current_close < current_middle:
        signals.append({
            'type': 'bollinger_middle_cross_down',
            'direction': 'bearish',
            'score': self.pattern_scores.get('bollinger_middle_cross_down', 2.0)
        })

    return signals
```

**Ù…Ø²Ø§ÛŒØ§:**
- ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„â€ŒØªØ± Bollinger Bands
- Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ BB Walk Ú©Ù‡ Ù†Ø´Ø§Ù†Ù‡ Ø±ÙˆÙ†Ø¯ Ù‚ÙˆÛŒ Ø§Ø³Øª
- BB Bounce Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø¨Ø±Ú¯Ø´ØªÛŒ Ù‚ÙˆÛŒ Ø§Ø³Øª

---

#### âŒ Ù…Ø´Ú©Ù„ 5: Ø¹Ø¯Ù… ØªØ±Ú©ÛŒØ¨ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ú†Ù†Ø¯Ú¯Ø§Ù†Ù‡ (Confluence)

**Ù…Ø´Ú©Ù„ ÙØ¹Ù„ÛŒ:**
- Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù…Ø³ØªÙ‚Ù„ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
- Ø§Ú¯Ø± Ú†Ù†Ø¯ Ø§Ù„Ú¯Ùˆ Ù‡Ù…Ø²Ù…Ø§Ù† Ø±Ø® Ø¯Ù‡Ù†Ø¯ØŒ Ø§Ø±Ø²Ø´ ØªØ±Ú©ÛŒØ¨ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯
- Ù…Ø«Ø§Ù„: Hammer + Bollinger Bounce + Support = Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø¨Ø³ÛŒØ§Ø± Ù‚ÙˆÛŒ

**Ø±Ø§Ù‡ Ø­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**

```python
def calculate_confluence_bonus(self, signals: List[Dict], context: Dict) -> float:
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ bonus Ø¨Ø±Ø§ÛŒ ØªØ±Ú©ÛŒØ¨ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù‡Ù…Ø³Ùˆ (confluence)
    """

    bullish_signals = [s for s in signals if s.get('direction') == 'bullish']
    bearish_signals = [s for s in signals if s.get('direction') == 'bearish']

    # ØªÙ‚Ø³ÛŒÙ…â€ŒØ¨Ù†Ø¯ÛŒ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¯Ø³ØªÙ‡â€ŒÙ‡Ø§
    reversal_patterns = ['hammer', 'morning_star', 'evening_star', 'shooting_star',
                        'head_and_shoulders', 'double_top', 'double_bottom']
    continuation_patterns = ['bull_flag', 'bear_flag', 'triangle']
    bollinger_signals = ['bollinger_upper_break', 'bollinger_lower_break',
                        'bollinger_upper_bounce', 'bollinger_lower_bounce']

    confluence_score = 0.0

    # Ø¨Ø±Ø±Ø³ÛŒ confluence Ø¨Ø±Ø§ÛŒ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØµØ¹ÙˆØ¯ÛŒ
    if len(bullish_signals) >= 2:
        has_reversal = any(s['type'] in reversal_patterns for s in bullish_signals)
        has_continuation = any(s['type'] in continuation_patterns for s in bullish_signals)
        has_bollinger = any(s['type'] in bollinger_signals for s in bullish_signals)
        has_volume = any('volume' in s['type'] for s in bullish_signals)

        confluence_count = sum([has_reversal, has_continuation, has_bollinger, has_volume])

        # Ø¨Ø±Ø±Ø³ÛŒ context
        is_at_support = context.get('is_near_support', False)
        is_trend_aligned = context.get('trend_aligned', False)

        if is_at_support:
            confluence_count += 1
        if is_trend_aligned:
            confluence_count += 1

        # Confluence bonus Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯
        if confluence_count >= 4:
            confluence_score = 0.5  # +50% bonus
        elif confluence_count == 3:
            confluence_score = 0.3  # +30% bonus
        elif confluence_count == 2:
            confluence_score = 0.15  # +15% bonus

    # Ù‡Ù…ÛŒÙ† Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø¨Ø±Ø§ÛŒ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù†Ø²ÙˆÙ„ÛŒ
    elif len(bearish_signals) >= 2:
        has_reversal = any(s['type'] in reversal_patterns for s in bearish_signals)
        has_continuation = any(s['type'] in continuation_patterns for s in bearish_signals)
        has_bollinger = any(s['type'] in bollinger_signals for s in bearish_signals)
        has_volume = any('volume' in s['type'] for s in bearish_signals)

        confluence_count = sum([has_reversal, has_continuation, has_bollinger, has_volume])

        is_at_resistance = context.get('is_near_resistance', False)
        is_trend_aligned = context.get('trend_aligned', False)

        if is_at_resistance:
            confluence_count += 1
        if is_trend_aligned:
            confluence_count += 1

        if confluence_count >= 4:
            confluence_score = 0.5
        elif confluence_count == 3:
            confluence_score = 0.3
        elif confluence_count == 2:
            confluence_score = 0.15

    return confluence_score
```

**Ø§Ø³ØªÙØ§Ø¯Ù‡:**

```python
# Ø¯Ø± Ø§Ù†ØªÙ‡Ø§ÛŒ analyze_price_action
confluence_bonus = self.calculate_confluence_bonus(price_action_signals, context)

# Ø§Ø¹Ù…Ø§Ù„ bonus Ø¨Ù‡ Ú©Ù„ score
if confluence_bonus > 0:
    if bullish_score > bearish_score:
        bullish_score *= (1.0 + confluence_bonus)
    else:
        bearish_score *= (1.0 + confluence_bonus)
```

**Ù…Ø«Ø§Ù„:**
```
Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØµØ¹ÙˆØ¯ÛŒ:
1. Hammer (2.0)
2. Bollinger Lower Bounce (2.3)
3. High Volume Bullish (2.8)
4. Ø¯Ø± Ù†Ø²Ø¯ÛŒÚ©ÛŒ Support
5. Ù‡Ù…Ø³Ùˆ Ø¨Ø§ Ø±ÙˆÙ†Ø¯

Confluence Count = 5
Confluence Bonus = +50%
Total Bullish Score = (2.0 + 2.3 + 2.8) Ã— 1.5 = 10.65
```

**Ù…Ø²Ø§ÛŒØ§:**
- ØªÙ‚ÙˆÛŒØª Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ø§Ø¬Ù…Ø§Ø¹ Ø¨Ø§Ù„Ø§ (high confluence)
- ØªØ´ÙˆÛŒÙ‚ Ø¨Ù‡ ÙˆØ±ÙˆØ¯ Ø¯Ø± Ù…ÙˆÙ‚Ø¹ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø¨Ø§ ØªØ£ÛŒÛŒØ¯ Ú†Ù†Ø¯Ú¯Ø§Ù†Ù‡
- Ú©Ø§Ù‡Ø´ Ø±ÛŒØ³Ú©

---

### ğŸ“‹ Ø®Ù„Ø§ØµÙ‡ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª

| # | Ù…Ø´Ú©Ù„ | Ø±Ø§Ù‡ Ø­Ù„ | Ø§ÙˆÙ„ÙˆÛŒØª | ØªØ£Ø«ÛŒØ± Ø¨Ø± Ø¯Ù‚Øª |
|---|------|--------|---------|--------------|
| 1 | Ø¹Ø¯Ù… Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Context | Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø­Ù„ Ø§Ù„Ú¯Ùˆ (S/RØŒ Ø±ÙˆÙ†Ø¯) | ğŸ”´ Ø¨Ø§Ù„Ø§ | +20% |
| 2 | Ø¹Ø¯Ù… Pattern Quality Ø¨Ø±Ø§ÛŒ candles | Ù…Ø­Ø§Ø³Ø¨Ù‡ quality Ø¨Ø± Ø§Ø³Ø§Ø³ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ù†Ø¯Ù„ | ğŸŸ¡ Ù…ØªÙˆØ³Ø· | +12% |
| 3 | ÙÙ‚Ø¯Ø§Ù† Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…Ù‡Ù… | Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Double Top/BottomØŒ Cup & Handle | ğŸŸ¡ Ù…ØªÙˆØ³Ø· | +15% |
| 4 | BB Ù…Ø­Ø¯ÙˆØ¯ | Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† BB BounceØŒ WalkØŒ Expansion | ğŸŸ¢ Ù¾Ø§ÛŒÛŒÙ† | +8% |
| 5 | Ø¹Ø¯Ù… Confluence | Ù…Ø­Ø§Ø³Ø¨Ù‡ bonus Ø¨Ø±Ø§ÛŒ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù‡Ù…Ø³Ùˆ | ğŸ”´ Ø¨Ø§Ù„Ø§ | +18% |

**ØªØ£Ø«ÛŒØ± Ú©Ù„ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª:** Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‚Øª Ø­Ø¯ÙˆØ¯ **+50-60%** Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Price Action

---

### ğŸ”¬ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª ØªØ³Øª Ùˆ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ

1. **Context Impact Analysis:**
   - Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø§Ù„Ú¯ÙˆÙ‡Ø§ Ø¨Ø§/Ø¨Ø¯ÙˆÙ† context evaluation
   - ØªØ­Ù„ÛŒÙ„ win rate Ø¯Ø± Ù…Ø­Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù (S/RØŒ mid-range)
   - Ø§Ù†Ø¯Ø§Ø²Ù‡â€ŒÚ¯ÛŒØ±ÛŒ ØªØ£Ø«ÛŒØ± trend alignment

2. **Pattern Quality Validation:**
   - Ø¨Ø±Ø±Ø³ÛŒ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø¨ÛŒÙ† quality score Ùˆ Ù†ØªÛŒØ¬Ù‡ Ù…Ø¹Ø§Ù…Ù„Ù‡
   - Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø¢Ø³ØªØ§Ù†Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø¶Ø¹ÛŒÙ
   - Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ high quality vs low quality

3. **New Patterns Backtesting:**
   - ØªØ³Øª Ø¹Ù…Ù„Ú©Ø±Ø¯ Double Top/Bottom Ùˆ Cup & Handle
   - Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯
   - ØªØ¹ÛŒÛŒÙ† Ø§Ù…ØªÛŒØ§Ø² Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø§Ù„Ú¯Ùˆ

4. **Confluence Analysis:**
   - ØªØ­Ù„ÛŒÙ„ win rate Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯ confluences
   - ÛŒØ§ÙØªÙ† Ø¨Ù‡ØªØ±ÛŒÙ† ØªØ±Ú©ÛŒØ¨â€ŒÙ‡Ø§ÛŒ Ø³ÛŒÚ¯Ù†Ø§Ù„
   - Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ bonus percentages

---

## Ù…Ø±Ø­Ù„Ù‡ 6: Ø¨Ù‡Ø¨ÙˆØ¯ Support/Resistance Detection

### Ù…Ø´Ú©Ù„ 1: Ø¹Ø¯Ù… Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ø¨Ù‡ Proximity Ø¨Ø§ Ø³Ø·ÙˆØ­

**Ø´Ø¯Øª Ù…Ø´Ú©Ù„:** ğŸ”´ Ø¨Ø§Ù„Ø§
**ØªØ£Ø«ÛŒØ± Ø¨Ø± Ø¯Ù‚Øª:** +25% Ø¨Ù‡Ø¨ÙˆØ¯

**ØªÙˆØ¶ÛŒØ­ Ù…Ø´Ú©Ù„:**

Ø¯Ø± Ú©Ø¯ ÙØ¹Ù„ÛŒ (`signal_generator.py:5284-5297`), **ÙÙ‚Ø· Ø´Ú©Ø³Øª Ø³Ø·ÙˆØ­** (broken S/R) Ø§Ù…ØªÛŒØ§Ø² Ù…ÛŒâ€ŒØ¯Ù‡Ù†Ø¯. Ø§Ù…Ø§ ÛŒÚ©ÛŒ Ø§Ø² Ù…Ù‡Ù…â€ŒØªØ±ÛŒÙ† Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡:
- Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø®Ø±ÛŒØ¯ **Ù†Ø²Ø¯ÛŒÚ© Ø­Ù…Ø§ÛŒØª Ù‚ÙˆÛŒ** â†’ Ø§Ø­ØªÙ…Ø§Ù„ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ù„Ø§
- Ø³ÛŒÚ¯Ù†Ø§Ù„ ÙØ±ÙˆØ´ **Ù†Ø²Ø¯ÛŒÚ© Ù…Ù‚Ø§ÙˆÙ…Øª Ù‚ÙˆÛŒ** â†’ Ø§Ø­ØªÙ…Ø§Ù„ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ù„Ø§

**Ù…Ø«Ø§Ù„ Ø§Ø² Ø¯Ø³Øª Ø±ÙØªÙ‡:**
```python
# Ø³Ø·ÙˆØ­ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡
nearest_support = {'price': 49800, 'strength': 0.90}
current_price = 49850  # ÙØ§ØµÙ„Ù‡: 50 (0.1%)

# Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø®Ø±ÛŒØ¯ Ø¨Ø§ RSI oversold + MACD cross
# Ø§Ù…Ø§ Ú©Ø¯ ÙØ¹Ù„ÛŒ Ù‡ÛŒÚ† Ø§Ù…ØªÛŒØ§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ø²Ø¯ÛŒÚ©ÛŒ Ø¨Ù‡ support Ù†Ù…ÛŒâ€ŒØ¯Ù‡Ø¯! âŒ
```

Ø§ÛŒÙ† Ø¯Ø± Ø­Ø§Ù„ÛŒ Ø§Ø³Øª Ú©Ù‡ **Ù‚ÛŒÙ…Øª Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ø±ÙˆÛŒ Ø­Ù…Ø§ÛŒØª Ù‚ÙˆÛŒ** Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯ Ùˆ Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø®Ø±ÛŒØ¯ Ø¨Ø³ÛŒØ§Ø± Ø¨Ø§ Ø§Ø±Ø²Ø´ Ø§Ø³Øª.

---

**Ø±Ø§Ù‡ Ø­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**

Ø§ÙØ²ÙˆØ¯Ù† ØªØ§Ø¨Ø¹ Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ **Proximity Score**:

```python
def calculate_proximity_score(
    self,
    current_price: float,
    nearest_support: Optional[Dict],
    nearest_resistance: Optional[Dict],
    signal_direction: str,  # 'bullish' or 'bearish'
    atr: float
) -> Dict[str, Any]:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†Ø²Ø¯ÛŒÚ©ÛŒ Ø¨Ù‡ Ø³Ø·ÙˆØ­ S/R"""

    proximity_score = 0.0
    proximity_type = None
    distance_pct = None

    if signal_direction == 'bullish' and nearest_support:
        # Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø®Ø±ÛŒØ¯ Ù†Ø²Ø¯ÛŒÚ© Ø­Ù…Ø§ÛŒØª
        support_price = nearest_support['price']
        support_strength = nearest_support['strength']

        distance = abs(current_price - support_price)
        distance_pct = (distance / current_price) * 100

        # Ù‡Ø± Ú†Ù‡ Ù†Ø²Ø¯ÛŒÚ©ØªØ± Ø¨Ø§Ø´ÛŒÙ… â†’ Ø§Ù…ØªÛŒØ§Ø² Ø¨ÛŒØ´ØªØ±
        if distance < atr * 0.5:  # Ø®ÛŒÙ„ÛŒ Ù†Ø²Ø¯ÛŒÚ© (< 0.5 ATR)
            proximity_multiplier = 1.0 - (distance / (atr * 0.5))  # 0.0 to 1.0
            proximity_score = 2.5 * proximity_multiplier * support_strength
            proximity_type = 'at_support'

            # Ù…Ø«Ø§Ù„: distance = 0.2*ATR, strength = 0.9
            # multiplier = 1.0 - 0.4 = 0.6
            # score = 2.5 * 0.6 * 0.9 = +1.35

        elif distance < atr * 1.0:  # Ù†Ø²Ø¯ÛŒÚ© (0.5-1.0 ATR)
            proximity_multiplier = 1.0 - (distance / atr)
            proximity_score = 1.5 * proximity_multiplier * support_strength
            proximity_type = 'near_support'

    elif signal_direction == 'bearish' and nearest_resistance:
        # Ø³ÛŒÚ¯Ù†Ø§Ù„ ÙØ±ÙˆØ´ Ù†Ø²Ø¯ÛŒÚ© Ù…Ù‚Ø§ÙˆÙ…Øª
        resistance_price = nearest_resistance['price']
        resistance_strength = nearest_resistance['strength']

        distance = abs(current_price - resistance_price)
        distance_pct = (distance / current_price) * 100

        if distance < atr * 0.5:  # Ø®ÛŒÙ„ÛŒ Ù†Ø²Ø¯ÛŒÚ©
            proximity_multiplier = 1.0 - (distance / (atr * 0.5))
            proximity_score = 2.5 * proximity_multiplier * resistance_strength
            proximity_type = 'at_resistance'

        elif distance < atr * 1.0:  # Ù†Ø²Ø¯ÛŒÚ©
            proximity_multiplier = 1.0 - (distance / atr)
            proximity_score = 1.5 * proximity_multiplier * resistance_strength
            proximity_type = 'near_resistance'

    return {
        'score': proximity_score,
        'type': proximity_type,
        'distance_pct': distance_pct,
        'distance_atr': distance / atr if atr > 0 else None
    }
```

**Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± `calculate_multi_timeframe_score`:**

```python
# Ø¨Ø¹Ø¯ Ø§Ø² Ù…Ø­Ø§Ø³Ø¨Ù‡ broken S/R (Ø®Ø· 5297)
sr_data = tf_data.get('support_resistance', {})
if sr_data.get('status') == 'ok':
    details = sr_data.get('details', {})
    nearest_support = details.get('nearest_support')
    nearest_resistance = details.get('nearest_resistance')
    atr = details.get('atr', 0)

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ proximity score Ø¨Ø±Ø§ÛŒ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØµØ¹ÙˆØ¯ÛŒ
    if bullish_score > 0:  # ÙÙ‚Ø· Ø§Ú¯Ø± Ø³ÛŒÚ¯Ù†Ø§Ù„ ØµØ¹ÙˆØ¯ÛŒ Ø¯Ø§Ø±ÛŒÙ…
        prox = self.calculate_proximity_score(
            current_price, nearest_support, nearest_resistance,
            'bullish', atr
        )
        if prox['score'] > 0:
            score = prox['score'] * tf_weight
            bullish_score += score
            all_signals.append({
                'type': prox['type'],
                'timeframe': tf,
                'score': score,
                'direction': 'bullish',
                'distance_pct': prox['distance_pct']
            })

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ proximity score Ø¨Ø±Ø§ÛŒ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù†Ø²ÙˆÙ„ÛŒ
    if bearish_score > 0:
        prox = self.calculate_proximity_score(
            current_price, nearest_support, nearest_resistance,
            'bearish', atr
        )
        if prox['score'] > 0:
            score = prox['score'] * tf_weight
            bearish_score += score
            all_signals.append({
                'type': prox['type'],
                'timeframe': tf,
                'score': score,
                'direction': 'bearish',
                'distance_pct': prox['distance_pct']
            })
```

**Ø¬Ø¯ÙˆÙ„ Ø§Ù…ØªÛŒØ§Ø²Ø§Øª Ø¬Ø¯ÛŒØ¯:**

| Ù…ÙˆÙ‚Ø¹ÛŒØª | ÙØ§ØµÙ„Ù‡ (ATR) | Ø§Ù…ØªÛŒØ§Ø² Ù¾Ø§ÛŒÙ‡ | Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ù†Ù‡Ø§ÛŒÛŒ | Ù…Ø«Ø§Ù„ |
|--------|------------|------------|--------------|------|
| At Support/Resistance | < 0.5 ATR | **2.5** | **0 ØªØ§ 2.5** | Ù‚ÛŒÙ…Øª Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ø±ÙˆÛŒ Ø³Ø·Ø­ |
| Near Support/Resistance | 0.5-1.0 ATR | **1.5** | **0 ØªØ§ 1.5** | Ù‚ÛŒÙ…Øª Ù†Ø²Ø¯ÛŒÚ© Ø³Ø·Ø­ |

**Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ù‡Ø¨ÙˆØ¯:** +25% Ø¯Ø± accuracy Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§

---

### Ù…Ø´Ú©Ù„ 2: Ø¹Ø¯Ù… Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ù‚Ø¯Ø±Øª Breakout

**Ø´Ø¯Øª Ù…Ø´Ú©Ù„:** ğŸŸ¡ Ù…ØªÙˆØ³Ø·
**ØªØ£Ø«ÛŒØ± Ø¨Ø± Ø¯Ù‚Øª:** +18% Ø¨Ù‡Ø¨ÙˆØ¯

**ØªÙˆØ¶ÛŒØ­ Ù…Ø´Ú©Ù„:**

Ø¯Ø± Ú©Ø¯ ÙØ¹Ù„ÛŒ (`signal_generator.py:2384-2387`), ØªØ´Ø®ÛŒØµ breakout ÙÙ‚Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ **Ù‚ÛŒÙ…Øª** Ø§Ø³Øª:

```python
broken_resistance = next((level for level in resistance_levels if
    current_close > level['price'] and prev_low < level['price']
), None)
```

Ø§Ù…Ø§ breakout Ù‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ **ØªØ£ÛŒÛŒØ¯** Ø¯Ø§Ø±Ù†Ø¯:
- âŒ Fake breakout: Ù‚ÛŒÙ…Øª Ù…ÛŒâ€ŒØ´Ú©Ù†Ø¯ Ø§Ù…Ø§ Ø³Ø±ÛŒØ¹ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø¯ (ÙØ±ÛŒØ¨)
- âœ… Strong breakout: Ù‚ÛŒÙ…Øª Ù…ÛŒâ€ŒØ´Ú©Ù†Ø¯ + Ø­Ø¬Ù… Ø¨Ø§Ù„Ø§ + Ø§Ø¯Ø§Ù…Ù‡ Ø­Ø±Ú©Øª

**Ù…Ø«Ø§Ù„ False Breakout:**
```python
# Ú©Ù†Ø¯Ù„ 1: close = 50,220 (> resistance 50,200) â†’ breakout ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ âœ“
# Ú©Ù†Ø¯Ù„ 2: close = 50,050 (< resistance) â†’ Ø¨Ø±Ú¯Ø´Øª! Ø§ÛŒÙ† fake Ø¨ÙˆØ¯ âœ—

# Ø³ÛŒØ³ØªÙ… ÙØ¹Ù„ÛŒ Ø§Ù…ØªÛŒØ§Ø² Ø¯Ø§Ø¯ Ø§Ù…Ø§ Ù…Ø¹Ø§Ù…Ù„Ù‡ Ø¶Ø±Ø± Ú©Ø±Ø¯
```

---

**Ø±Ø§Ù‡ Ø­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**

Ø§ÙØ²ÙˆØ¯Ù† **Breakout Validation** Ø¨Ø§ Ú†Ù†Ø¯ Ù…Ø¹ÛŒØ§Ø±:

```python
def validate_breakout_strength(
    self,
    df: pd.DataFrame,
    broken_level: Dict[str, Any],
    breakout_type: str  # 'resistance' or 'support'
) -> Dict[str, Any]:
    """Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ù‚Ø¯Ø±Øª Ø´Ú©Ø³Øª Ø³Ø·Ø­"""

    current_close = df['close'].iloc[-1]
    level_price = broken_level['price']
    level_strength = broken_level['strength']

    # 1. Body Strength: Ø¢ÛŒØ§ Ø¨Ø¯Ù†Ù‡ Ú©Ù†Ø¯Ù„ Ø§Ø² Ø³Ø·Ø­ Ø¹Ø¨ÙˆØ± Ú©Ø±Ø¯Ù‡ØŸ
    current_open = df['open'].iloc[-1]
    if breakout_type == 'resistance':
        body_above = min(current_open, current_close) > level_price
        body_strength = 1.0 if body_above else 0.5  # ÙÙ‚Ø· Ø³Ø§ÛŒÙ‡ Ø¹Ø¨ÙˆØ± Ú©Ø±Ø¯Ù‡
    else:  # support
        body_below = max(current_open, current_close) < level_price
        body_strength = 1.0 if body_below else 0.5

    # 2. Penetration Depth: Ú†Ù‚Ø¯Ø± Ø§Ø² Ø³Ø·Ø­ ÙØ§ØµÙ„Ù‡ Ú¯Ø±ÙØªÙ‡ØŸ
    penetration = abs(current_close - level_price) / level_price

    atr_values = talib.ATR(df['high'].values, df['low'].values,
                           df['close'].values, timeperiod=14)
    current_atr = atr_values[-1]

    penetration_atr = abs(current_close - level_price) / current_atr

    # Ù‚ÙˆÛŒ: > 0.5 ATR, Ù…ØªÙˆØ³Ø·: 0.2-0.5 ATR, Ø¶Ø¹ÛŒÙ: < 0.2 ATR
    if penetration_atr > 0.5:
        penetration_strength = 1.0
    elif penetration_atr > 0.2:
        penetration_strength = 0.6
    else:
        penetration_strength = 0.3  # Ø¶Ø¹ÛŒÙ

    # 3. Volume Confirmation: Ø¢ÛŒØ§ Ø¨Ø§ Ø­Ø¬Ù… Ø¨Ø§Ù„Ø§ Ù‡Ù…Ø±Ø§Ù‡ Ø¨ÙˆØ¯Ù‡ØŸ
    if 'volume' in df.columns:
        current_volume = df['volume'].iloc[-1]
        avg_volume = df['volume'].iloc[-20:-1].mean()
        volume_ratio = current_volume / avg_volume if avg_volume > 0 else 1.0

        # Ø­Ø¬Ù… Ø¨Ø§Ù„Ø§ â†’ ØªØ£ÛŒÛŒØ¯ Ø¨ÛŒØ´ØªØ±
        if volume_ratio > 1.5:
            volume_strength = 1.0
        elif volume_ratio > 1.0:
            volume_strength = 0.7
        else:
            volume_strength = 0.4  # Ø­Ø¬Ù… Ù¾Ø§ÛŒÛŒÙ† = Ù…Ø´Ú©ÙˆÚ©
    else:
        volume_strength = 0.7  # Ù¾ÛŒØ´â€ŒÙØ±Ø¶

    # 4. Follow-Through: Ø¢ÛŒØ§ Ú†Ù†Ø¯ Ú©Ù†Ø¯Ù„ Ù‚Ø¨Ù„ Ù‡Ù… momentum Ø¯Ø§Ø´ØªÙ‡ØŸ
    momentum_strength = 1.0
    if len(df) >= 3:
        prev_closes = df['close'].iloc[-4:-1].values
        if breakout_type == 'resistance':
            # Ø¢ÛŒØ§ Ù‚ÛŒÙ…Øª Ø¯Ø± Ø­Ø§Ù„ ØµØ¹ÙˆØ¯ Ø¨ÙˆØ¯Ù‡ØŸ
            if all(prev_closes[i] < prev_closes[i+1] for i in range(len(prev_closes)-1)):
                momentum_strength = 1.2  # Bonus!
            elif prev_closes[-1] < prev_closes[-2]:
                momentum_strength = 0.8  # Ø¶Ø¹ÛŒÙâ€ŒØªØ±
        else:
            # Ø¢ÛŒØ§ Ù‚ÛŒÙ…Øª Ø¯Ø± Ø­Ø§Ù„ Ù†Ø²ÙˆÙ„ Ø¨ÙˆØ¯Ù‡ØŸ
            if all(prev_closes[i] > prev_closes[i+1] for i in range(len(prev_closes)-1)):
                momentum_strength = 1.2
            elif prev_closes[-1] > prev_closes[-2]:
                momentum_strength = 0.8

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ù†Ù‡Ø§ÛŒÛŒ
    overall_strength = (
        body_strength * 0.25 +
        penetration_strength * 0.30 +
        volume_strength * 0.25 +
        momentum_strength * 0.20
    )

    # Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
    if overall_strength >= 0.8:
        quality = 'strong'
        score_multiplier = 1.5  # +50% bonus
    elif overall_strength >= 0.6:
        quality = 'moderate'
        score_multiplier = 1.0
    else:
        quality = 'weak'
        score_multiplier = 0.5  # -50% penalty

    return {
        'quality': quality,
        'overall_strength': overall_strength,
        'score_multiplier': score_multiplier,
        'components': {
            'body_strength': body_strength,
            'penetration_strength': penetration_strength,
            'penetration_atr': penetration_atr,
            'volume_strength': volume_strength,
            'volume_ratio': volume_ratio if 'volume' in df.columns else None,
            'momentum_strength': momentum_strength
        }
    }
```

**Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ:**

```python
# Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ú©Ø¯ ÙØ¹Ù„ÛŒ (Ø®Ø·ÙˆØ· 5284-5297)
if sr_data.get('broken_resistance'):
    resistance_level = sr_data['broken_resistance']

    # Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ù‚Ø¯Ø±Øª breakout
    validation = self.validate_breakout_strength(
        df, resistance_level, 'resistance'
    )

    level_str = resistance_level.get('strength', 1.0)
    base_score = self.pattern_scores.get('broken_resistance', 3.0)

    # Ø§Ø¹Ù…Ø§Ù„ multiplier Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©ÛŒÙÛŒØª
    score = (base_score * tf_weight * level_str *
             validation['score_multiplier'])

    bullish_score += score
    all_signals.append({
        'type': 'broken_resistance',
        'timeframe': tf,
        'score': score,
        'direction': 'bullish',
        'breakout_quality': validation['quality'],
        'breakout_strength': validation['overall_strength']
    })
```

**Ø¬Ø¯ÙˆÙ„ Ø§Ù…ØªÛŒØ§Ø²Ø§Øª Ø¨Ù‡â€ŒØ±ÙˆØ² Ø´Ø¯Ù‡:**

| Ú©ÛŒÙÛŒØª Breakout | Overall Strength | Multiplier | Ø§Ù…ØªÛŒØ§Ø² Ù†Ù‡Ø§ÛŒÛŒ (base=3.0, strength=0.9) |
|----------------|-----------------|-----------|--------------------------------------|
| Strong | â‰¥ 0.8 | **1.5x** | 3.0 Ã— 0.9 Ã— 1.5 = **4.05** |
| Moderate | 0.6-0.8 | **1.0x** | 3.0 Ã— 0.9 Ã— 1.0 = **2.70** |
| Weak | < 0.6 | **0.5x** | 3.0 Ã— 0.9 Ã— 0.5 = **1.35** |

**Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ù‡Ø¨ÙˆØ¯:** +18% Ø¨Ø§ ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† fake breakouts

---

### Ù…Ø´Ú©Ù„ 3: Ø¹Ø¯Ù… Ø±Ø¯ÛŒØ§Ø¨ÛŒ ØªØ¹Ø¯Ø§Ø¯ Test Ù‡Ø§ÛŒ Ø³Ø·Ø­

**Ø´Ø¯Øª Ù…Ø´Ú©Ù„:** ğŸŸ¡ Ù…ØªÙˆØ³Ø·
**ØªØ£Ø«ÛŒØ± Ø¨Ø± Ø¯Ù‚Øª:** +12% Ø¨Ù‡Ø¨ÙˆØ¯

**ØªÙˆØ¶ÛŒØ­ Ù…Ø´Ú©Ù„:**

Ø¯Ø± Ú©Ø¯ ÙØ¹Ù„ÛŒ (`signal_generator.py:2333-2370`), Ù‚Ø¯Ø±Øª Ø³Ø·Ø­ ÙÙ‚Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ **ØªØ¹Ø¯Ø§Ø¯ peaks Ø¯Ø± cluster** Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯:

```python
cluster_strength = min(1.0, len(cluster) / 3)
```

Ø§Ù…Ø§ Ø§ÛŒÙ† Ø¯Ù‚ÛŒÙ‚ Ù†ÛŒØ³Øª Ú†ÙˆÙ†:
- **ØªØ¹Ø¯Ø§Ø¯ peaks â‰  ØªØ¹Ø¯Ø§Ø¯ test Ù‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ø³Ø·Ø­**
- ÛŒÚ© Ø³Ø·Ø­ Ù…Ù…Ú©Ù† Ø§Ø³Øª 10 Ø¨Ø§Ø± ØªØ³Øª Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ Ø§Ù…Ø§ ÙÙ‚Ø· 2 peak Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯
- Ø³Ø·ÙˆØ­ÛŒ Ú©Ù‡ Ø¨ÛŒØ´ØªØ± ØªØ³Øª Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯ â†’ **Ù‚ÙˆÛŒâ€ŒØªØ±** Ù‡Ø³ØªÙ†Ø¯

**Ù…Ø«Ø§Ù„:**
```python
# Ø³Ø·Ø­ 50,000:
# - Ø¨Ø§Ø± 1: Ù‚ÛŒÙ…Øª Ø§Ø² 50,100 Ø¨Ù‡ 49,900 Ø¨Ø±Ú¯Ø´Øª (peak Ø«Ø¨Øª Ù†Ø´Ø¯)
# - Ø¨Ø§Ø± 2: Ù‚ÛŒÙ…Øª Ø§Ø² 50,050 Ø¨Ù‡ 49,920 Ø¨Ø±Ú¯Ø´Øª (peak Ø«Ø¨Øª Ù†Ø´Ø¯)
# - Ø¨Ø§Ø± 3: Ù‚ÛŒÙ…Øª Ø§Ø² 50,200 Ø¨Ù‡ 49,850 Ø¨Ø±Ú¯Ø´Øª (peak Ø«Ø¨Øª Ø´Ø¯) âœ“

# Ø³Ø·Ø­ 3 Ø¨Ø§Ø± ØªØ³Øª Ø´Ø¯Ù‡ Ø§Ù…Ø§ ÙÙ‚Ø· 1 peak â†’ strength = 0.33 âŒ
# Ø¨Ø§ÛŒØ¯ strength = 1.0 Ø¨Ø§Ø´Ø¯ âœ“
```

---

**Ø±Ø§Ù‡ Ø­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**

Ø§ÙØ²ÙˆØ¯Ù† ØªØ§Ø¨Ø¹ **Ø´Ù…Ø§Ø±Ø´ ÙˆØ§Ù‚Ø¹ÛŒ test Ù‡Ø§**:

```python
def count_level_touches(
    self,
    df: pd.DataFrame,
    level_price: float,
    level_type: str,  # 'support' or 'resistance'
    tolerance_atr: float = 0.5
) -> Dict[str, Any]:
    """Ø´Ù…Ø§Ø±Ø´ ØªØ¹Ø¯Ø§Ø¯ Ø¯ÙØ¹Ø§ØªÛŒ Ú©Ù‡ Ù‚ÛŒÙ…Øª Ø³Ø·Ø­ Ø±Ø§ ØªØ³Øª Ú©Ø±Ø¯Ù‡"""

    atr_values = talib.ATR(df['high'].values, df['low'].values,
                           df['close'].values, timeperiod=14)
    current_atr = atr_values[~np.isnan(atr_values)][-1]

    threshold = current_atr * tolerance_atr

    touches = 0
    rejections = 0  # ØªØ¹Ø¯Ø§Ø¯ Ø¯ÙØ¹Ø§Øª Ø¨Ø±Ú¯Ø´Øª Ø§Ø² Ø³Ø·Ø­

    for i in range(len(df)):
        high = df['high'].iloc[i]
        low = df['low'].iloc[i]
        close = df['close'].iloc[i]

        if level_type == 'resistance':
            # Ø¢ÛŒØ§ Ú©Ù†Ø¯Ù„ Ø¨Ù‡ Ø³Ø·Ø­ Ù…Ù‚Ø§ÙˆÙ…Øª Ø±Ø³ÛŒØ¯Ù‡ØŸ
            if abs(high - level_price) <= threshold:
                touches += 1

                # Ø¢ÛŒØ§ Ø¨Ø±Ú¯Ø´Øª Ø®ÙˆØ±Ø¯Ù‡ØŸ (rejection)
                if close < level_price - (threshold * 0.5):
                    rejections += 1

        else:  # support
            # Ø¢ÛŒØ§ Ú©Ù†Ø¯Ù„ Ø¨Ù‡ Ø³Ø·Ø­ Ø­Ù…Ø§ÛŒØª Ø±Ø³ÛŒØ¯Ù‡ØŸ
            if abs(low - level_price) <= threshold:
                touches += 1

                # Ø¢ÛŒØ§ Ø¨Ø±Ú¯Ø´Øª Ø®ÙˆØ±Ø¯Ù‡ØŸ
                if close > level_price + (threshold * 0.5):
                    rejections += 1

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ rejection rate
    rejection_rate = rejections / touches if touches > 0 else 0

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‚Ø¯Ø±Øª Ø¨Ø± Ø§Ø³Ø§Ø³ touches
    # Ù‡Ø± Ú†Ù‡ Ø¨ÛŒØ´ØªØ± ØªØ³Øª Ø´Ø¯Ù‡ â†’ Ù‚ÙˆÛŒâ€ŒØªØ±
    if touches >= 5:
        touch_strength = 1.0
    elif touches >= 3:
        touch_strength = 0.8
    elif touches >= 2:
        touch_strength = 0.6
    else:
        touch_strength = 0.4

    # rejection rate Ø¨Ø§Ù„Ø§ â†’ Ø³Ø·Ø­ Ù‚ÙˆÛŒâ€ŒØªØ±
    rejection_strength = rejection_rate  # 0.0 to 1.0

    # ØªØ±Ú©ÛŒØ¨
    overall_strength = (touch_strength * 0.6 + rejection_strength * 0.4)

    return {
        'touches': touches,
        'rejections': rejections,
        'rejection_rate': rejection_rate,
        'touch_strength': touch_strength,
        'rejection_strength': rejection_strength,
        'overall_strength': overall_strength
    }
```

**Ø§Ø¯ØºØ§Ù… Ø¯Ø± `detect_support_resistance`:**

```python
# Ø¨Ø¹Ø¯ Ø§Ø² consolidate_levels (Ø®Ø· 2376-2377)
results['resistance_levels'] = consolidate_levels(resistance_levels_raw, last_atr)
results['support_levels'] = consolidate_levels(support_levels_raw, last_atr)

# Ø§ÙØ²ÙˆØ¯Ù† touch count Ø¨Ù‡ Ù‡Ø± Ø³Ø·Ø­
for level in results['resistance_levels']:
    touch_data = self.count_level_touches(df, level['price'], 'resistance')
    level['touches'] = touch_data['touches']
    level['rejection_rate'] = touch_data['rejection_rate']

    # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ strength Ø¨Ø± Ø§Ø³Ø§Ø³ touches
    level['strength'] = (
        level['strength'] * 0.5 +           # cluster strength (Ù‚Ø¨Ù„ÛŒ)
        touch_data['overall_strength'] * 0.5  # touch strength (Ø¬Ø¯ÛŒØ¯)
    )

for level in results['support_levels']:
    touch_data = self.count_level_touches(df, level['price'], 'support')
    level['touches'] = touch_data['touches']
    level['rejection_rate'] = touch_data['rejection_rate']
    level['strength'] = (
        level['strength'] * 0.5 +
        touch_data['overall_strength'] * 0.5
    )
```

**Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ù‡â€ŒØ±ÙˆØ² Ø´Ø¯Ù‡:**

```python
'resistance_levels': [
    {
        'price': 50200,
        'strength': 0.92,  # ØªØ±Ú©ÛŒØ¨ cluster + touches
        'touches': 5,      # 5 Ø¨Ø§Ø± ØªØ³Øª Ø´Ø¯Ù‡ âœ“
        'rejection_rate': 0.80  # 4 Ø§Ø² 5 Ø¨Ø§Ø± Ø¨Ø±Ú¯Ø´Øª Ø®ÙˆØ±Ø¯Ù‡
    }
]
```

**Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ù‡Ø¨ÙˆØ¯:** +12% Ø¨Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ù‚Ø¯Ø±Øª Ø³Ø·ÙˆØ­

---

### Ù…Ø´Ú©Ù„ 4: Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Clustering Ø³Ø§Ø¯Ù‡

**Ø´Ø¯Øª Ù…Ø´Ú©Ù„:** ğŸŸ¢ Ù¾Ø§ÛŒÛŒÙ†
**ØªØ£Ø«ÛŒØ± Ø¨Ø± Ø¯Ù‚Øª:** +8% Ø¨Ù‡Ø¨ÙˆØ¯

**ØªÙˆØ¶ÛŒØ­ Ù…Ø´Ú©Ù„:**

Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… ÙØ¹Ù„ÛŒ (`signal_generator.py:2333-2370`) Ø§Ø² **Sequential Clustering** Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯:

```python
for level in sorted_levels:
    if abs(level - cluster_mean) <= threshold:
        current_cluster.append(level)
    else:
        save_cluster()
        start_new_cluster(level)
```

Ø§ÛŒÙ† Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø³Ø§Ø¯Ù‡ Ø§Ø³Øª Ø§Ù…Ø§ Ù…Ø´Ú©Ù„Ø§ØªÛŒ Ø¯Ø§Ø±Ø¯:
- ÙÙ‚Ø· Ø³Ø·ÙˆØ­ **Ù…Ø±ØªØ¨ Ø´Ø¯Ù‡** Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
- Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ clusters Ø¨Ø§ **Ú†Ú¯Ø§Ù„ÛŒ Ù…ØªØºÛŒØ±** Ø±Ø§ ØªØ´Ø®ÛŒØµ Ø¯Ù‡Ø¯
- Ø­Ø³Ø§Ø³ Ø¨Ù‡ **outlier** Ø§Ø³Øª

**Ù…Ø«Ø§Ù„ Ù…Ø´Ú©Ù„:**
```python
levels = [50000, 50050, 50500, 50520, 50550]
# Ø¨Ø§ threshold = 100:

# Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… ÙØ¹Ù„ÛŒ:
Cluster 1: [50000, 50050]  # OK
Cluster 2: [50500, 50520, 50550]  # OK

# Ø§Ù…Ø§ Ø§Ú¯Ø±:
levels = [50000, 50050, 50500, 50100, 50520]  # ÛŒÚ© outlier Ø¯Ø± ÙˆØ³Ø·

# Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… ÙØ¹Ù„ÛŒ:
Cluster 1: [50000, 50050, 50100]  # 50100 Ù†Ø¨Ø§ÛŒØ¯ Ø§ÛŒÙ†Ø¬Ø§ Ø¨Ø§Ø´Ø¯!
Cluster 2: [50500, 50520]
```

---

**Ø±Ø§Ù‡ Ø­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**

Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² **DBSCAN** (Density-Based Spatial Clustering):

```python
from sklearn.cluster import DBSCAN

def consolidate_levels_dbscan(
    self,
    levels: np.ndarray,
    atr: float
) -> List[Dict[str, Any]]:
    """Clustering Ø³Ø·ÙˆØ­ Ø¨Ø§ DBSCAN"""

    if len(levels) == 0:
        return []

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ eps (Ø­Ø¯Ø§Ú©Ø«Ø± ÙØ§ØµÙ„Ù‡ Ø¯Ø± ÛŒÚ© cluster)
    eps = atr * 0.3
    if eps <= 1e-9:
        eps = np.mean(levels) * 0.001 if np.mean(levels) > 0 else 1e-5

    # Reshape Ø¨Ø±Ø§ÛŒ DBSCAN
    X = levels.reshape(-1, 1)

    # DBSCAN clustering
    clustering = DBSCAN(eps=eps, min_samples=1).fit(X)
    labels = clustering.labels_

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ clusters
    unique_labels = set(labels)
    clusters = []

    for label in unique_labels:
        if label == -1:  # Outliers
            # Ù‡Ø± outlier ÛŒÚ© cluster Ù…Ø³ØªÙ‚Ù„
            outlier_indices = np.where(labels == label)[0]
            for idx in outlier_indices:
                clusters.append({
                    'price': float(levels[idx]),
                    'strength': 0.3,  # Ù‚Ø¯Ø±Øª Ù¾Ø§ÛŒÛŒÙ† (ØªÙ†Ù‡Ø§)
                    'touches': 1,
                    'is_outlier': True
                })
        else:
            # Cluster Ø¹Ø§Ø¯ÛŒ
            cluster_indices = np.where(labels == label)[0]
            cluster_levels = levels[cluster_indices]

            cluster_mean = np.mean(cluster_levels)
            cluster_std = np.std(cluster_levels)
            cluster_size = len(cluster_levels)

            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‚Ø¯Ø±Øª
            size_strength = min(1.0, cluster_size / 3)
            uniformity = 1.0 - (cluster_std / cluster_mean if cluster_mean > 0 else 0)
            cluster_strength = size_strength * uniformity

            clusters.append({
                'price': float(cluster_mean),
                'strength': float(cluster_strength),
                'touches': cluster_size,
                'is_outlier': False
            })

    return sorted(clusters, key=lambda x: x['price'])
```

**Ù…Ù‚Ø§ÛŒØ³Ù‡:**

```python
# Ø³Ø·ÙˆØ­: [50000, 50050, 50500, 50100, 50520]

# Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… ÙØ¹Ù„ÛŒ (Sequential):
[
    {'price': 50050, 'strength': 0.66},  # [50000, 50050, 50100]
    {'price': 50510, 'strength': 0.66}   # [50500, 50520]
]

# Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø¬Ø¯ÛŒØ¯ (DBSCAN):
[
    {'price': 50025, 'strength': 0.66},  # [50000, 50050]
    {'price': 50100, 'strength': 0.30, 'is_outlier': True},  # outlier
    {'price': 50510, 'strength': 0.66}   # [50500, 50520]
]
# Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±! âœ“
```

**Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ù‡Ø¨ÙˆØ¯:** +8% Ø¨Ø§ clustering Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±

---

### Ù…Ø´Ú©Ù„ 5: Ø¹Ø¯Ù… Multi-Timeframe Confluence Ø¯Ø± S/R

**Ø´Ø¯Øª Ù…Ø´Ú©Ù„:** ğŸ”´ Ø¨Ø§Ù„Ø§
**ØªØ£Ø«ÛŒØ± Ø¨Ø± Ø¯Ù‚Øª:** +22% Ø¨Ù‡Ø¨ÙˆØ¯

**ØªÙˆØ¶ÛŒØ­ Ù…Ø´Ú©Ù„:**

Ø³Ø·ÙˆØ­ S/R Ø¯Ø± ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ Ø§Ù…Ø§ **Ù‡ÛŒÚ† Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù¾ÙˆØ´Ø§Ù†ÛŒ** ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯:

```python
# 1h: support at 50,000
# 4h: support at 49,980
# 1d: support at 50,020

# Ø§ÛŒÙ† 3 Ø³Ø·Ø­ Ø¯Ø± ÙˆØ§Ù‚Ø¹ ÛŒÚ© Ù†Ø§Ø­ÛŒÙ‡ Ù‚ÙˆÛŒ 49,980-50,020 Ù…ÛŒâ€ŒØ³Ø§Ø²Ù†Ø¯!
# Ø§Ù…Ø§ Ø³ÛŒØ³ØªÙ… ÙØ¹Ù„ÛŒ Ù‡Ø± Ú©Ø¯Ø§Ù… Ø±Ø§ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ù…ÛŒâ€ŒØ¨ÛŒÙ†Ø¯
```

Ø³Ø·ÙˆØ­ÛŒ Ú©Ù‡ Ø¯Ø± **Ú†Ù†Ø¯ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ… Ù‡Ù…Ø²Ù…Ø§Ù†** Ù‡Ø³ØªÙ†Ø¯ â†’ **Ø¨Ø³ÛŒØ§Ø± Ù‚ÙˆÛŒâ€ŒØªØ±**

---

**Ø±Ø§Ù‡ Ø­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**

```python
def calculate_mtf_sr_confluence(
    self,
    all_timeframes_sr: Dict[str, Dict]  # {tf: sr_data}
) -> Dict[str, Any]:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‡Ù…Ù¾ÙˆØ´Ø§Ù†ÛŒ Ø³Ø·ÙˆØ­ S/R Ø¯Ø± ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù"""

    # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ ØªÙ…Ø§Ù… Ø³Ø·ÙˆØ­
    all_support_levels = []
    all_resistance_levels = []

    for tf, sr_data in all_timeframes_sr.items():
        if sr_data.get('status') != 'ok':
            continue

        tf_weight = self.timeframe_weights.get(tf, 1.0)

        for level in sr_data.get('support_levels', []):
            all_support_levels.append({
                'price': level['price'],
                'strength': level['strength'],
                'timeframe': tf,
                'tf_weight': tf_weight
            })

        for level in sr_data.get('resistance_levels', []):
            all_resistance_levels.append({
                'price': level['price'],
                'strength': level['strength'],
                'timeframe': tf,
                'tf_weight': tf_weight
            })

    # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† confluences
    def find_confluences(levels: List[Dict], atr: float) -> List[Dict]:
        """Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø³Ø·ÙˆØ­ÛŒ Ú©Ù‡ Ø¯Ø± Ú†Ù†Ø¯ TF Ù‡Ù…Ù¾ÙˆØ´Ø§Ù†ÛŒ Ø¯Ø§Ø±Ù†Ø¯"""

        if len(levels) == 0:
            return []

        threshold = atr * 0.5  # Ø³Ø·ÙˆØ­ Ù†Ø²Ø¯ÛŒÚ©ØªØ± Ø§Ø² 0.5 ATR

        confluences = []
        used_indices = set()

        for i, level1 in enumerate(levels):
            if i in used_indices:
                continue

            # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø³Ø·ÙˆØ­ Ù†Ø²Ø¯ÛŒÚ© Ø¯Ø± Ø³Ø§ÛŒØ± TF Ù‡Ø§
            cluster = [level1]
            used_indices.add(i)

            for j, level2 in enumerate(levels):
                if j <= i or j in used_indices:
                    continue

                # Ø¢ÛŒØ§ Ù†Ø²Ø¯ÛŒÚ© Ù‡Ø³ØªÙ†Ø¯ Ùˆ Ø§Ø² TF Ù…ØªÙØ§ÙˆØªØŸ
                if (abs(level1['price'] - level2['price']) <= threshold and
                    level1['timeframe'] != level2['timeframe']):
                    cluster.append(level2)
                    used_indices.add(j)

            # Ø§Ú¯Ø± ÙÙ‚Ø· ÛŒÚ© TF Ø¨Ø§Ø´Ø¯ â†’ confluence Ù†ÛŒØ³Øª
            if len(cluster) == 1:
                continue

            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø´Ø®ØµØ§Øª confluence
            prices = [lv['price'] for lv in cluster]
            strengths = [lv['strength'] for lv in cluster]
            tf_weights = [lv['tf_weight'] for lv in cluster]
            timeframes = [lv['timeframe'] for lv in cluster]

            avg_price = np.mean(prices)

            # Ù‚Ø¯Ø±Øª = Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ÙˆØ²Ù†â€ŒØ¯Ø§Ø± strength Ù‡Ø§
            weighted_strength = sum(s * w for s, w in zip(strengths, tf_weights)) / sum(tf_weights)

            # Confluence bonus: +20% Ø¨Ø±Ø§ÛŒ Ù‡Ø± TF Ø§Ø¶Ø§ÙÛŒ
            confluence_count = len(set(timeframes))
            confluence_bonus = 1.0 + (confluence_count - 1) * 0.20

            final_strength = min(1.0, weighted_strength * confluence_bonus)

            confluences.append({
                'price': avg_price,
                'strength': final_strength,
                'timeframes': list(set(timeframes)),
                'confluence_count': confluence_count,
                'price_range': (min(prices), max(prices))
            })

        return sorted(confluences, key=lambda x: x['strength'], reverse=True)

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ ATR Ø§Ø² ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ… Ø§ØµÙ„ÛŒ
    primary_tf = list(all_timeframes_sr.keys())[0]
    atr = all_timeframes_sr[primary_tf].get('details', {}).get('atr', 100)

    support_confluences = find_confluences(all_support_levels, atr)
    resistance_confluences = find_confluences(all_resistance_levels, atr)

    return {
        'status': 'ok',
        'support_confluences': support_confluences,
        'resistance_confluences': resistance_confluences,
        'total_confluences': len(support_confluences) + len(resistance_confluences)
    }
```

**Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÚ¯ÛŒØ±ÛŒ:**

```python
# Ù…Ø­Ø§Ø³Ø¨Ù‡ confluences
all_tf_sr = {
    '15m': tf_results['15m'].get('support_resistance'),
    '1h': tf_results['1h'].get('support_resistance'),
    '4h': tf_results['4h'].get('support_resistance')
}

mtf_confluences = self.calculate_mtf_sr_confluence(all_tf_sr)

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø²
for confluence in mtf_confluences['support_confluences']:
    if signal_direction == 'bullish':
        # Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø²Ø¯ÛŒÚ©ÛŒ Ù‚ÛŒÙ…Øª Ø¨Ù‡ confluence
        distance = abs(current_price - confluence['price'])
        if distance < atr * 0.5:
            # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø§Ù„Ø§ Ø¨Ø±Ø§ÛŒ MTF confluence
            bonus_score = 3.0 * confluence['strength']
            bullish_score += bonus_score

            all_signals.append({
                'type': 'mtf_support_confluence',
                'score': bonus_score,
                'timeframes': confluence['timeframes'],
                'confluence_count': confluence['confluence_count']
            })
```

**Ù…Ø«Ø§Ù„:**

```python
# Ø³Ø·ÙˆØ­:
# 15m: support at 50,000 (strength: 0.7)
# 1h:  support at 49,990 (strength: 0.8)
# 4h:  support at 50,010 (strength: 0.9)

# Confluence ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯:
{
    'price': 50000,  # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†
    'strength': 0.95,  # 0.8 (weighted avg) Ã— 1.4 (3-TF bonus)
    'timeframes': ['15m', '1h', '4h'],
    'confluence_count': 3,
    'price_range': (49990, 50010)
}

# Ø§Ù…ØªÛŒØ§Ø²: 3.0 Ã— 0.95 = +2.85 (Ø¨Ø³ÛŒØ§Ø± Ù‚ÙˆÛŒ!)
```

**Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ù‡Ø¨ÙˆØ¯:** +22% Ø¨Ø§ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø³Ø·ÙˆØ­ Ú†Ù†Ø¯-ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…ÛŒ

---

### Ù…Ø´Ú©Ù„ 6: Ø¹Ø¯Ù… ØªØ´Ø®ÛŒØµ Retest/Pullback

**Ø´Ø¯Øª Ù…Ø´Ú©Ù„:** ğŸŸ¡ Ù…ØªÙˆØ³Ø·
**ØªØ£Ø«ÛŒØ± Ø¨Ø± Ø¯Ù‚Øª:** +15% Ø¨Ù‡Ø¨ÙˆØ¯

**ØªÙˆØ¶ÛŒØ­ Ù…Ø´Ú©Ù„:**

Ø¨Ø¹Ø¯ Ø§Ø² ÛŒÚ© breakout Ù…ÙˆÙÙ‚ØŒ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ù‚ÛŒÙ…Øª **Ø¨Ø§Ø²Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø¯ Ùˆ Ø³Ø·Ø­ Ø´Ú©Ø³ØªÙ‡ Ø´Ø¯Ù‡ Ø±Ø§ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªØ³Øª Ù…ÛŒâ€ŒÚ©Ù†Ø¯** (retest/pullback). Ø§ÛŒÙ† ÛŒÚ© ÙØ±ØµØª Ø¹Ø§Ù„ÛŒ Ø¨Ø±Ø§ÛŒ ÙˆØ±ÙˆØ¯ Ø§Ø³Øª:

```python
# Ú©Ù†Ø¯Ù„ 1-5: Ù‚ÛŒÙ…Øª Ù†Ø²Ø¯ÛŒÚ© Ù…Ù‚Ø§ÙˆÙ…Øª 50,000
# Ú©Ù†Ø¯Ù„ 6: Breakout! Ù‚ÛŒÙ…Øª Ø¨Ù‡ 50,300 Ù…ÛŒâ€ŒØ±Ø³Ø¯
# Ú©Ù†Ø¯Ù„ 7-10: Pullback Ø¨Ù‡ 50,050 (ØªØ³Øª Ù…Ø¬Ø¯Ø¯ Ø³Ø·Ø­ Ø´Ú©Ø³ØªÙ‡ Ø´Ø¯Ù‡)
# Ú©Ù†Ø¯Ù„ 11+: Ø§Ø¯Ø§Ù…Ù‡ ØµØ¹ÙˆØ¯ Ø¨Ù‡ 51,000

# ÙˆØ±ÙˆØ¯ Ø¯Ø± pullback (50,050) Ø¨Ù‡ØªØ± Ø§Ø² ÙˆØ±ÙˆØ¯ Ø¯Ø± breakout (50,300) Ø§Ø³Øª!
```

Ø§Ù…Ø§ Ø³ÛŒØ³ØªÙ… ÙØ¹Ù„ÛŒ Ø§ÛŒÙ† Ø±Ø§ **ØªØ´Ø®ÛŒØµ Ù†Ù…ÛŒâ€ŒØ¯Ù‡Ø¯**.

---

**Ø±Ø§Ù‡ Ø­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**

```python
def detect_retest_opportunity(
    self,
    df: pd.DataFrame,
    sr_data: Dict[str, Any],
    lookback: int = 20
) -> Dict[str, Any]:
    """ØªØ´Ø®ÛŒØµ ÙØ±ØµØª retest Ø¨Ø¹Ø¯ Ø§Ø² breakout"""

    current_close = df['close'].iloc[-1]

    # Ø¨Ø±Ø±Ø³ÛŒ breakout Ù‡Ø§ÛŒ Ø§Ø®ÛŒØ±
    recent_broken_resistance = None
    recent_broken_support = None
    breakout_candle_idx = None

    # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ú©Ù†Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø§Ø®ÛŒØ±
    for i in range(1, min(lookback, len(df))):
        idx = -i
        past_close = df['close'].iloc[idx]
        past_high = df['high'].iloc[idx]
        past_low = df['low'].iloc[idx]

        # Ø¨Ø±Ø±Ø³ÛŒ breakout Ù…Ù‚Ø§ÙˆÙ…Øª Ø¯Ø± Ú¯Ø°Ø´ØªÙ‡
        for res_level in sr_data.get('resistance_levels', []):
            res_price = res_level['price']

            # Ø¢ÛŒØ§ Ø¯Ø± Ø§ÛŒÙ† Ú©Ù†Ø¯Ù„ breakout Ø±Ø® Ø¯Ø§Ø¯Ù‡ØŸ
            if (past_close > res_price and
                df['close'].iloc[idx-1] < res_price):  # Ú©Ù†Ø¯Ù„ Ù‚Ø¨Ù„Ø´ Ø²ÛŒØ± Ø¨ÙˆØ¯

                # Ø¢ÛŒØ§ Ù‚ÛŒÙ…Øª ÙØ¹Ù„ÛŒ Ø¯Ø± Ø­Ø§Ù„ retest Ø§Ø³ØªØŸ
                distance = abs(current_close - res_price)
                atr = sr_data.get('details', {}).get('atr', 100)

                if distance < atr * 0.5 and current_close > res_price:
                    # Ù‚ÛŒÙ…Øª Ù†Ø²Ø¯ÛŒÚ© Ø³Ø·Ø­ Ø´Ú©Ø³ØªÙ‡ Ø´Ø¯Ù‡ Ùˆ Ù‡Ù†ÙˆØ² Ø¨Ø§Ù„Ø§ØªØ± Ø§Ø³Øª
                    recent_broken_resistance = res_level
                    breakout_candle_idx = i
                    break

        # Ø¨Ø±Ø±Ø³ÛŒ breakout Ø­Ù…Ø§ÛŒØª
        for sup_level in sr_data.get('support_levels', []):
            sup_price = sup_level['price']

            if (past_close < sup_price and
                df['close'].iloc[idx-1] > sup_price):

                distance = abs(current_close - sup_price)
                atr = sr_data.get('details', {}).get('atr', 100)

                if distance < atr * 0.5 and current_close < sup_price:
                    recent_broken_support = sup_level
                    breakout_candle_idx = i
                    break

        if recent_broken_resistance or recent_broken_support:
            break

    # Ø§Ú¯Ø± retest Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯
    if not recent_broken_resistance and not recent_broken_support:
        return {'status': 'no_retest'}

    # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©ÛŒÙÛŒØª retest
    if recent_broken_resistance:
        level_price = recent_broken_resistance['price']
        level_strength = recent_broken_resistance['strength']
        direction = 'bullish'
    else:
        level_price = recent_broken_support['price']
        level_strength = recent_broken_support['strength']
        direction = 'bearish'

    # 1. Timing: Ú†Ù‚Ø¯Ø± Ø§Ø² breakout Ú¯Ø°Ø´ØªÙ‡ØŸ
    if breakout_candle_idx <= 5:
        timing_score = 1.0  # ØªØ§Ø²Ù‡
    elif breakout_candle_idx <= 10:
        timing_score = 0.7  # Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„
    else:
        timing_score = 0.4  # Ù‚Ø¯ÛŒÙ…ÛŒ

    # 2. Price Action: Ø¢ÛŒØ§ Ø¨Ø§ Ø­Ø¬Ù… Ù¾Ø§ÛŒÛŒÙ† retest Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŸ
    recent_volume = df['volume'].iloc[-3:].mean() if 'volume' in df.columns else 1
    breakout_volume = df['volume'].iloc[-breakout_candle_idx] if 'volume' in df.columns else 1

    volume_ratio = recent_volume / breakout_volume if breakout_volume > 0 else 1

    # Retest Ø®ÙˆØ¨: Ø­Ø¬Ù… Ù¾Ø§ÛŒÛŒÙ† (Ø¨ÛŒâ€ŒØ¹Ù„Ø§Ù‚Ú¯ÛŒ ÙØ±ÙˆØ´Ù†Ø¯Ú¯Ø§Ù†)
    if volume_ratio < 0.7:
        volume_score = 1.0  # Ø¹Ø§Ù„ÛŒ
    elif volume_ratio < 1.0:
        volume_score = 0.7
    else:
        volume_score = 0.4  # Ø­Ø¬Ù… Ø¨Ø§Ù„Ø§ = Ù…Ø´Ú©ÙˆÚ©

    # 3. Price Respect: Ø¢ÛŒØ§ Ù‚ÛŒÙ…Øª Ø³Ø·Ø­ Ø±Ø§ Ø±Ø¹Ø§ÛŒØª Ú©Ø±Ø¯Ù‡ØŸ
    lowest_since_breakout = df['low'].iloc[-breakout_candle_idx:].min()
    highest_since_breakout = df['high'].iloc[-breakout_candle_idx:].max()

    if direction == 'bullish':
        # Ø¢ÛŒØ§ Ù‚ÛŒÙ…Øª Ø²ÛŒØ± Ø³Ø·Ø­ Ù†Ø±ÙØªÙ‡ØŸ
        if lowest_since_breakout >= level_price:
            respect_score = 1.0  # Ú©Ø§Ù…Ù„
        elif lowest_since_breakout >= level_price * 0.995:
            respect_score = 0.7  # Ù†Ø³Ø¨ÛŒ
        else:
            respect_score = 0.3  # Ø´Ú©Ø³Øª Ø®ÙˆØ±Ø¯Ù‡
    else:
        # Ø¢ÛŒØ§ Ù‚ÛŒÙ…Øª Ø¨Ø§Ù„Ø§ÛŒ Ø³Ø·Ø­ Ù†Ø±ÙØªÙ‡ØŸ
        if highest_since_breakout <= level_price:
            respect_score = 1.0
        elif highest_since_breakout <= level_price * 1.005:
            respect_score = 0.7
        else:
            respect_score = 0.3

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ú©Ù„ÛŒ
    retest_quality = (
        timing_score * 0.3 +
        volume_score * 0.4 +
        respect_score * 0.3
    )

    # Ø§Ù…ØªÛŒØ§Ø² Ù†Ù‡Ø§ÛŒÛŒ
    if retest_quality >= 0.7:
        retest_score = 3.5 * level_strength  # ÙØ±ØµØª Ø¹Ø§Ù„ÛŒ
        quality_label = 'excellent'
    elif retest_quality >= 0.5:
        retest_score = 2.0 * level_strength
        quality_label = 'good'
    else:
        retest_score = 0.8 * level_strength
        quality_label = 'weak'

    return {
        'status': 'retest_detected',
        'direction': direction,
        'level_price': level_price,
        'level_strength': level_strength,
        'breakout_candles_ago': breakout_candle_idx,
        'retest_quality': retest_quality,
        'quality_label': quality_label,
        'score': retest_score,
        'components': {
            'timing_score': timing_score,
            'volume_score': volume_score,
            'respect_score': respect_score
        }
    }
```

**Ø§Ø³ØªÙØ§Ø¯Ù‡:**

```python
# Ø¯Ø± calculate_multi_timeframe_score
for tf, tf_data in analysis.items():
    sr_data = tf_data.get('support_resistance', {})

    # ØªØ´Ø®ÛŒØµ retest
    retest = self.detect_retest_opportunity(dfs[tf], sr_data)

    if retest['status'] == 'retest_detected':
        score = retest['score'] * tf_weight

        if retest['direction'] == 'bullish':
            bullish_score += score
        else:
            bearish_score += score

        all_signals.append({
            'type': 'retest_opportunity',
            'timeframe': tf,
            'score': score,
            'direction': retest['direction'],
            'quality': retest['quality_label'],
            'level_price': retest['level_price']
        })
```

**Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ù‡Ø¨ÙˆØ¯:** +15% Ø¨Ø§ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ÙØ±ØµØªâ€ŒÙ‡Ø§ÛŒ retest

---

## Ø®Ù„Ø§ØµÙ‡ Ø¨Ù‡Ø¨ÙˆØ¯Ù‡Ø§ÛŒ Support/Resistance Detection

| # | Ù…Ø´Ú©Ù„ | ØªØ£Ø«ÛŒØ± | Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ |
|---|------|-------|-------------------|
| 1 | Ø¹Ø¯Ù… Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Proximity | **+25%** | Ù…ØªÙˆØ³Ø· |
| 2 | Ø¹Ø¯Ù… Validation Ù‚Ø¯Ø±Øª Breakout | **+18%** | Ù…ØªÙˆØ³Ø· |
| 3 | Ø¹Ø¯Ù… Ø±Ø¯ÛŒØ§Ø¨ÛŒ Test Ù‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ | **+12%** | Ø³Ø§Ø¯Ù‡ |
| 4 | Clustering Ø³Ø§Ø¯Ù‡ | **+8%** | Ù…ØªÙˆØ³Ø· |
| 5 | Ø¹Ø¯Ù… MTF Confluence | **+22%** | Ù¾ÛŒÚ†ÛŒØ¯Ù‡ |
| 6 | Ø¹Ø¯Ù… ØªØ´Ø®ÛŒØµ Retest | **+15%** | Ù…ØªÙˆØ³Ø· |

**Ù…Ø¬Ù…ÙˆØ¹ ØªØ£Ø«ÛŒØ± ØªØ®Ù…ÛŒÙ†ÛŒ:** +60-70% Ø¨Ù‡Ø¨ÙˆØ¯ Ø¯Ø± Ø¯Ù‚Øª Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ S/R

---

## Ø§ÙˆÙ„ÙˆÛŒØªâ€ŒØ¨Ù†Ø¯ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ

**ÙØ§Ø² 1 (Ø¶Ø±ÙˆØ±ÛŒ - 2 Ù‡ÙØªÙ‡):**
1. Proximity Scoring (Ù…Ø´Ú©Ù„ 1)
2. Breakout Validation (Ù…Ø´Ú©Ù„ 2)

**ÙØ§Ø² 2 (Ù…Ù‡Ù… - 1 Ù‡ÙØªÙ‡):**
3. Touch Counting (Ù…Ø´Ú©Ù„ 3)
4. Retest Detection (Ù…Ø´Ú©Ù„ 6)

**ÙØ§Ø² 3 (Ø¨Ù‡Ø¨ÙˆØ¯ - 2 Ù‡ÙØªÙ‡):**
5. MTF Confluence (Ù…Ø´Ú©Ù„ 5)
6. DBSCAN Clustering (Ù…Ø´Ú©Ù„ 4)

---

## Backtesting Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ

1. **Proximity Effectiveness:**
   - Ù…Ù‚Ø§ÛŒØ³Ù‡ win rate Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù†Ø²Ø¯ÛŒÚ© S/R vs Ø¯ÙˆØ± Ø§Ø² S/R
   - ÛŒØ§ÙØªÙ† ÙØ§ØµÙ„Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡ (Ú†Ù†Ø¯ ATRØŸ)

2. **Breakout Quality:**
   - ØªØ­Ù„ÛŒÙ„ correlation Ø¨ÛŒÙ† validation score Ùˆ Ø³ÙˆØ¯ Ù…Ø¹Ø§Ù…Ù„Ù‡
   - Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø¢Ø³ØªØ§Ù†Ù‡ Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„ØªØ± fake breakouts

3. **Retest Success Rate:**
   - Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø±ØµØ¯ Ù…ÙˆÙÙ‚ÛŒØª ÙˆØ±ÙˆØ¯ Ø¯Ø± retest vs ÙˆØ±ÙˆØ¯ Ø¯Ø± breakout
   - ØªØ¹ÛŒÛŒÙ† Ø¨Ù‡ØªØ±ÛŒÙ† ØªØ§ÛŒÙ…ÛŒÙ†Ú¯ Ø¨Ø±Ø§ÛŒ ÙˆØ±ÙˆØ¯

---

## Ù…Ø±Ø­Ù„Ù‡ 7: Ø¨Ù‡Ø¨ÙˆØ¯ Price Channels Detection

### Ù…Ø´Ú©Ù„ 1: Ø¹Ø¯Ù… ØªØ´Ø®ÛŒØµ Ú†Ù†Ø¯ÛŒÙ† Ú©Ø§Ù†Ø§Ù„ Ù‡Ù…Ø²Ù…Ø§Ù†

**Ø´Ø¯Øª Ù…Ø´Ú©Ù„:** ğŸŸ¡ Ù…ØªÙˆØ³Ø·
**ØªØ£Ø«ÛŒØ± Ø¨Ø± Ø¯Ù‚Øª:** +15% Ø¨Ù‡Ø¨ÙˆØ¯

**ØªÙˆØ¶ÛŒØ­ Ù…Ø´Ú©Ù„:**

Ø¯Ø± Ú©Ø¯ ÙØ¹Ù„ÛŒ (`signal_generator.py:2666-2768`), **ÙÙ‚Ø· ÛŒÚ© Ú©Ø§Ù†Ø§Ù„ Ø§ØµÙ„ÛŒ** Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ø§Ù…Ø§ Ø¯Ø± ÙˆØ§Ù‚Ø¹ÛŒØª Ù…Ù…Ú©Ù† Ø§Ø³Øª Ú†Ù†Ø¯ÛŒÙ† Ú©Ø§Ù†Ø§Ù„ Ø¯Ø± ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯ (Ú©Ø§Ù†Ø§Ù„ Ú©ÙˆØªØ§Ù‡â€ŒÙ…Ø¯Øª Ø¯Ø§Ø®Ù„ Ú©Ø§Ù†Ø§Ù„ Ø¨Ù„Ù†Ø¯Ù…Ø¯Øª - Nested Channels).

**Ø±Ø§Ù‡ Ø­Ù„:** Multi-Scale Channel Detection Ø¨Ø±Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ Ø¯Ø± lookback Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù (50, 100, 200) Ùˆ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¢Ù†Ù‡Ø§ Ø¨Ù‡ major/minor/nested.

**Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ù‡Ø¨ÙˆØ¯:** +15%

---

### Ù…Ø´Ú©Ù„ 2: Ø¹Ø¯Ù… Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Breakout Ø¨Ø§ Ø­Ø¬Ù…

**Ø´Ø¯Øª Ù…Ø´Ú©Ù„:** ğŸ”´ Ø¨Ø§Ù„Ø§
**ØªØ£Ø«ÛŒØ± Ø¨Ø± Ø¯Ù‚Øª:** +20% Ø¨Ù‡Ø¨ÙˆØ¯

**ØªÙˆØ¶ÛŒØ­ Ù…Ø´Ú©Ù„:**

Breakout ÙÙ‚Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ Ù‚ÛŒÙ…Øª ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ø¨Ø¯ÙˆÙ† Ø¨Ø±Ø±Ø³ÛŒ Ø­Ø¬Ù…. Ø§ÛŒÙ† Ù…Ù†Ø¬Ø± Ø¨Ù‡ False Breakouts Ù…ÛŒâ€ŒØ´ÙˆØ¯.

**Ø±Ø§Ù‡ Ø­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**

```python
def validate_channel_breakout(df, channel, breakout_direction):
    # 1. Volume Confirmation (volume_ratio > 1.5 = strong)
    # 2. Penetration Depth (> 10% channel width = strong)
    # 3. Body vs Wick (body_ratio > 60% = strong)
    # 4. Momentum (3 candles trend = strong)

    # Score Multiplier:
    # Strong: 1.5x, Moderate: 1.0x, Weak: 0.5x (reject)
```

**Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ù‡Ø¨ÙˆØ¯:** +20%

---

### Ù…Ø´Ú©Ù„ 3: Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø¨Ù‡ Ø®Ø·ÙˆØ· Ø®Ø·ÛŒ

**Ø´Ø¯Øª Ù…Ø´Ú©Ù„:** ğŸŸ¢ Ù¾Ø§ÛŒÛŒÙ†
**ØªØ£Ø«ÛŒØ± Ø¨Ø± Ø¯Ù‚Øª:** +8% Ø¨Ù‡Ø¨ÙˆØ¯

**ØªÙˆØ¶ÛŒØ­ Ù…Ø´Ú©Ù„:**

ÙÙ‚Ø· Linear Regression (Ø®Ø· Ø±Ø§Ø³Øª) Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ø§Ù…Ø§ Ø¨Ø³ÛŒØ§Ø±ÛŒ Ø§Ø² Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ Ù…Ù†Ø­Ù†ÛŒ Ù‡Ø³ØªÙ†Ø¯ (Polynomial/Exponential).

**Ø±Ø§Ù‡ Ø­Ù„:** Polynomial Channel Detection Ø¨Ø§ degree=2 (quadratic) Ùˆ Ù…Ù‚Ø§ÛŒØ³Ù‡ RÂ² Ø¨Ø§ linear.

**Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ù‡Ø¨ÙˆØ¯:** +8%

---

### Ù…Ø´Ú©Ù„ 4: Ø¹Ø¯Ù… Ø±Ø¯ÛŒØ§Ø¨ÛŒ Channel Age

**Ø´Ø¯Øª Ù…Ø´Ú©Ù„:** ğŸŸ¡ Ù…ØªÙˆØ³Ø·
**ØªØ£Ø«ÛŒØ± Ø¨Ø± Ø¯Ù‚Øª:** +10% Ø¨Ù‡Ø¨ÙˆØ¯

**ØªÙˆØ¶ÛŒØ­ Ù…Ø´Ú©Ù„:**

Ù…Ø¯Øª Ø²Ù…Ø§Ù† Ú©Ø§Ù†Ø§Ù„ (Ø³Ù†) Ø±Ø¯ÛŒØ§Ø¨ÛŒ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒâ€ŒØªØ± Ù‚ÙˆÛŒâ€ŒØªØ± Ùˆ Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªÙ…Ø§Ø¯ØªØ± Ù‡Ø³ØªÙ†Ø¯.

**Ø±Ø§Ù‡ Ø­Ù„:** Ù…Ø­Ø§Ø³Ø¨Ù‡ channel duration Ùˆ Ø§Ø¹Ù…Ø§Ù„ age_multiplier (Ø¬Ø¯ÛŒØ¯: 0.9xØŒ Ù‚Ø¯ÛŒÙ…ÛŒ: 1.3x).

**Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ù‡Ø¨ÙˆØ¯:** +10%

---

### Ù…Ø´Ú©Ù„ 5: Ø¹Ø¯Ù… ØªØ´Ø®ÛŒØµ False Breakout

**Ø´Ø¯Øª Ù…Ø´Ú©Ù„:** ğŸ”´ Ø¨Ø§Ù„Ø§
**ØªØ£Ø«ÛŒØ± Ø¨Ø± Ø¯Ù‚Øª:** +18% Ø¨Ù‡Ø¨ÙˆØ¯

**ØªÙˆØ¶ÛŒØ­ Ù…Ø´Ú©Ù„:**

Ø¨Ø¹Ø¯ Ø§Ø² breakoutØŒ Ø§Ú¯Ø± Ù‚ÛŒÙ…Øª Ø¨Ø±Ú¯Ø±Ø¯Ø¯ Ø¨Ù‡ Ø¯Ø§Ø®Ù„ Ú©Ø§Ù†Ø§Ù„ (False Breakout)ØŒ Ø³ÛŒØ³ØªÙ… Ø§ÛŒÙ† Ø±Ø§ ØªØ´Ø®ÛŒØµ Ù†Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.

**Ø±Ø§Ù‡ Ø­Ù„:** ChannelBreakoutTracker Ø¨Ø±Ø§ÛŒ Ø±Ø¯ÛŒØ§Ø¨ÛŒ breakout Ù‡Ø§ Ùˆ Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ø§Ø²Ú¯Ø´Øª Ø¨Ù‡ Ú©Ø§Ù†Ø§Ù„ Ø¯Ø± 5 Ú©Ù†Ø¯Ù„ Ø¨Ø¹Ø¯ÛŒ. Ø¯Ø± ØµÙˆØ±Øª Ø¨Ø§Ø²Ú¯Ø´Øª â†’ Ø³ÛŒÚ¯Ù†Ø§Ù„ Ù…Ø¹Ú©ÙˆØ³ (return to channel).

**Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ù‡Ø¨ÙˆØ¯:** +18%

---

## Ø®Ù„Ø§ØµÙ‡ Ø¨Ù‡Ø¨ÙˆØ¯Ù‡Ø§ÛŒ Price Channels

| # | Ù…Ø´Ú©Ù„ | ØªØ£Ø«ÛŒØ± | Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ |
|---|------|-------|---------|
| 1 | Ø¹Ø¯Ù… ØªØ´Ø®ÛŒØµ Ú†Ù†Ø¯ÛŒÙ† Ú©Ø§Ù†Ø§Ù„ | **+15%** | Ù…ØªÙˆØ³Ø· |
| 2 | Ø¹Ø¯Ù… Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Breakout Ø¨Ø§ Ø­Ø¬Ù… | **+20%** | Ù…ØªÙˆØ³Ø· |
| 3 | Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø¨Ù‡ Ø®Ø·ÙˆØ· Ø®Ø·ÛŒ | **+8%** | Ù¾ÛŒÚ†ÛŒØ¯Ù‡ |
| 4 | Ø¹Ø¯Ù… Ø±Ø¯ÛŒØ§Ø¨ÛŒ Channel Age | **+10%** | Ø³Ø§Ø¯Ù‡ |
| 5 | Ø¹Ø¯Ù… ØªØ´Ø®ÛŒØµ False Breakout | **+18%** | Ù…ØªÙˆØ³Ø· |

**Ù…Ø¬Ù…ÙˆØ¹ ØªØ£Ø«ÛŒØ± ØªØ®Ù…ÛŒÙ†ÛŒ:** +55-65% Ø¨Ù‡Ø¨ÙˆØ¯

---

**ØªØ§Ø±ÛŒØ® Ø¢Ø®Ø±ÛŒÙ† Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ:** 2025-10-28

---

## Ø¨Ø®Ø´ 3.3: Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ú†Ø±Ø®Ù‡â€ŒØ§ÛŒ (Cyclical Patterns)

### Ù…Ø´Ú©Ù„Ø§Øª Ø´Ù†Ø§Ø³Ø§ÛŒÛŒâ€ŒØ´Ø¯Ù‡

#### 1. Ù…Ø­Ø¯ÙˆØ¯ÛŒØª FFT Ø¯Ø± ØªØ­Ù„ÛŒÙ„ ØºÛŒØ±-Ø§ÛŒØ³ØªØ§ (Non-Stationary Data)
**Ø´Ø¯Øª:** Ù…ØªÙˆØ³Ø· | **ØªØ£Ø«ÛŒØ± Ø¨Ù‡Ø¨ÙˆØ¯:** +25%

**ØªÙˆØ¶ÛŒØ­ Ù…Ø´Ú©Ù„:**
```python
# Ú©Ø¯ ÙØ¹Ù„ÛŒ (signal_generator.py:2781-2785)
close_fft = fft.rfft(detrended)
fft_freqs = fft.rfftfreq(len(detrended))
close_fft_mag = np.abs(close_fft)
```

FFT ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø§ÛŒØ³ØªØ§ (stationary) Ø§Ø³ØªØŒ Ø§Ù…Ø§ Ù‚ÛŒÙ…Øªâ€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø± ØºÛŒØ±-Ø§ÛŒØ³ØªØ§ Ù‡Ø³ØªÙ†Ø¯. Ø§ÛŒÙ† Ø¨Ø§Ø¹Ø« Ù…ÛŒâ€ŒØ´ÙˆØ¯:
- Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆÙ‚Øª Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø¯Ø§Ø¦Ù…ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´ÙˆÙ†Ø¯
- ØªØºÛŒÛŒØ±Ø§Øª ÙØ±Ú©Ø§Ù†Ø³ Ø¯Ø± Ø·ÙˆÙ„ Ø²Ù…Ø§Ù† Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´ÙˆÙ†Ø¯
- Ø¯Ù‚Øª Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¯Ø± Ø¨Ø§Ø²Ø§Ø±Ù‡Ø§ÛŒ Ù¾Ø±Ù†ÙˆØ³Ø§Ù† Ú©Ø§Ù‡Ø´ ÛŒØ§Ø¨Ø¯

**Ø±Ø§Ù‡â€ŒØ­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**
Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Wavelet Transform (Ù…Ø§Ù†Ù†Ø¯ Continuous Wavelet Transform ÛŒØ§ Empirical Mode Decomposition) Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø²Ù…Ø§Ù†-ÙØ±Ú©Ø§Ù†Ø³ Ø¨Ù‡ØªØ±:

```python
import pywt

def detect_cyclical_patterns_wavelet(self, candles: List[Dict], period: str = '1h') -> Dict:
    """ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ú†Ø±Ø®Ù‡â€ŒØ§ÛŒ Ø¨Ø§ Wavelet Transform"""
    closes = np.array([c['close'] for c in candles])

    # 1. Continuous Wavelet Transform
    scales = np.arange(1, min(128, len(closes) // 4))
    coefficients, frequencies = pywt.cwt(closes, scales, 'morl')

    # 2. ÛŒØ§ÙØªÙ† Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚ÙˆÛŒ Ø¯Ø± Ø²Ù…Ø§Ù† Ø§Ø®ÛŒØ± (50 Ú©Ù†Ø¯Ù„ Ø¢Ø®Ø±)
    power = np.abs(coefficients[:, -50:]) ** 2
    recent_power = np.mean(power, axis=1)

    # 3. Ø§Ù†ØªØ®Ø§Ø¨ Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ù†Ø§Ø¯Ø§Ø±
    threshold = np.mean(recent_power) + 1.5 * np.std(recent_power)
    significant_scales = scales[recent_power > threshold]

    # 4. Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¯ÙˆØ±Ù‡ Ùˆ ÙØ§Ø² Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú†Ø±Ø®Ù‡
    cycles = []
    for scale in significant_scales[:5]:  # 5 Ú†Ø±Ø®Ù‡ Ù‚ÙˆÛŒ
        # Ø¯ÙˆØ±Ù‡ = scale Ã— sampling_period / center_frequency
        period = int(scale * 1.0 / 0.25)  # Ø¨Ø±Ø§ÛŒ Ù…ÙˆØ¬Ú© Morlet

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙØ§Ø² Ø§Ø² Ø¶Ø±Ø§ÛŒØ¨
        scale_idx = np.where(scales == scale)[0][0]
        phase = np.angle(coefficients[scale_idx, -1])

        # Ù‚Ø¯Ø±Øª Ú†Ø±Ø®Ù‡ Ø¯Ø± Ø²Ù…Ø§Ù† Ø§Ø®ÛŒØ±
        strength = recent_power[scale_idx] / np.max(recent_power)

        cycles.append({
            'period': period,
            'phase': phase,
            'strength': strength
        })

    # 5. Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§ ØªØ±Ú©ÛŒØ¨ Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒâ€ŒØ´Ø¯Ù‡
    forecast = self._generate_wavelet_forecast(closes, cycles, 20)

    return {
        'cycles': cycles,
        'forecast': forecast,
        'method': 'wavelet'
    }
```

**Ù…Ø²Ø§ÛŒØ§:**
- Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªØºÛŒØ± Ø¯Ø± Ø·ÙˆÙ„ Ø²Ù…Ø§Ù†
- Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§ØªØ± Ø¯Ø± Ø¨Ø§Ø²Ø§Ø±Ù‡Ø§ÛŒ Ù¾Ø±Ù†ÙˆØ³Ø§Ù† (+25%)
- ØªØ´Ø®ÛŒØµ Ø²Ù…Ø§Ù† Ø´Ø±ÙˆØ¹ Ùˆ Ù¾Ø§ÛŒØ§Ù† Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§

---

#### 2. Ø¹Ø¯Ù… Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Market Structure
**Ø´Ø¯Øª:** Ù…ØªÙˆØ³Ø· | **ØªØ£Ø«ÛŒØ± Ø¨Ù‡Ø¨ÙˆØ¯:** +20%

**ØªÙˆØ¶ÛŒØ­ Ù…Ø´Ú©Ù„:**
```python
# Ú©Ø¯ ÙØ¹Ù„ÛŒ (signal_generator.py:2795-2805)
significant_freq_indices = np.where(close_fft_mag > threshold)[0]
for idx in significant_freq_indices:
    period = int(1 / fft_freqs[idx])
    # Ù‡ÛŒÚ† Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø¨Ø§ Ø³Ø§Ø®ØªØ§Ø± Ø¨Ø§Ø²Ø§Ø± Ø§Ù†Ø¬Ø§Ù… Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯
```

Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒâ€ŒØ´Ø¯Ù‡ Ù…Ù…Ú©Ù† Ø§Ø³Øª:
- Ø¨Ø§ Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø§Ø²Ø§Ø± (Ù…Ø§Ù†Ù†Ø¯ Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ø±ÙˆØ²Ø§Ù†Ù‡ØŒ Ù‡ÙØªÚ¯ÛŒ) Ù‡Ù…Ø®ÙˆØ§Ù†ÛŒ Ù†Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯
- Ø¯Ø± Ù†Ù‚Ø§Ø· Ú©Ù„ÛŒØ¯ÛŒ (SRØŒ Pivot Points) Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ù†Ø´ÙˆÙ†Ø¯
- Ø¨Ø§ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Price Action ØªØ·Ø§Ø¨Ù‚ Ù†Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯

**Ø±Ø§Ù‡â€ŒØ­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**
Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ø³Ø§Ø®ØªØ§Ø± Ø¨Ø§Ø²Ø§Ø± Ùˆ ØªØ·Ø§Ø¨Ù‚ Ø¨Ø§ Ø³Ø·ÙˆØ­ Ú©Ù„ÛŒØ¯ÛŒ:

```python
def validate_cycles_with_market_structure(self, candles: List[Dict], cycles: List[Dict]) -> List[Dict]:
    """Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ø³Ø§Ø®ØªØ§Ø± Ø¨Ø§Ø²Ø§Ø±"""
    closes = np.array([c['close'] for c in candles])
    validated_cycles = []

    for cycle in cycles:
        period = cycle['period']
        validation_score = 0.0

        # 1. Ø¨Ø±Ø±Ø³ÛŒ Ù‡Ù…Ø®ÙˆØ§Ù†ÛŒ Ø¨Ø§ Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ù†Ø§Ø®ØªÙ‡â€ŒØ´Ø¯Ù‡ Ø¨Ø§Ø²Ø§Ø±
        known_cycles = {
            'daily': 24,      # 24 Ø³Ø§Ø¹Øª
            'weekly': 168,    # 7 Ø±ÙˆØ²
            'monthly': 720    # 30 Ø±ÙˆØ²
        }

        for cycle_name, cycle_period in known_cycles.items():
            tolerance = cycle_period * 0.1  # ØªÙ„Ø±Ø§Ù†Ø³ 10%
            if abs(period - cycle_period) <= tolerance:
                validation_score += 0.3
                break

        # 2. Ø¨Ø±Ø±Ø³ÛŒ ØªØ·Ø§Ø¨Ù‚ Ø¨Ø§ SR Levels
        sr_levels = self.detect_support_resistance(candles)
        cycle_forecasts = []

        for i in range(len(closes) - period, len(closes), period):
            if i >= 0 and i < len(closes):
                cycle_forecasts.append(closes[i])

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØ·Ø§Ø¨Ù‚ Ø¨Ø§ SR
        sr_match_count = 0
        for forecast_price in cycle_forecasts:
            for sr in sr_levels[:5]:  # Ø¨Ø±Ø±Ø³ÛŒ 5 SR Ù‚ÙˆÛŒ
                if abs(forecast_price - sr['price']) / sr['price'] < 0.005:  # ØªÙ„Ø±Ø§Ù†Ø³ 0.5%
                    sr_match_count += 1

        if len(cycle_forecasts) > 0:
            sr_match_ratio = sr_match_count / len(cycle_forecasts)
            validation_score += sr_match_ratio * 0.4

        # 3. Ø¨Ø±Ø±Ø³ÛŒ ØªØ·Ø§Ø¨Ù‚ Ø¨Ø§ Swing Highs/Lows
        peaks, valleys = self.find_peaks_and_valleys(closes, distance=5)
        all_pivots = sorted(peaks + valleys)

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒÙ† pivots
        pivot_distances = [all_pivots[i+1] - all_pivots[i]
                          for i in range(len(all_pivots)-1)]

        if len(pivot_distances) > 0:
            avg_pivot_distance = np.mean(pivot_distances)
            if abs(period - avg_pivot_distance) / avg_pivot_distance < 0.2:  # ØªÙ„Ø±Ø§Ù†Ø³ 20%
                validation_score += 0.3

        # 4. Ø°Ø®ÛŒØ±Ù‡ Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø± (Ø§Ù…ØªÛŒØ§Ø² > 0.4)
        if validation_score >= 0.4:
            cycle['validation_score'] = validation_score
            validated_cycles.append(cycle)

    # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù‚Ø¯Ø±Øª Ã— Ø§Ø¹ØªØ¨Ø§Ø±
    validated_cycles.sort(
        key=lambda c: c['strength'] * c.get('validation_score', 0),
        reverse=True
    )

    return validated_cycles
```

**Ù…Ø²Ø§ÛŒØ§:**
- Ú©Ø§Ù‡Ø´ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ø¯Ø±Ø³Øª (+20%)
- Ù‡Ù…Ø®ÙˆØ§Ù†ÛŒ Ø¨Ø§ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø§Ø²Ø§Ø±
- Ø§ÙØ²Ø§ÛŒØ´ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø¨Ù‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§

---

#### 3. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÛŒÚ© Ø±ÙˆØ´ Detrending (Linear)
**Ø´Ø¯Øª:** Ø³Ø§Ø¯Ù‡ | **ØªØ£Ø«ÛŒØ± Ø¨Ù‡Ø¨ÙˆØ¯:** +12%

**ØªÙˆØ¶ÛŒØ­ Ù…Ø´Ú©Ù„:**
```python
# Ú©Ø¯ ÙØ¹Ù„ÛŒ (signal_generator.py:2773-2777)
x = np.arange(len(closes))
trend_coeffs = np.polyfit(x, closes, 1)  # ÙÙ‚Ø· Linear
trend = np.polyval(trend_coeffs, x)
detrended = closes - trend
```

Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙÙ‚Ø· Linear Detrending Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø¯Ø§Ø±Ø¯:
- Ø±ÙˆÙ†Ø¯Ù‡Ø§ÛŒ ØºÛŒØ±Ø®Ø·ÛŒ (Ù¾Ø§Ø±Ø§Ø¨ÙˆÙ„ÛŒÚ©ØŒ Ù†Ù…Ø§ÛŒÛŒ) Ø¨Ù‡ Ø¯Ø±Ø³ØªÛŒ Ø­Ø°Ù Ù†Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
- Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù„Ù†Ø¯Ù…Ø¯Øª Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ØªØ±Ù†Ø¯ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
- Ø¯Ø± Ø¨Ø§Ø²Ø§Ø±Ù‡Ø§ÛŒ Ø±Ù†Ø¬ (sideways) ØªØ±Ù†Ø¯ Ø§Ø´ØªØ¨Ø§Ù‡ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯

**Ø±Ø§Ù‡â€ŒØ­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**
Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Detrending Ø¨Ø³ØªÙ‡ Ø¨Ù‡ Ø±Ú˜ÛŒÙ… Ø¨Ø§Ø²Ø§Ø±:

```python
def adaptive_detrending(self, closes: np.ndarray) -> Tuple[np.ndarray, str]:
    """Detrending ØªØ·Ø¨ÛŒÙ‚ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø±Ú˜ÛŒÙ… Ø¨Ø§Ø²Ø§Ø±"""
    x = np.arange(len(closes))

    # 1. ØªØ³Øª Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
    methods = {}

    # Linear Detrending
    linear_coeffs = np.polyfit(x, closes, 1)
    linear_trend = np.polyval(linear_coeffs, x)
    linear_detrended = closes - linear_trend
    methods['linear'] = (linear_detrended, np.std(linear_detrended))

    # Polynomial Detrending (Ø¯Ø±Ø¬Ù‡ 2)
    poly_coeffs = np.polyfit(x, closes, 2)
    poly_trend = np.polyval(poly_coeffs, x)
    poly_detrended = closes - poly_trend
    methods['polynomial'] = (poly_detrended, np.std(poly_detrended))

    # Moving Average Detrending (EMA 50)
    ema_50 = self._calculate_ema(closes, 50)
    ma_detrended = closes - ema_50
    methods['moving_average'] = (ma_detrended, np.std(ma_detrended))

    # HP Filter (Hodrick-Prescott)
    # Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø¹ØªÛŒ: lambda = 1600
    hp_cycle, hp_trend = self._hp_filter(closes, lamb=1600)
    methods['hp_filter'] = (hp_cycle, np.std(hp_cycle))

    # 2. Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† Ø±ÙˆØ´ (Ú©Ù…ØªØ±ÛŒÙ† Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø± = Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ø­Ø°Ù ØªØ±Ù†Ø¯)
    best_method = max(methods.items(), key=lambda x: x[1][1])

    return best_method[1][0], best_method[0]

def _hp_filter(self, x: np.ndarray, lamb: float = 1600) -> Tuple[np.ndarray, np.ndarray]:
    """Hodrick-Prescott Filter Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ±Ù†Ø¯"""
    from scipy import sparse
    from scipy.sparse.linalg import spsolve

    n = len(x)
    I = sparse.eye(n)
    D = sparse.diags([1, -2, 1], [0, 1, 2], shape=(n-2, n))

    trend = spsolve(I + lamb * D.T @ D, x)
    cycle = x - trend

    return cycle, trend
```

**Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ú©Ø¯ Ø§ØµÙ„ÛŒ:**
```python
# Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ø¯Ø± detect_cyclical_patterns
detrended, detrend_method = self.adaptive_detrending(closes)

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ output
return {
    'cycles': cycles,
    'forecast': forecast,
    'detrend_method': detrend_method  # 'linear', 'polynomial', 'moving_average', ÛŒØ§ 'hp_filter'
}
```

**Ù…Ø²Ø§ÛŒØ§:**
- Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§ØªØ± Ø¯Ø± Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§ (+12%)
- Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ Ø±Ú˜ÛŒÙ…â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¨Ø§Ø²Ø§Ø±
- Ú©Ø§Ù‡Ø´ Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø°Ø¨

---

#### 4. Ø¹Ø¯Ù… Ø¨Ù‡Ø±Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡ Ø§Ø² Phase Information
**Ø´Ø¯Øª:** Ù…ØªÙˆØ³Ø· | **ØªØ£Ø«ÛŒØ± Ø¨Ù‡Ø¨ÙˆØ¯:** +18%

**ØªÙˆØ¶ÛŒØ­ Ù…Ø´Ú©Ù„:**
```python
# Ú©Ø¯ ÙØ¹Ù„ÛŒ (signal_generator.py:2807-2815)
phase = np.angle(close_fft[idx])
cycles.append({
    'period': period,
    'amplitude': amplitude,
    'phase': phase  # ÙÙ‚Ø· Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯
})
```

ÙØ§Ø² (phase) Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø­ÛŒØ§ØªÛŒ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ú†Ø±Ø®Ù‡ Ø±Ø§ Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ø§Ù…Ø§:
- Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯
- Ø¨Ø±Ø§ÛŒ ØªØ¹ÛŒÛŒÙ† Ø²Ù…Ø§Ù† Ø¨Ù‡ÛŒÙ†Ù‡ ÙˆØ±ÙˆØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯
- Ø¯Ø± ØªØ±Ú©ÛŒØ¨ Ø¨Ø§ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø± Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ‡ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯

**Ø±Ø§Ù‡â€ŒØ­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**
Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙØ§Ø² Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø²Ù…Ø§Ù† ÙˆØ±ÙˆØ¯ Ùˆ Ø§ÙØ²Ø§ÛŒØ´ Ø§Ù…ØªÛŒØ§Ø²:

```python
def calculate_phase_based_timing(self, cycles: List[Dict], current_index: int) -> Dict:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ ÙØ§Ø² Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§"""

    # 1. ØªØ¹ÛŒÛŒÙ† Ù…ÙˆÙ‚Ø¹ÛŒØª ÙØ¹Ù„ÛŒ Ø¯Ø± Ú†Ø±Ø®Ù‡ (0 ØªØ§ 2Ï€)
    cycle_positions = []
    for cycle in cycles:
        period = cycle['period']
        phase = cycle['phase']

        # Ù…ÙˆÙ‚Ø¹ÛŒØª ÙØ¹Ù„ÛŒ = (current_index % period) / period * 2Ï€ + phase
        current_phase = ((current_index % period) / period * 2 * np.pi + phase) % (2 * np.pi)

        cycle_positions.append({
            'period': period,
            'current_phase': current_phase,
            'strength': cycle['strength']
        })

    # 2. ØªØ¹ÛŒÛŒÙ† Ø¨Ù‡ØªØ±ÛŒÙ† Ø²Ù…Ø§Ù†â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø®Ø±ÛŒØ¯ Ùˆ ÙØ±ÙˆØ´
    # - Ø®Ø±ÛŒØ¯ Ø¨Ù‡ÛŒÙ†Ù‡: Ù†Ø²Ø¯ÛŒÚ© Ø¨Ù‡ Ú©Ù Ú†Ø±Ø®Ù‡ (phase ~ 3Ï€/2)
    # - ÙØ±ÙˆØ´ Ø¨Ù‡ÛŒÙ†Ù‡: Ù†Ø²Ø¯ÛŒÚ© Ø¨Ù‡ Ø³Ù‚Ù Ú†Ø±Ø®Ù‡ (phase ~ Ï€/2)

    buy_score = 0.0
    sell_score = 0.0

    for pos in cycle_positions:
        phase = pos['current_phase']
        strength = pos['strength']

        # ÙØ§ØµÙ„Ù‡ Ø§Ø² Ú©Ù Ú†Ø±Ø®Ù‡ (3Ï€/2 = 4.71)
        buy_distance = min(
            abs(phase - 4.71),
            abs(phase - 4.71 + 2 * np.pi),
            abs(phase - 4.71 - 2 * np.pi)
        )

        # ÙØ§ØµÙ„Ù‡ Ø§Ø² Ø³Ù‚Ù Ú†Ø±Ø®Ù‡ (Ï€/2 = 1.57)
        sell_distance = min(
            abs(phase - 1.57),
            abs(phase - 1.57 + 2 * np.pi),
            abs(phase - 1.57 - 2 * np.pi)
        )

        # ØªØ¨Ø¯ÛŒÙ„ ÙØ§ØµÙ„Ù‡ Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² (0 ØªØ§ 1)
        # ÙØ§ØµÙ„Ù‡ Ú©Ù…ØªØ± = Ø§Ù…ØªÛŒØ§Ø² Ø¨ÛŒØ´ØªØ±
        buy_phase_score = max(0, 1 - buy_distance / np.pi) * strength
        sell_phase_score = max(0, 1 - sell_distance / np.pi) * strength

        buy_score += buy_phase_score
        sell_score += sell_phase_score

    # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ (0 ØªØ§ 1)
    total_strength = sum(c['strength'] for c in cycles)
    if total_strength > 0:
        buy_score /= total_strength
        sell_score /= total_strength

    # 3. ØªØ¹ÛŒÛŒÙ† Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù…ØªÛŒØ§Ø²Ù‡Ø§
    if buy_score > 0.7:
        signal = 'buy'
        phase_multiplier = 1.0 + (buy_score - 0.7) * 0.5  # ØªØ§ 1.15Ã—
    elif sell_score > 0.7:
        signal = 'sell'
        phase_multiplier = 1.0 + (sell_score - 0.7) * 0.5
    else:
        signal = 'neutral'
        phase_multiplier = 0.8  # Ú©Ø§Ù‡Ø´ Ø§Ù…ØªÛŒØ§Ø² Ø¯Ø± Ø²Ù…Ø§Ù† Ù†Ø§Ù…Ù†Ø§Ø³Ø¨

    # 4. Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ù†Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡ ØªØ§ Ø¨Ù‡ØªØ±ÛŒÙ† Ø²Ù…Ø§Ù†
    if signal == 'neutral':
        # ÛŒØ§ÙØªÙ† Ù†Ø²Ø¯ÛŒÚ©â€ŒØªØ±ÛŒÙ† Ú©Ù/Ø³Ù‚Ù Ú†Ø±Ø®Ù‡
        candles_to_buy = []
        candles_to_sell = []

        for pos in cycle_positions:
            phase = pos['current_phase']
            period = pos['period']

            # Ú©Ù†Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡ ØªØ§ Ú©Ù (3Ï€/2)
            if phase < 4.71:
                candles_to_next_low = int((4.71 - phase) / (2 * np.pi) * period)
            else:
                candles_to_next_low = int((4.71 + 2*np.pi - phase) / (2 * np.pi) * period)

            candles_to_buy.append(candles_to_next_low)

            # Ú©Ù†Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡ ØªØ§ Ø³Ù‚Ù (Ï€/2)
            if phase < 1.57:
                candles_to_next_high = int((1.57 - phase) / (2 * np.pi) * period)
            else:
                candles_to_next_high = int((1.57 + 2*np.pi - phase) / (2 * np.pi) * period)

            candles_to_sell.append(candles_to_next_high)

        next_buy_in = min(candles_to_buy) if candles_to_buy else None
        next_sell_in = min(candles_to_sell) if candles_to_sell else None
    else:
        next_buy_in = 0 if signal == 'buy' else None
        next_sell_in = 0 if signal == 'sell' else None

    return {
        'signal': signal,
        'phase_multiplier': phase_multiplier,
        'buy_score': buy_score,
        'sell_score': sell_score,
        'next_buy_in_candles': next_buy_in,
        'next_sell_in_candles': next_sell_in
    }
```

**Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø²:**
```python
def calculate_cyclical_score(self, pattern_data: Dict) -> float:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² phase information"""
    cycles = pattern_data.get('cycles', [])

    if not cycles:
        return 0.0

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ù¾Ø§ÛŒÙ‡
    prediction_clarity = pattern_data.get('prediction_clarity', 0.5)
    cycles_strength = sum(c['strength'] for c in cycles[:5])
    base_score = (prediction_clarity * 1.5 + cycles_strength * 0.5) / 2

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ phase timing
    current_index = len(pattern_data.get('candles', []))
    timing = self.calculate_phase_based_timing(cycles, current_index)

    # Ø§Ø¹Ù…Ø§Ù„ Ø¶Ø±ÛŒØ¨ ÙØ§Ø²
    final_score = base_score * timing['phase_multiplier']

    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª timing Ø¨Ù‡ pattern_data
    pattern_data['timing'] = timing

    return final_score
```

**Ù…Ø²Ø§ÛŒØ§:**
- Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‚Øª Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ ÙˆØ±ÙˆØ¯/Ø®Ø±ÙˆØ¬ (+18%)
- Ú©Ø§Ù‡Ø´ ÙˆØ±ÙˆØ¯ Ø¯Ø± Ø²Ù…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ù…Ù†Ø§Ø³Ø¨ Ú†Ø±Ø®Ù‡
- Ø§Ø±Ø§Ø¦Ù‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ø²Ù…Ø§Ù† ÙˆØ±ÙˆØ¯ Ø¨Ø¹Ø¯ÛŒ

---

#### 5. Ø¹Ø¯Ù… Ù…Ø­Ø§Ø³Ø¨Ù‡ Cycle Strength Decay
**Ø´Ø¯Øª:** Ø³Ø§Ø¯Ù‡ | **ØªØ£Ø«ÛŒØ± Ø¨Ù‡Ø¨ÙˆØ¯:** +10%

**ØªÙˆØ¶ÛŒØ­ Ù…Ø´Ú©Ù„:**
```python
# Ú©Ø¯ ÙØ¹Ù„ÛŒ (signal_generator.py:2820-2835)
# Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø¯ÙˆÙ† Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ† Ø¶Ø¹ÛŒÙ Ø´Ø¯Ù† Ú†Ø±Ø®Ù‡ Ø¯Ø± Ø¢ÛŒÙ†Ø¯Ù‡
for t in range(len(closes), len(closes) + forecast_length):
    value = trend_at_t
    for cycle in cycles[:5]:
        amplitude = cycle['amplitude']
        period = cycle['period']
        phase = cycle['phase']

        # ÙØ±Ø¶: Ø¯Ø§Ù…Ù†Ù‡ Ø«Ø§Ø¨Øª Ù…ÛŒâ€ŒÙ…Ø§Ù†Ø¯
        cycle_value = amplitude * np.cos(2 * np.pi * t / period + phase)
        value += cycle_value
```

Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø¯Ø± Ø·ÙˆÙ„ Ø²Ù…Ø§Ù† Ø¶Ø¹ÛŒÙ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ (Damping):
- Ù‚Ø¯Ø±Øª Ú†Ø±Ø®Ù‡ Ú©Ø§Ù‡Ø´ Ù…ÛŒâ€ŒÛŒØ§Ø¨Ø¯
- Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨Ù„Ù†Ø¯Ù…Ø¯Øª Ø¨ÛŒØ´ Ø§Ø² Ø­Ø¯ Ø®ÙˆØ´â€ŒØ¨ÛŒÙ†Ø§Ù†Ù‡ Ù‡Ø³ØªÙ†Ø¯
- Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¨Ù‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§ÛŒØ¯ Ø¨Ø§ ÙØ§ØµÙ„Ù‡ Ú©Ø§Ù‡Ø´ ÛŒØ§Ø¨Ø¯

**Ø±Ø§Ù‡â€ŒØ­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**
Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Exponential Decay Ø¨Ù‡ Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§:

```python
def generate_forecast_with_decay(self, closes: np.ndarray, cycles: List[Dict],
                                 forecast_length: int = 20, decay_rate: float = 0.05) -> List[float]:
    """ØªÙˆÙ„ÛŒØ¯ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§ Cycle Strength Decay"""

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØ±Ù†Ø¯
    x = np.arange(len(closes))
    trend_coeffs = np.polyfit(x, closes, 1)

    forecast = []
    for t in range(len(closes), len(closes) + forecast_length):
        trend_at_t = np.polyval(trend_coeffs, t)
        value = trend_at_t

        # ÙØ§ØµÙ„Ù‡ Ø§Ø² Ø¢Ø®Ø±ÛŒÙ† Ú©Ù†Ø¯Ù„ ÙˆØ§Ù‚Ø¹ÛŒ
        distance = t - len(closes) + 1

        for cycle in cycles[:5]:
            amplitude = cycle['amplitude']
            period = cycle['period']
            phase = cycle['phase']

            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Decay Factor Ø¨Ø± Ø§Ø³Ø§Ø³:
            # 1. ÙØ§ØµÙ„Ù‡ Ø²Ù…Ø§Ù†ÛŒ (Ù‡Ø±Ú†Ù‡ Ø¯ÙˆØ±ØªØ±ØŒ Ø¶Ø¹ÛŒÙâ€ŒØªØ±)
            # 2. Ø¯ÙˆØ±Ù‡ Ú†Ø±Ø®Ù‡ (Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ú©ÙˆØªØ§Ù‡â€ŒØªØ± Ø³Ø±ÛŒØ¹â€ŒØªØ± Ø¶Ø¹ÛŒÙ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯)

            # Decay = e^(-decay_rate Ã— distance / period)
            decay_factor = np.exp(-decay_rate * distance / period)

            # Ø§Ø¹Ù…Ø§Ù„ Decay Ø¨Ù‡ Ø¯Ø§Ù…Ù†Ù‡
            damped_amplitude = amplitude * decay_factor

            cycle_value = damped_amplitude * np.cos(2 * np.pi * t / period + phase)
            value += cycle_value

        forecast.append(value)

    return forecast
```

**Ù…Ø­Ø§Ø³Ø¨Ù‡ Confidence Intervals:**
```python
def calculate_forecast_confidence(self, forecast: List[float],
                                  cycles: List[Dict], decay_rate: float = 0.05) -> List[Dict]:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙÙˆØ§ØµÙ„ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ"""

    confidence_intervals = []

    for i, value in enumerate(forecast):
        distance = i + 1

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø± Ø¨Ø± Ø§Ø³Ø§Ø³:
        # - Decay Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§
        # - ÙØ§ØµÙ„Ù‡ Ø²Ù…Ø§Ù†ÛŒ

        # Uncertainty Ø§ÙØ²Ø§ÛŒØ´ Ù…ÛŒâ€ŒÛŒØ§Ø¨Ø¯ Ø¨Ø§ ÙØ§ØµÙ„Ù‡
        base_uncertainty = 0.01  # 1% Ø§Ù†Ø­Ø±Ø§Ù Ù¾Ø§ÛŒÙ‡

        # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ÙˆØ²Ù†ÛŒ Decay Ø§Ø² ØªÙ…Ø§Ù… Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§
        total_strength = sum(c['strength'] for c in cycles)
        weighted_decay = 0.0

        for cycle in cycles:
            period = cycle['period']
            strength = cycle['strength']

            decay = 1 - np.exp(-decay_rate * distance / period)
            weighted_decay += decay * (strength / total_strength)

        # Uncertainty = base Ã— (1 + weighted_decay Ã— 10)
        # Ù…Ø«Ù„Ø§Ù‹: decay=0.5 â†’ uncertainty = 1% Ã— (1 + 5) = 6%
        uncertainty = base_uncertainty * (1 + weighted_decay * 10)

        confidence_intervals.append({
            'value': value,
            'lower_bound': value * (1 - uncertainty),
            'upper_bound': value * (1 + uncertainty),
            'confidence': max(0, 1 - weighted_decay)  # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ú©Ø§Ù‡Ø´ Ù…ÛŒâ€ŒÛŒØ§Ø¨Ø¯
        })

    return confidence_intervals
```

**Ù…Ø²Ø§ÛŒØ§:**
- Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹â€ŒØ¨ÛŒÙ†Ø§Ù†Ù‡â€ŒØªØ± (+10% Ø¯Ù‚Øª)
- ÙÙˆØ§ØµÙ„ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ø±ÛŒØ³Ú©
- Ú©Ø§Ù‡Ø´ Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¨Ù‡ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¶Ø¹ÛŒÙ

---

#### 6. Ù…Ø­Ø¯ÙˆØ¯ÛŒØª ØªØ¹Ø¯Ø§Ø¯ Ú©Ù†Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² (200)
**Ø´Ø¯Øª:** Ø³Ø§Ø¯Ù‡ | **ØªØ£Ø«ÛŒØ± Ø¨Ù‡Ø¨ÙˆØ¯:** +8%

**ØªÙˆØ¶ÛŒØ­ Ù…Ø´Ú©Ù„:**
```python
# Ú©Ø¯ ÙØ¹Ù„ÛŒ (signal_generator.py:2768)
if len(candles) < 200:
    return {}
```

Ù†ÛŒØ§Ø² Ø¨Ù‡ 200 Ú©Ù†Ø¯Ù„ Ù…Ø­Ø¯ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ø±Ø§ Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯:
- Ø¯Ø± ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ù„Ø§ØªØ± (4H, 1D) Ø¯Ø§Ø¯Ù‡ Ú©Ø§ÙÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª
- Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ú©ÙˆØªØ§Ù‡â€ŒÙ…Ø¯Øª Ù‚Ø§Ø¨Ù„ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†ÛŒØ³ØªÙ†Ø¯
- Ø¯Ø± Ø§Ø±Ø²Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ ÛŒØ§ Ø¨Ø§Ø²Ø§Ø±Ù‡Ø§ÛŒ Ù†ÙˆØ¸Ù‡ÙˆØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯

**Ø±Ø§Ù‡â€ŒØ­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**
Ø±ÙˆØ´ ØªØ·Ø¨ÛŒÙ‚ÛŒ Ø¨Ø§ Ø­Ø¯Ø§Ù‚Ù„ 50 Ú©Ù†Ø¯Ù„:

```python
def detect_cyclical_patterns_adaptive(self, candles: List[Dict], period: str = '1h') -> Dict:
    """ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ú†Ø±Ø®Ù‡â€ŒØ§ÛŒ Ø¨Ø§ Ø­Ø¯Ø§Ù‚Ù„ Ø¯Ø§Ø¯Ù‡ ØªØ·Ø¨ÛŒÙ‚ÛŒ"""
    closes = np.array([c['close'] for c in candles])
    n_candles = len(closes)

    # Ø­Ø¯Ø§Ù‚Ù„ 50 Ú©Ù†Ø¯Ù„
    if n_candles < 50:
        return {}

    # 1. ØªØ¹ÛŒÛŒÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù†Ø¯Ù„
    if n_candles >= 200:
        # Ø­Ø§Ù„Øª Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯: FFT Ú©Ø§Ù…Ù„
        max_cycles = 5
        min_period = 10
        max_period = n_candles // 3
    elif n_candles >= 100:
        # Ø­Ø§Ù„Øª Ù…ØªÙˆØ³Ø·: ÙÙˆÚ©ÙˆØ³ Ø¨Ø± Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ú©ÙˆØªØ§Ù‡â€ŒØªØ±
        max_cycles = 3
        min_period = 5
        max_period = n_candles // 2
    else:  # 50-99 Ú©Ù†Ø¯Ù„
        # Ø­Ø§Ù„Øª Ù…Ø­Ø¯ÙˆØ¯: ÙÙ‚Ø· Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø³ÛŒØ§Ø± Ú©ÙˆØªØ§Ù‡
        max_cycles = 2
        min_period = 3
        max_period = n_candles // 2

    # 2. Detrending ØªØ·Ø¨ÛŒÙ‚ÛŒ
    if n_candles >= 150:
        detrended, method = self.adaptive_detrending(closes)
    else:
        # Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡ Ú©Ù…: ÙÙ‚Ø· Ø­Ø°Ù Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†
        detrended = closes - np.mean(closes)
        method = 'mean_removal'

    # 3. FFT Ø¨Ø§ Ù…Ø­Ø¯ÙˆØ¯Ù‡ ÙØ±Ú©Ø§Ù†Ø³ÛŒ ØªØ·Ø¨ÛŒÙ‚ÛŒ
    close_fft = fft.rfft(detrended)
    fft_freqs = fft.rfftfreq(n_candles)
    close_fft_mag = np.abs(close_fft)

    # ÙÛŒÙ„ØªØ± ÙØ±Ú©Ø§Ù†Ø³ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ø¯ÙˆØ±Ù‡
    valid_indices = []
    for idx, freq in enumerate(fft_freqs):
        if freq == 0:
            continue
        period = 1 / freq
        if min_period <= period <= max_period:
            valid_indices.append(idx)

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§
    if len(valid_indices) == 0:
        return {}

    valid_mags = close_fft_mag[valid_indices]
    threshold = np.mean(valid_mags) + 0.5 * np.std(valid_mags)  # Ú©Ø§Ù‡Ø´ threshold

    significant_indices = [valid_indices[i] for i, mag in enumerate(valid_mags) if mag > threshold]

    # ... Ø§Ø¯Ø§Ù…Ù‡ Ù…Ù†Ø·Ù‚ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú†Ø±Ø®Ù‡

    cycles = []
    for idx in sorted(significant_indices, key=lambda i: close_fft_mag[i], reverse=True)[:max_cycles]:
        period = int(1 / fft_freqs[idx])
        amplitude = close_fft_mag[idx] / n_candles
        phase = np.angle(close_fft[idx])
        strength = close_fft_mag[idx] / np.max(close_fft_mag)

        cycles.append({
            'period': period,
            'amplitude': amplitude,
            'phase': phase,
            'strength': strength
        })

    # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§ Ø·ÙˆÙ„ ØªØ·Ø¨ÛŒÙ‚ÛŒ
    forecast_length = min(20, n_candles // 10)  # Ø­Ø¯Ø§Ú©Ø«Ø± 10% Ø·ÙˆÙ„ Ø¯Ø§Ø¯Ù‡
    forecast = self.generate_forecast_with_decay(closes, cycles, forecast_length)

    return {
        'cycles': cycles,
        'forecast': forecast,
        'method': method,
        'data_sufficiency': 'full' if n_candles >= 200 else 'limited'
    }
```

**Ù…Ø²Ø§ÛŒØ§:**
- Ù‚Ø§Ø¨Ù„ÛŒØª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ù„Ø§ØªØ± (+8%)
- Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú†Ø±Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ú©ÙˆØªØ§Ù‡â€ŒÙ…Ø¯Øª
- Ø§Ù†Ø¹Ø·Ø§Ùâ€ŒÙ¾Ø°ÛŒØ±ÛŒ Ø¨ÛŒØ´ØªØ±

---

### Ø¬Ø¯ÙˆÙ„ Ø®Ù„Ø§ØµÙ‡ Ø¨Ù‡Ø¨ÙˆØ¯Ù‡Ø§

| # | Ù…Ø´Ú©Ù„ | ØªØ£Ø«ÛŒØ± ØªØ®Ù…ÛŒÙ†ÛŒ | Ø³Ø®ØªÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ |
|---|------|-------|---------|
| 1 | Ù…Ø­Ø¯ÙˆØ¯ÛŒØª FFT (Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Wavelet) | **+25%** | Ù¾ÛŒÚ†ÛŒØ¯Ù‡ |
| 2 | Ø¹Ø¯Ù… Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø¨Ø§ Market Structure | **+20%** | Ù…ØªÙˆØ³Ø· |
| 3 | Ø±ÙˆØ´ ÛŒÚ©Ø³Ø§Ù† Detrending | **+12%** | Ø³Ø§Ø¯Ù‡ |
| 4 | Ø¹Ø¯Ù… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡ Ø§Ø² Phase | **+18%** | Ù…ØªÙˆØ³Ø· |
| 5 | Ø¹Ø¯Ù… Ù…Ø­Ø§Ø³Ø¨Ù‡ Decay | **+10%** | Ø³Ø§Ø¯Ù‡ |
| 6 | Ù…Ø­Ø¯ÙˆØ¯ÛŒØª ØªØ¹Ø¯Ø§Ø¯ Ú©Ù†Ø¯Ù„ | **+8%** | Ø³Ø§Ø¯Ù‡ |

**Ù…Ø¬Ù…ÙˆØ¹ ØªØ£Ø«ÛŒØ± ØªØ®Ù…ÛŒÙ†ÛŒ:** +60-75% Ø¨Ù‡Ø¨ÙˆØ¯

---

**ØªØ§Ø±ÛŒØ® Ø¢Ø®Ø±ÛŒÙ† Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ:** 2025-10-28

---

## Ø¨Ø®Ø´ 3.4: ØªØ­Ù„ÛŒÙ„ Ø´Ø±Ø§ÛŒØ· Ù†ÙˆØ³Ø§Ù† (Volatility Analysis)

### Ù…Ø´Ú©Ù„Ø§Øª Ø´Ù†Ø§Ø³Ø§ÛŒÛŒâ€ŒØ´Ø¯Ù‡

#### 1. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø«Ø§Ø¨Øª (Fixed Thresholds)
**Ø´Ø¯Øª:** Ù…ØªÙˆØ³Ø· | **ØªØ£Ø«ÛŒØ± Ø¨Ù‡Ø¨ÙˆØ¯:** +20%

**ØªÙˆØ¶ÛŒØ­ Ù…Ø´Ú©Ù„:**
```python
# Ú©Ø¯ ÙØ¹Ù„ÛŒ (signal_generator.py:1514-1516)
self.vol_high_thresh = 1.3      # Ø«Ø§Ø¨Øª
self.vol_low_thresh = 0.7       # Ø«Ø§Ø¨Øª
self.vol_extreme_thresh = 1.8   # Ø«Ø§Ø¨Øª
```

Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø«Ø§Ø¨Øª Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ø¨Ø§Ø²Ø§Ø±Ù‡Ø§ Ùˆ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…â€ŒÙ‡Ø§ ÛŒÚ©Ø³Ø§Ù† Ù‡Ø³ØªÙ†Ø¯:
- Ø¨Ø§Ø²Ø§Ø±Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ù†ÙˆØ³Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡ Ù…ØªÙØ§ÙˆØªÛŒ Ø¯Ø§Ø±Ù†Ø¯ (BTC vs Altcoins)
- ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ù†ÙˆØ³Ø§Ù† Ù…ØªÙØ§ÙˆØªÛŒ Ø¯Ø§Ø±Ù†Ø¯
- Ø¯Ø± Ø±Ú˜ÛŒÙ…â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¨Ø§Ø²Ø§Ø±ØŒ Ù†ÙˆØ³Ø§Ù† "Ø¹Ø§Ø¯ÛŒ" Ù…ØªÙØ§ÙˆØª Ø§Ø³Øª

**Ø±Ø§Ù‡â€ŒØ­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**
Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ·Ø¨ÛŒÙ‚ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ø¯Ù‡ ØªØ§Ø±ÛŒØ®ÛŒ Ùˆ Ø±Ú˜ÛŒÙ… Ø¨Ø§Ø²Ø§Ø±:

```python
def calculate_adaptive_volatility_thresholds(self, df: pd.DataFrame,
                                             lookback_period: int = 100) -> Dict[str, float]:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù†ÙˆØ³Ø§Ù† Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ø¯Ù‡ ØªØ§Ø±ÛŒØ®ÛŒ"""

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ ATR% Ø¨Ø±Ø§ÛŒ Ø¯ÙˆØ±Ù‡ ØªØ§Ø±ÛŒØ®ÛŒ
    high_p = df['high'].values.astype(np.float64)
    low_p = df['low'].values.astype(np.float64)
    close_p = df['close'].values.astype(np.float64)

    atr = talib.ATR(high_p, low_p, close_p, timeperiod=14)
    atr_pct = (atr / close_p) * 100

    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² lookback_period Ø§Ø®ÛŒØ± Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÙ‡Ø§
    recent_atr_pct = atr_pct[-lookback_period:]
    recent_atr_pct = recent_atr_pct[~np.isnan(recent_atr_pct)]

    if len(recent_atr_pct) < 50:
        # Ø¨Ø§Ø²Ú¯Ø´Øª Ø¨Ù‡ Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶
        return {
            'low': 0.7,
            'high': 1.3,
            'extreme': 1.8
        }

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ percentiles Ø¨Ø±Ø§ÛŒ ØªØ¹ÛŒÛŒÙ† Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÙ‡Ø§
    p25 = np.percentile(recent_atr_pct, 25)
    p50 = np.percentile(recent_atr_pct, 50)  # Ù…ÛŒØ§Ù†Ù‡
    p75 = np.percentile(recent_atr_pct, 75)
    p90 = np.percentile(recent_atr_pct, 90)
    p95 = np.percentile(recent_atr_pct, 95)

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ùˆ Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø±
    mean_atr_pct = np.mean(recent_atr_pct)
    std_atr_pct = np.std(recent_atr_pct)

    # ØªØ¹ÛŒÛŒÙ† Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¢Ù…Ø§Ø±
    # Low: Ø²ÛŒØ± 30% Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
    low_threshold = p25 / p50  # Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù…ÛŒØ§Ù†Ù‡

    # High: Ø¨Ø§Ù„Ø§ÛŒ 75% Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
    high_threshold = p75 / p50

    # Extreme: Ø¨Ø§Ù„Ø§ÛŒ 95% Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
    extreme_threshold = p95 / p50

    # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Ø¨Ø§Ø²Ù‡ Ù…Ø¹Ù‚ÙˆÙ„
    low_threshold = max(0.5, min(0.9, low_threshold))
    high_threshold = max(1.2, min(1.5, high_threshold))
    extreme_threshold = max(1.6, min(2.5, extreme_threshold))

    return {
        'low': round(low_threshold, 2),
        'high': round(high_threshold, 2),
        'extreme': round(extreme_threshold, 2),
        'stats': {
            'mean': mean_atr_pct,
            'std': std_atr_pct,
            'p50': p50,
            'p75': p75,
            'p95': p95
        }
    }

def analyze_volatility_conditions_adaptive(self, df: pd.DataFrame) -> Dict[str, Any]:
    """ØªØ­Ù„ÛŒÙ„ Ù†ÙˆØ³Ø§Ù† Ø¨Ø§ Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ·Ø¨ÛŒÙ‚ÛŒ"""

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ·Ø¨ÛŒÙ‚ÛŒ
    adaptive_thresholds = self.calculate_adaptive_volatility_thresholds(df)

    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ·Ø¨ÛŒÙ‚ÛŒ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ø§Øª
    # ... (Ø¨Ù‚ÛŒÙ‡ Ú©Ø¯ Ù…Ø´Ø§Ø¨Ù‡ Ú©Ø¯ ÙØ¹Ù„ÛŒ)

    vol_low_thresh = adaptive_thresholds['low']
    vol_high_thresh = adaptive_thresholds['high']
    vol_extreme_thresh = adaptive_thresholds['extreme']

    # Ø§Ø¯Ø§Ù…Ù‡ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø´Ø§Ø¨Ù‡ Ú©Ø¯ ÙØ¹Ù„ÛŒ
    if volatility_ratio > vol_extreme_thresh:
        vol_condition = 'extreme'
        vol_score = 0.5
    elif volatility_ratio > vol_high_thresh:
        vol_condition = 'high'
        vol_score = 0.8
    elif volatility_ratio < vol_low_thresh:
        vol_condition = 'low'
        vol_score = 0.9

    return {
        'condition': vol_condition,
        'score': vol_score,
        'thresholds': adaptive_thresholds,  # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ output
        # ... Ø¨Ù‚ÛŒÙ‡ ÙÛŒÙ„Ø¯Ù‡Ø§
    }
```

**Ù…Ø²Ø§ÛŒØ§:**
- ØªØ·Ø§Ø¨Ù‚ Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ø§ Ù†ÙˆØ³Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ Ù‡Ø± Ø¨Ø§Ø²Ø§Ø± (+20%)
- Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹â€ŒØ¨ÛŒÙ†Ø§Ù†Ù‡ Ø¨Ø±Ø§ÛŒ Altcoins Ù¾Ø±Ù†ÙˆØ³Ø§Ù†
- Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ ØªØºÛŒÛŒØ±Ø§Øª Ø¨Ù„Ù†Ø¯Ù…Ø¯Øª Ø¨Ø§Ø²Ø§Ø±

---

#### 2. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÛŒÚ© Ø´Ø§Ø®Øµ Ù†ÙˆØ³Ø§Ù† (ATR)
**Ø´Ø¯Øª:** Ù…ØªÙˆØ³Ø· | **ØªØ£Ø«ÛŒØ± Ø¨Ù‡Ø¨ÙˆØ¯:** +15%

**ØªÙˆØ¶ÛŒØ­ Ù…Ø´Ú©Ù„:**
```python
# Ú©Ø¯ ÙØ¹Ù„ÛŒ (signal_generator.py:4472)
atr = talib.ATR(high_p, low_p, close_p, timeperiod=14)
# ÙÙ‚Ø· ATR Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
```

ATR ØªÙ†Ù‡Ø§ ÛŒÚ© Ø¬Ù†Ø¨Ù‡ Ø§Ø² Ù†ÙˆØ³Ø§Ù† Ø±Ø§ Ø§Ù†Ø¯Ø§Ø²Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯:
- Ù†ÙˆØ³Ø§Ù† Ù‚ÛŒÙ…Øª Ù¾Ø§ÛŒØ§Ù†ÛŒ Ø±Ø§ Ø¯Ø± Ù†Ø¸Ø± Ù†Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ (Close-to-Close)
- ÙØ´Ø±Ø¯Ú¯ÛŒ Ù‚ÛŒÙ…Øª (Price Compression) Ø±Ø§ ØªØ´Ø®ÛŒØµ Ù†Ù…ÛŒâ€ŒØ¯Ù‡Ø¯
- Ø¬Ù‡Øª Ù†ÙˆØ³Ø§Ù† (Ø¨Ù‡ Ø¨Ø§Ù„Ø§ ÛŒØ§ Ù¾Ø§ÛŒÛŒÙ†) Ø±Ø§ Ù…Ø´Ø®Øµ Ù†Ù…ÛŒâ€ŒÚ©Ù†Ø¯

**Ø±Ø§Ù‡â€ŒØ­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**
ØªØ±Ú©ÛŒØ¨ Ú†Ù†Ø¯ÛŒÙ† Ø´Ø§Ø®Øµ Ù†ÙˆØ³Ø§Ù† Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¬Ø§Ù…Ø¹â€ŒØªØ±:

```python
def calculate_multi_volatility_indicators(self, df: pd.DataFrame) -> Dict[str, np.ndarray]:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ù†ÙˆØ³Ø§Ù†"""
    high_p = df['high'].values.astype(np.float64)
    low_p = df['low'].values.astype(np.float64)
    close_p = df['close'].values.astype(np.float64)

    indicators = {}

    # 1. ATR (Average True Range) - Ù†ÙˆØ³Ø§Ù† ÙˆØ§Ù‚Ø¹ÛŒ
    atr = talib.ATR(high_p, low_p, close_p, timeperiod=14)
    atr_pct = (atr / close_p) * 100
    indicators['atr'] = atr_pct

    # 2. Historical Volatility (Standard Deviation of Returns)
    returns = np.diff(np.log(close_p))  # Log returns
    hist_vol = np.zeros_like(close_p)
    for i in range(20, len(close_p)):
        hist_vol[i] = np.std(returns[i-20:i]) * np.sqrt(252) * 100  # Ø³Ø§Ù„Ø§Ù†Ù‡ Ø´Ø¯Ù‡
    indicators['hist_vol'] = hist_vol

    # 3. Bollinger Bands Width - ÙØ´Ø±Ø¯Ú¯ÛŒ Ù‚ÛŒÙ…Øª
    upper, middle, lower = talib.BBANDS(close_p, timeperiod=20, nbdevup=2, nbdevdn=2)
    bb_width = ((upper - lower) / middle) * 100
    indicators['bb_width'] = bb_width

    # 4. Garman-Klass Volatility - Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² OHLC
    open_p = df['open'].values.astype(np.float64)
    gk_vol = np.zeros_like(close_p)
    for i in range(20, len(close_p)):
        # ÙØ±Ù…ÙˆÙ„ Garman-Klass
        hl = np.log(high_p[i-20:i] / low_p[i-20:i]) ** 2
        co = np.log(close_p[i-20:i] / open_p[i-20:i]) ** 2
        gk_vol[i] = np.sqrt(np.mean(0.5 * hl - (2*np.log(2) - 1) * co)) * np.sqrt(252) * 100
    indicators['gk_vol'] = gk_vol

    # 5. Parkinson's Volatility - Ø¨Ø± Ø§Ø³Ø§Ø³ High-Low
    park_vol = np.zeros_like(close_p)
    for i in range(20, len(close_p)):
        hl_ratio = np.log(high_p[i-20:i] / low_p[i-20:i])
        park_vol[i] = np.sqrt(np.mean(hl_ratio ** 2) / (4 * np.log(2))) * np.sqrt(252) * 100
    indicators['park_vol'] = park_vol

    return indicators

def analyze_volatility_multi_indicator(self, df: pd.DataFrame) -> Dict[str, Any]:
    """ØªØ­Ù„ÛŒÙ„ Ù†ÙˆØ³Ø§Ù† Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒ Ù…ØªØ¹Ø¯Ø¯"""

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‡Ù…Ù‡ Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§
    indicators = self.calculate_multi_volatility_indicators(df)

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú© Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø´Ø§Ø®Øµ
    volatility_ratios = {}

    for name, values in indicators.items():
        valid_values = values[~np.isnan(values)]
        if len(valid_values) < 20:
            continue

        current = valid_values[-1]
        ma_20 = np.mean(valid_values[-20:])

        ratio = current / ma_20 if ma_20 > 0 else 1.0
        volatility_ratios[name] = ratio

    # ØªØ±Ú©ÛŒØ¨ ÙˆØ²Ù†ÛŒ Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§
    weights = {
        'atr': 0.3,        # ÙˆØ²Ù† Ø¨ÛŒØ´ØªØ± Ø¨Ø±Ø§ÛŒ ATR
        'hist_vol': 0.25,
        'bb_width': 0.2,
        'gk_vol': 0.15,
        'park_vol': 0.1
    }

    combined_ratio = 0.0
    total_weight = 0.0

    for name, ratio in volatility_ratios.items():
        weight = weights.get(name, 0)
        combined_ratio += ratio * weight
        total_weight += weight

    if total_weight > 0:
        combined_ratio /= total_weight
    else:
        combined_ratio = 1.0

    # Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ combined_ratio
    if combined_ratio > 1.8:
        condition = 'extreme'
        score = 0.5
    elif combined_ratio > 1.3:
        condition = 'high'
        score = 0.8
    elif combined_ratio < 0.7:
        condition = 'low'
        score = 0.9
    else:
        condition = 'normal'
        score = 1.0

    return {
        'condition': condition,
        'score': score,
        'combined_ratio': round(combined_ratio, 2),
        'individual_ratios': {k: round(v, 2) for k, v in volatility_ratios.items()},
        'indicators': {k: round(v[-1], 3) for k, v in indicators.items() if len(v[~np.isnan(v)]) > 0}
    }
```

**Ù…Ø²Ø§ÛŒØ§:**
- ØªØ­Ù„ÛŒÙ„ Ø¬Ø§Ù…Ø¹â€ŒØªØ± Ù†ÙˆØ³Ø§Ù† Ø§Ø² Ø²ÙˆØ§ÛŒØ§ÛŒ Ù…Ø®ØªÙ„Ù (+15%)
- ØªØ´Ø®ÛŒØµ Ø¨Ù‡ØªØ± ÙØ´Ø±Ø¯Ú¯ÛŒ Ù‚ÛŒÙ…Øª (Squeeze) Ù‚Ø¨Ù„ Ø§Ø² Ø­Ø±Ú©Ø§Øª Ø¨Ø²Ø±Ú¯
- Ú©Ø§Ù‡Ø´ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ø¯Ø±Ø³Øª Ù†Ø§Ø´ÛŒ Ø§Ø² Ù…Ø­Ø¯ÙˆØ¯ÛŒØª ATR

---

#### 3. Ø¹Ø¯Ù… ØªØ´Ø®ÛŒØµ Volatility Clustering
**Ø´Ø¯Øª:** Ù¾ÛŒÚ†ÛŒØ¯Ù‡ | **ØªØ£Ø«ÛŒØ± Ø¨Ù‡Ø¨ÙˆØ¯:** +18%

**ØªÙˆØ¶ÛŒØ­ Ù…Ø´Ú©Ù„:**
```python
# Ú©Ø¯ ÙØ¹Ù„ÛŒ ÙÙ‚Ø· Ù†ÙˆØ³Ø§Ù† ÙØ¹Ù„ÛŒ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
volatility_ratio = current_atr_pct / current_atr_pct_ma
# ØªØ±Ù†Ø¯ Ù†ÙˆØ³Ø§Ù† (Ø§ÙØ²Ø§ÛŒØ´ÛŒ ÛŒØ§ Ú©Ø§Ù‡Ø´ÛŒ) Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ‡ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯
```

Ø¯Ø± Ø¨Ø§Ø²Ø§Ø±Ù‡Ø§ÛŒ Ù…Ø§Ù„ÛŒØŒ Ù†ÙˆØ³Ø§Ù† ØªÙ…Ø§ÛŒÙ„ Ø¨Ù‡ "Ø®ÙˆØ´Ù‡â€ŒØ§ÛŒ Ø´Ø¯Ù†" Ø¯Ø§Ø±Ø¯:
- Ù†ÙˆØ³Ø§Ù† Ø¨Ø§Ù„Ø§ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø¨Ø§ Ù†ÙˆØ³Ø§Ù† Ø¨Ø§Ù„Ø§ Ø¯Ù†Ø¨Ø§Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯
- Ù†ÙˆØ³Ø§Ù† Ù¾Ø§ÛŒÛŒÙ† Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø¨Ø§ Ù†ÙˆØ³Ø§Ù† Ù¾Ø§ÛŒÛŒÙ† Ø¯Ù†Ø¨Ø§Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯
- ØªØºÛŒÛŒØ±Ø§Øª Ù†Ø§Ú¯Ù‡Ø§Ù†ÛŒ Ù†ÙˆØ³Ø§Ù† Ø³ÛŒÚ¯Ù†Ø§Ù„ Ù…Ù‡Ù…ÛŒ Ø§Ø³Øª

**Ø±Ø§Ù‡â€ŒØ­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**
Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Volatility Clustering Ø¨Ø§ GARCH ÛŒØ§ Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø¯Ù‡â€ŒØªØ±:

```python
def detect_volatility_clustering(self, atr_pct: np.ndarray, window: int = 20) -> Dict[str, Any]:
    """ØªØ´Ø®ÛŒØµ Ø®ÙˆØ´Ù‡â€ŒØ§ÛŒ Ø¨ÙˆØ¯Ù† Ù†ÙˆØ³Ø§Ù† (Volatility Clustering)"""

    # Ø­Ø°Ù NaN
    valid_atr = atr_pct[~np.isnan(atr_pct)]

    if len(valid_atr) < window * 2:
        return {'status': 'insufficient_data'}

    # 1. Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØºÛŒÛŒØ±Ø§Øª Ù†ÙˆØ³Ø§Ù† (Volatility of Volatility)
    vol_changes = np.diff(valid_atr)
    vol_of_vol = np.std(vol_changes[-window:])
    vol_of_vol_ma = np.std(vol_changes[-window*2:-window])

    vvol_ratio = vol_of_vol / vol_of_vol_ma if vol_of_vol_ma > 0 else 1.0

    # 2. Ù…Ø­Ø§Ø³Ø¨Ù‡ Autocorrelation Ù†ÙˆØ³Ø§Ù†
    # Ù†ÙˆØ³Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ´Ù‡â€ŒØ§ÛŒ Ø¯Ø§Ø±Ø§ÛŒ autocorrelation Ø¨Ø§Ù„Ø§ Ù‡Ø³ØªÙ†Ø¯
    recent_vol = valid_atr[-window:]
    lagged_vol = valid_atr[-window-1:-1]

    # Pearson correlation
    correlation = np.corrcoef(recent_vol, lagged_vol)[0, 1]

    # 3. ØªØ´Ø®ÛŒØµ ØªØ±Ù†Ø¯ Ù†ÙˆØ³Ø§Ù† (Ø§ÙØ²Ø§ÛŒØ´ÛŒ ÛŒØ§ Ú©Ø§Ù‡Ø´ÛŒ)
    # Linear regression Ø¨Ø±Ø§ÛŒ 20 Ù†Ù‚Ø·Ù‡ Ø§Ø®ÛŒØ±
    x = np.arange(window)
    y = valid_atr[-window:]

    slope, intercept = np.polyfit(x, y, 1)

    # Normalize slope Ø¨Ù‡ percentage change
    avg_vol = np.mean(y)
    vol_trend = (slope * window / avg_vol) * 100 if avg_vol > 0 else 0

    # 4. ØªØ¹ÛŒÛŒÙ† ÙˆØ¶Ø¹ÛŒØª clustering
    is_clustering = correlation > 0.3  # Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ù…Ø¹Ù†Ø§Ø¯Ø§Ø±
    is_increasing = vol_trend > 5      # Ø§ÙØ²Ø§ÛŒØ´ > 5%
    is_decreasing = vol_trend < -5     # Ú©Ø§Ù‡Ø´ > 5%
    is_volatile = vvol_ratio > 1.3     # Ù†ÙˆØ³Ø§Ù† Ø®ÙˆØ¯ Ù†ÙˆØ³Ø§Ù† Ø¨Ø§Ù„Ø§

    # 5. ØªØ¹ÛŒÛŒÙ† Ø§Ù…ØªÛŒØ§Ø² ØªØ¹Ø¯ÛŒÙ„
    adjustment = 1.0

    if is_clustering and is_increasing:
        # Ù†ÙˆØ³Ø§Ù† Ø¯Ø± Ø­Ø§Ù„ Ø§ÙØ²Ø§ÛŒØ´ - Ø§Ø­ØªÛŒØ§Ø· Ø¨ÛŒØ´ØªØ±
        adjustment = 0.85
    elif is_clustering and is_decreasing:
        # Ù†ÙˆØ³Ø§Ù† Ø¯Ø± Ø­Ø§Ù„ Ú©Ø§Ù‡Ø´ - ÙØ±ØµØª Ø¨Ù‡ØªØ±
        adjustment = 1.05
    elif is_volatile:
        # Ù†ÙˆØ³Ø§Ù† ØºÛŒØ±Ù‚Ø§Ø¨Ù„ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ - Ø®Ø·Ø±Ù†Ø§Ú©
        adjustment = 0.9

    return {
        'status': 'ok',
        'is_clustering': is_clustering,
        'correlation': round(correlation, 3),
        'trend': 'increasing' if is_increasing else 'decreasing' if is_decreasing else 'stable',
        'trend_percent': round(vol_trend, 2),
        'vol_of_vol_ratio': round(vvol_ratio, 2),
        'adjustment': adjustment
    }

def analyze_volatility_with_clustering(self, df: pd.DataFrame) -> Dict[str, Any]:
    """ØªØ­Ù„ÛŒÙ„ Ù†ÙˆØ³Ø§Ù† Ø¨Ø§ Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ† clustering"""

    # ØªØ­Ù„ÛŒÙ„ Ù…Ø¹Ù…ÙˆÙ„ÛŒ
    base_analysis = self.analyze_volatility_conditions(df)

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ ATR Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ clustering
    high_p = df['high'].values.astype(np.float64)
    low_p = df['low'].values.astype(np.float64)
    close_p = df['close'].values.astype(np.float64)

    atr = talib.ATR(high_p, low_p, close_p, timeperiod=14)
    atr_pct = (atr / close_p) * 100

    # ØªØ´Ø®ÛŒØµ clustering
    clustering = self.detect_volatility_clustering(atr_pct)

    # Ø§Ø¹Ù…Ø§Ù„ adjustment
    if clustering.get('status') == 'ok':
        base_analysis['score'] *= clustering['adjustment']
        base_analysis['clustering'] = clustering

    return base_analysis
```

**Ù…Ø²Ø§ÛŒØ§:**
- ØªØ´Ø®ÛŒØµ Ø²ÙˆØ¯ØªØ± Ø§ÙØ²Ø§ÛŒØ´ Ù†ÙˆØ³Ø§Ù† (+18%)
- Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø¯ÙˆØ±Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø±Ø§Ù… Ù‚Ø¨Ù„ Ø§Ø² Ø·ÙˆÙØ§Ù†
- Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ±Ù†Ø¯ Ù†ÙˆØ³Ø§Ù†

---

#### 4. Ø¹Ø¯Ù… ØªÙÚ©ÛŒÚ© Ù†ÙˆØ³Ø§Ù† ØµØ¹ÙˆØ¯ÛŒ Ùˆ Ù†Ø²ÙˆÙ„ÛŒ
**Ø´Ø¯Øª:** Ø³Ø§Ø¯Ù‡ | **ØªØ£Ø«ÛŒØ± Ø¨Ù‡Ø¨ÙˆØ¯:** +12%

**ØªÙˆØ¶ÛŒØ­ Ù…Ø´Ú©Ù„:**
```python
# Ú©Ø¯ ÙØ¹Ù„ÛŒ
atr = talib.ATR(high_p, low_p, close_p, timeperiod=14)
# ATR ØªÙØ§ÙˆØªÛŒ Ø¨ÛŒÙ† Ù†ÙˆØ³Ø§Ù† ØµØ¹ÙˆØ¯ÛŒ Ùˆ Ù†Ø²ÙˆÙ„ÛŒ Ù‚Ø§Ø¦Ù„ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯
```

Ù†ÙˆØ³Ø§Ù† ØµØ¹ÙˆØ¯ÛŒ (Ø¨Ù‡ Ø³Ù…Øª Ø¨Ø§Ù„Ø§) Ùˆ Ù†ÙˆØ³Ø§Ù† Ù†Ø²ÙˆÙ„ÛŒ (Ø¨Ù‡ Ø³Ù…Øª Ù¾Ø§ÛŒÛŒÙ†) ØªØ£Ø«ÛŒØ±Ø§Øª Ù…ØªÙØ§ÙˆØªÛŒ Ø¯Ø§Ø±Ù†Ø¯:
- Ù†ÙˆØ³Ø§Ù† Ù†Ø²ÙˆÙ„ÛŒ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø´Ø¯ÛŒØ¯ØªØ± Ùˆ Ø®Ø·Ø±Ù†Ø§Ú©â€ŒØªØ± Ø§Ø³Øª
- Ø¯Ø± Ø³ÛŒÚ¯Ù†Ø§Ù„ LONGØŒ Ù†ÙˆØ³Ø§Ù† Ù†Ø²ÙˆÙ„ÛŒ Ø¨Ø³ÛŒØ§Ø± Ø®Ø·Ø±Ù†Ø§Ú© Ø§Ø³Øª
- Ø¯Ø± Ø³ÛŒÚ¯Ù†Ø§Ù„ SHORTØŒ Ù†ÙˆØ³Ø§Ù† ØµØ¹ÙˆØ¯ÛŒ Ø®Ø·Ø±Ù†Ø§Ú© Ø§Ø³Øª

**Ø±Ø§Ù‡â€ŒØ­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**
ØªÙÚ©ÛŒÚ© Ù†ÙˆØ³Ø§Ù† Ø¨Ù‡ ØµØ¹ÙˆØ¯ÛŒ Ùˆ Ù†Ø²ÙˆÙ„ÛŒ:

```python
def calculate_directional_volatility(self, df: pd.DataFrame) -> Dict[str, Any]:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†ÙˆØ³Ø§Ù† Ø¬Ù‡Øªâ€ŒØ¯Ø§Ø± (ØµØ¹ÙˆØ¯ÛŒ vs Ù†Ø²ÙˆÙ„ÛŒ)"""
    close_p = df['close'].values.astype(np.float64)
    high_p = df['high'].values.astype(np.float64)
    low_p = df['low'].values.astype(np.float64)

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¨Ø§Ø²Ø¯Ù‡â€ŒÙ‡Ø§
    returns = np.diff(close_p) / close_p[:-1]

    # Ø¬Ø¯Ø§Ø³Ø§Ø²ÛŒ Ø¨Ø§Ø²Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø«Ø¨Øª Ùˆ Ù…Ù†ÙÛŒ
    positive_returns = returns.copy()
    positive_returns[positive_returns < 0] = 0

    negative_returns = returns.copy()
    negative_returns[negative_returns > 0] = 0

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†ÙˆØ³Ø§Ù† ØµØ¹ÙˆØ¯ÛŒ Ùˆ Ù†Ø²ÙˆÙ„ÛŒ (20 Ø¯ÙˆØ±Ù‡)
    window = 20
    upside_vol = np.zeros(len(returns))
    downside_vol = np.zeros(len(returns))

    for i in range(window, len(returns)):
        upside_vol[i] = np.std(positive_returns[i-window:i]) * np.sqrt(252) * 100
        downside_vol[i] = np.std(negative_returns[i-window:i]) * np.sqrt(252) * 100

    # Ù†Ø³Ø¨Øª Ù†ÙˆØ³Ø§Ù† (Downside / Upside)
    vol_ratio = np.zeros(len(returns))
    for i in range(len(returns)):
        if upside_vol[i] > 0:
            vol_ratio[i] = downside_vol[i] / upside_vol[i]
        else:
            vol_ratio[i] = 1.0

    # ÙˆØ¶Ø¹ÛŒØª ÙØ¹Ù„ÛŒ
    current_upside = upside_vol[-1]
    current_downside = downside_vol[-1]
    current_ratio = vol_ratio[-1]

    # ØªÙØ³ÛŒØ±
    if current_ratio > 1.5:
        bias = 'downside_heavy'  # Ù†ÙˆØ³Ø§Ù† Ù†Ø²ÙˆÙ„ÛŒ ØºØ§Ù„Ø¨ - Ø®Ø·Ø±Ù†Ø§Ú©
        risk_level = 'high'
    elif current_ratio > 1.2:
        bias = 'downside'
        risk_level = 'medium'
    elif current_ratio < 0.8:
        bias = 'upside'  # Ù†ÙˆØ³Ø§Ù† ØµØ¹ÙˆØ¯ÛŒ ØºØ§Ù„Ø¨
        risk_level = 'low'
    else:
        bias = 'balanced'
        risk_level = 'medium'

    return {
        'upside_volatility': round(current_upside, 2),
        'downside_volatility': round(current_downside, 2),
        'ratio': round(current_ratio, 2),
        'bias': bias,
        'risk_level': risk_level
    }

def adjust_score_for_directional_volatility(self, base_score: float,
                                            directional_vol: Dict,
                                            signal_direction: str) -> float:
    """ØªØ¹Ø¯ÛŒÙ„ Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ³Ø§Ù† Ø¬Ù‡Øªâ€ŒØ¯Ø§Ø±"""

    bias = directional_vol['bias']
    ratio = directional_vol['ratio']

    # Ø¨Ø±Ø§ÛŒ Ø³ÛŒÚ¯Ù†Ø§Ù„ LONG
    if signal_direction == 'long':
        if bias == 'downside_heavy':
            # Ù†ÙˆØ³Ø§Ù† Ù†Ø²ÙˆÙ„ÛŒ Ø¨Ø³ÛŒØ§Ø± Ø¨Ø§Ù„Ø§ - Ú©Ø§Ù‡Ø´ Ø´Ø¯ÛŒØ¯ Ø§Ù…ØªÛŒØ§Ø²
            return base_score * 0.7
        elif bias == 'downside':
            # Ù†ÙˆØ³Ø§Ù† Ù†Ø²ÙˆÙ„ÛŒ Ø¨Ø§Ù„Ø§ - Ú©Ø§Ù‡Ø´ Ù…ØªÙˆØ³Ø·
            return base_score * 0.85
        elif bias == 'upside':
            # Ù†ÙˆØ³Ø§Ù† ØµØ¹ÙˆØ¯ÛŒ - Ù…Ø«Ø¨Øª Ø¨Ø±Ø§ÛŒ LONG
            return base_score * 1.05

    # Ø¨Ø±Ø§ÛŒ Ø³ÛŒÚ¯Ù†Ø§Ù„ SHORT
    elif signal_direction == 'short':
        if bias == 'downside_heavy':
            # Ù†ÙˆØ³Ø§Ù† Ù†Ø²ÙˆÙ„ÛŒ Ø¨Ø§Ù„Ø§ - Ù…Ø«Ø¨Øª Ø¨Ø±Ø§ÛŒ SHORT
            return base_score * 1.05
        elif bias == 'upside':
            # Ù†ÙˆØ³Ø§Ù† ØµØ¹ÙˆØ¯ÛŒ - Ø®Ø·Ø±Ù†Ø§Ú© Ø¨Ø±Ø§ÛŒ SHORT
            return base_score * 0.85

    return base_score
```

**Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ú©Ø¯ Ø§ØµÙ„ÛŒ:**
```python
def analyze_volatility_conditions_full(self, df: pd.DataFrame, signal_direction: str = 'long') -> Dict:
    """ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„ Ù†ÙˆØ³Ø§Ù† Ø¨Ø§ Ù†ÙˆØ³Ø§Ù† Ø¬Ù‡Øªâ€ŒØ¯Ø§Ø±"""

    # ØªØ­Ù„ÛŒÙ„ Ù¾Ø§ÛŒÙ‡
    base_analysis = self.analyze_volatility_conditions(df)

    # Ù†ÙˆØ³Ø§Ù† Ø¬Ù‡Øªâ€ŒØ¯Ø§Ø±
    directional_vol = self.calculate_directional_volatility(df)

    # ØªØ¹Ø¯ÛŒÙ„ Ø§Ù…ØªÛŒØ§Ø²
    adjusted_score = self.adjust_score_for_directional_volatility(
        base_analysis['score'],
        directional_vol,
        signal_direction
    )

    base_analysis['score'] = adjusted_score
    base_analysis['directional_volatility'] = directional_vol

    return base_analysis
```

**Ù…Ø²Ø§ÛŒØ§:**
- Ø­ÙØ§Ø¸Øª Ø¨Ù‡ØªØ± Ø¯Ø± Ø¨Ø±Ø§Ø¨Ø± Ø±ÛŒØ²Ø´â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ú¯Ù‡Ø§Ù†ÛŒ (+12%)
- Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ù…ØªÙ†Ø§Ø³Ø¨ Ø¨Ø§ Ø¬Ù‡Øª Ø³ÛŒÚ¯Ù†Ø§Ù„
- Ú©Ø§Ù‡Ø´ Ø¶Ø±Ø± Ø¯Ø± Ù†ÙˆØ³Ø§Ù†Ø§Øª Ù†Ø²ÙˆÙ„ÛŒ Ø´Ø¯ÛŒØ¯

---

#### 5. Ø¹Ø¯Ù… Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
**Ø´Ø¯Øª:** Ø³Ø§Ø¯Ù‡ | **ØªØ£Ø«ÛŒØ± Ø¨Ù‡Ø¨ÙˆØ¯:** +10%

**ØªÙˆØ¶ÛŒØ­ Ù…Ø´Ú©Ù„:**
```python
# Ú©Ø¯ ÙØ¹Ù„ÛŒ
atr = talib.ATR(high_p, low_p, close_p, timeperiod=14)  # Ù‡Ù…ÛŒØ´Ù‡ 14
atr_pct_ma = moving_average(atr_pct, 20)                # Ù‡Ù…ÛŒØ´Ù‡ 20
```

Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø«Ø§Ø¨Øª Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…â€ŒÙ‡Ø§ ÛŒÚ©Ø³Ø§Ù† Ø§Ø³Øª:
- Ø¯Ø± ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ… 5mØŒ 14 Ø¯ÙˆØ±Ù‡ = 70 Ø¯Ù‚ÛŒÙ‚Ù‡
- Ø¯Ø± ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ… 1DØŒ 14 Ø¯ÙˆØ±Ù‡ = 2 Ù‡ÙØªÙ‡
- Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ù†ÙˆØ³Ø§Ù† Ø¯Ø± ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ù…ØªÙØ§ÙˆØª Ø§Ø³Øª

**Ø±Ø§Ù‡â€ŒØ­Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**
Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ ØªØ·Ø¨ÛŒÙ‚ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…:

```python
def get_timeframe_adjusted_parameters(self, timeframe: str) -> Dict[str, int]:
    """ØªØ¹ÛŒÛŒÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…"""

    # Ø¬Ø¯ÙˆÙ„ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡
    params = {
        '1m':  {'atr_period': 20, 'ma_period': 30, 'lookback': 200},
        '5m':  {'atr_period': 14, 'ma_period': 24, 'lookback': 150},
        '15m': {'atr_period': 14, 'ma_period': 20, 'lookback': 120},
        '1h':  {'atr_period': 14, 'ma_period': 20, 'lookback': 100},
        '4h':  {'atr_period': 12, 'ma_period': 18, 'lookback': 80},
        '1d':  {'atr_period': 10, 'ma_period': 14, 'lookback': 60}
    }

    return params.get(timeframe, {'atr_period': 14, 'ma_period': 20, 'lookback': 100})

def analyze_volatility_timeframe_adjusted(self, df: pd.DataFrame, timeframe: str = '1h') -> Dict:
    """ØªØ­Ù„ÛŒÙ„ Ù†ÙˆØ³Ø§Ù† Ø¨Ø§ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ ØªØ·Ø¨ÛŒÙ‚â€ŒÛŒØ§ÙØªÙ‡ Ø¨Ø§ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…"""

    # Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡
    params = self.get_timeframe_adjusted_parameters(timeframe)

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ ATR Ø¨Ø§ Ø¯ÙˆØ±Ù‡ ØªØ·Ø¨ÛŒÙ‚ÛŒ
    high_p = df['high'].values.astype(np.float64)
    low_p = df['low'].values.astype(np.float64)
    close_p = df['close'].values.astype(np.float64)

    atr = talib.ATR(high_p, low_p, close_p, timeperiod=params['atr_period'])
    atr_pct = (atr / close_p) * 100

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ MA Ø¨Ø§ Ø¯ÙˆØ±Ù‡ ØªØ·Ø¨ÛŒÙ‚ÛŒ
    atr_pct_ma = np.zeros_like(atr_pct)
    for i in range(len(atr_pct)):
        start_idx = max(0, i - params['ma_period'] + 1)
        atr_pct_ma[i] = np.mean(atr_pct[start_idx:i + 1])

    # Ø§Ø¯Ø§Ù…Ù‡ Ù…Ø­Ø§Ø³Ø¨Ø§Øª
    current_atr_pct = atr_pct[-1]
    current_atr_pct_ma = atr_pct_ma[-1]

    volatility_ratio = current_atr_pct / current_atr_pct_ma if current_atr_pct_ma > 0 else 1.0

    # Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ (Ù…Ø´Ø§Ø¨Ù‡ Ú©Ø¯ ÙØ¹Ù„ÛŒ)
    # ...

    return {
        'condition': vol_condition,
        'score': vol_score,
        'volatility_ratio': volatility_ratio,
        'timeframe': timeframe,
        'parameters': params
    }
```

**Ù…Ø²Ø§ÛŒØ§:**
- Ø¯Ù‚Øª Ø¨Ù‡ØªØ± Ø¯Ø± Ù‡Ù…Ù‡ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…â€ŒÙ‡Ø§ (+10%)
- Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©ÙˆØªØ§Ù‡â€ŒÙ…Ø¯Øª Ùˆ Ø¨Ù„Ù†Ø¯Ù…Ø¯Øª
- Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§

---

### Ø¬Ø¯ÙˆÙ„ Ø®Ù„Ø§ØµÙ‡ Ø¨Ù‡Ø¨ÙˆØ¯Ù‡Ø§

| # | Ù…Ø´Ú©Ù„ | ØªØ£Ø«ÛŒØ± ØªØ®Ù…ÛŒÙ†ÛŒ | Ø³Ø®ØªÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ |
|---|------|-------|---------|
| 1 | Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø«Ø§Ø¨Øª | **+20%** | Ù…ØªÙˆØ³Ø· |
| 2 | Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÛŒÚ© Ø´Ø§Ø®Øµ (ATR) | **+15%** | Ù…ØªÙˆØ³Ø· |
| 3 | Ø¹Ø¯Ù… ØªØ´Ø®ÛŒØµ Volatility Clustering | **+18%** | Ù¾ÛŒÚ†ÛŒØ¯Ù‡ |
| 4 | Ø¹Ø¯Ù… ØªÙÚ©ÛŒÚ© Ù†ÙˆØ³Ø§Ù† Ø¬Ù‡Øªâ€ŒØ¯Ø§Ø± | **+12%** | Ø³Ø§Ø¯Ù‡ |
| 5 | Ø¹Ø¯Ù… Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ… | **+10%** | Ø³Ø§Ø¯Ù‡ |

**Ù…Ø¬Ù…ÙˆØ¹ ØªØ£Ø«ÛŒØ± ØªØ®Ù…ÛŒÙ†ÛŒ:** +50-60% Ø¨Ù‡Ø¨ÙˆØ¯

---

**ØªØ§Ø±ÛŒØ® Ø¢Ø®Ø±ÛŒÙ† Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ:** 2025-10-28

